{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autorec import *\n",
    "import sys, importlib\n",
    "importlib.reload(sys.modules['movielens_recommender'])\n",
    "importlib.reload(sys.modules['autorec'])\n",
    "from autorec import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<autorec.AutoRec at 0x28545c19e48>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autorec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = np.array([[0.9*3, 0, 0, 0]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24592996, 0.08943301, 0.7530067 , 0.46349978]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autorec.model.predict(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    autorec.training_step(batch=ratings, batch_mask=ratings_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 3., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 2, 3, 4]], dtype='float32') * np.array([[1, 0, 1, 0]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = np.array(\n",
    "    [[0.1, 0.9, 1.0],\n",
    "     [0.3, 0.8, 0.8],\n",
    "     [0.8, 0.1, 0.2],\n",
    "     [0.7, 0.1, 0.1]], dtype='float32').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.3, 0.8, 0.7],\n",
       "       [1. , 0.8, 0.2, 0.1]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings[[0,2], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1, 0.3, 0.8, 0.7],\n",
       "       [0.9, 0.8, 0.1, 0.1],\n",
       "       [1. , 0.8, 0.2, 0.1]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 2000. Loss per movie: 15.369809188117028, test rmse: 3.61223, train rmse: 3.60319\n",
      "Epoch 1 of 2000. Loss per movie: 14.443714513506531, test rmse: 3.48392, train rmse: 3.47304\n",
      "Epoch 2 of 2000. Loss per movie: 13.335557472124679, test rmse: 3.31148, train rmse: 3.29994\n",
      "Epoch 3 of 2000. Loss per movie: 11.971447904384943, test rmse: 3.11214, train rmse: 3.10114\n",
      "Epoch 4 of 2000. Loss per movie: 10.539001341807289, test rmse: 2.91092, train rmse: 2.89954\n",
      "Epoch 5 of 2000. Loss per movie: 9.202884491502035, test rmse: 2.71917, train rmse: 2.70734\n",
      "Epoch 6 of 2000. Loss per movie: 8.020915666176505, test rmse: 2.53994, train rmse: 2.52812\n",
      "Epoch 7 of 2000. Loss per movie: 6.996769110739727, test rmse: 2.37513, train rmse: 2.36331\n",
      "Epoch 8 of 2000. Loss per movie: 6.11734425241967, test rmse: 2.22374, train rmse: 2.21192\n",
      "Epoch 9 of 2000. Loss per movie: 5.362564938530485, test rmse: 2.08555, train rmse: 2.07359\n",
      "Epoch 10 of 2000. Loss per movie: 4.717223744330027, test rmse: 1.95990, train rmse: 1.94773\n",
      "Epoch 11 of 2000. Loss per movie: 4.1665696533624965, test rmse: 1.84606, train rmse: 1.83367\n",
      "Epoch 12 of 2000. Loss per movie: 3.6975672249278047, test rmse: 1.74339, train rmse: 1.73076\n",
      "Epoch 13 of 2000. Loss per movie: 3.298960406488244, test rmse: 1.65125, train rmse: 1.63838\n",
      "Epoch 14 of 2000. Loss per movie: 2.960932581376519, test rmse: 1.56902, train rmse: 1.55589\n",
      "Epoch 15 of 2000. Loss per movie: 2.6749046357718433, test rmse: 1.49602, train rmse: 1.48264\n",
      "Epoch 16 of 2000. Loss per movie: 2.433381180700876, test rmse: 1.43158, train rmse: 1.41794\n",
      "Epoch 17 of 2000. Loss per movie: 2.229825658123502, test rmse: 1.37501, train rmse: 1.36111\n",
      "Epoch 18 of 2000. Loss per movie: 2.0585544843594326, test rmse: 1.32561, train rmse: 1.31144\n",
      "Epoch 19 of 2000. Loss per movie: 1.914645270700262, test rmse: 1.28268, train rmse: 1.26825\n",
      "Epoch 20 of 2000. Loss per movie: 1.793853250751881, test rmse: 1.24555, train rmse: 1.23086\n",
      "Epoch 21 of 2000. Loss per movie: 1.6925351889039901, test rmse: 1.21355, train rmse: 1.19861\n",
      "Epoch 22 of 2000. Loss per movie: 1.607577982895723, test rmse: 1.18608, train rmse: 1.17089\n",
      "Epoch 23 of 2000. Loss per movie: 1.536333485817654, test rmse: 1.16255, train rmse: 1.14712\n",
      "Epoch 24 of 2000. Loss per movie: 1.4765581125028069, test rmse: 1.14244, train rmse: 1.12678\n",
      "Epoch 25 of 2000. Loss per movie: 1.4263582048886738, test rmse: 1.12527, train rmse: 1.10940\n",
      "Epoch 26 of 2000. Loss per movie: 1.384141191432649, test rmse: 1.11062, train rmse: 1.09455\n",
      "Epoch 27 of 2000. Loss per movie: 1.3485712046855696, test rmse: 1.09812, train rmse: 1.08186\n",
      "Epoch 28 of 2000. Loss per movie: 1.3185309319802327, test rmse: 1.08745, train rmse: 1.07100\n",
      "Epoch 29 of 2000. Loss per movie: 1.2930875660831664, test rmse: 1.07831, train rmse: 1.06169\n",
      "Epoch 30 of 2000. Loss per movie: 1.2714635356994928, test rmse: 1.07047, train rmse: 1.05370\n",
      "Epoch 31 of 2000. Loss per movie: 1.253011452031901, test rmse: 1.06372, train rmse: 1.04680\n",
      "Epoch 32 of 2000. Loss per movie: 1.2371924087210302, test rmse: 1.05789, train rmse: 1.04083\n",
      "Epoch 33 of 2000. Loss per movie: 1.2235577541639349, test rmse: 1.05282, train rmse: 1.03563\n",
      "Epoch 34 of 2000. Loss per movie: 1.2117336035200021, test rmse: 1.04838, train rmse: 1.03107\n",
      "Epoch 35 of 2000. Loss per movie: 1.201407723562896, test rmse: 1.04448, train rmse: 1.02705\n",
      "Epoch 36 of 2000. Loss per movie: 1.1923188593384768, test rmse: 1.04101, train rmse: 1.02348\n",
      "Epoch 37 of 2000. Loss per movie: 1.1842471016714888, test rmse: 1.03791, train rmse: 1.02027\n",
      "Epoch 38 of 2000. Loss per movie: 1.1770062606059697, test rmse: 1.03509, train rmse: 1.01735\n",
      "Epoch 39 of 2000. Loss per movie: 1.1704369645906827, test rmse: 1.03251, train rmse: 1.01468\n",
      "Epoch 40 of 2000. Loss per movie: 1.1644003847691449, test rmse: 1.03011, train rmse: 1.01218\n",
      "Epoch 41 of 2000. Loss per movie: 1.1587724056311934, test rmse: 1.02784, train rmse: 1.00982\n",
      "Epoch 42 of 2000. Loss per movie: 1.1534372405121358, test rmse: 1.02566, train rmse: 1.00755\n",
      "Epoch 43 of 2000. Loss per movie: 1.1482796335191987, test rmse: 1.02350, train rmse: 1.00530\n",
      "Epoch 44 of 2000. Loss per movie: 1.1431734782336866, test rmse: 1.02130, train rmse: 1.00302\n",
      "Epoch 45 of 2000. Loss per movie: 1.137963385842785, test rmse: 1.01897, train rmse: 1.00063\n",
      "Epoch 46 of 2000. Loss per movie: 1.1324475241756327, test rmse: 1.01643, train rmse: 0.99802\n",
      "Epoch 47 of 2000. Loss per movie: 1.1264209341209086, test rmse: 1.01358, train rmse: 0.99511\n",
      "Epoch 48 of 2000. Loss per movie: 1.1198070455391256, test rmse: 1.01064, train rmse: 0.99213\n",
      "Epoch 49 of 2000. Loss per movie: 1.1131885848345853, test rmse: 1.00779, train rmse: 0.98921\n",
      "Epoch 50 of 2000. Loss per movie: 1.106655868069312, test rmse: 1.00482, train rmse: 0.98616\n",
      "Epoch 51 of 2000. Loss per movie: 1.0998014160625715, test rmse: 1.00182, train rmse: 0.98304\n",
      "Epoch 52 of 2000. Loss per movie: 1.0929539674244086, test rmse: 0.99896, train rmse: 0.98007\n",
      "Epoch 53 of 2000. Loss per movie: 1.0864086932858161, test rmse: 0.99623, train rmse: 0.97722\n",
      "Epoch 54 of 2000. Loss per movie: 1.080192821913185, test rmse: 0.99362, train rmse: 0.97451\n",
      "Epoch 55 of 2000. Loss per movie: 1.0742967785042616, test rmse: 0.99114, train rmse: 0.97194\n",
      "Epoch 56 of 2000. Loss per movie: 1.0687006565960353, test rmse: 0.98882, train rmse: 0.96949\n",
      "Epoch 57 of 2000. Loss per movie: 1.0633935074318603, test rmse: 0.98663, train rmse: 0.96717\n",
      "Epoch 58 of 2000. Loss per movie: 1.0583943848660953, test rmse: 0.98458, train rmse: 0.96499\n",
      "Epoch 59 of 2000. Loss per movie: 1.0537082284299144, test rmse: 0.98268, train rmse: 0.96296\n",
      "Epoch 60 of 2000. Loss per movie: 1.049365229187057, test rmse: 0.98090, train rmse: 0.96108\n",
      "Epoch 61 of 2000. Loss per movie: 1.0453508984603157, test rmse: 0.97925, train rmse: 0.95933\n",
      "Epoch 62 of 2000. Loss per movie: 1.0416027996401158, test rmse: 0.97771, train rmse: 0.95767\n",
      "Epoch 63 of 2000. Loss per movie: 1.0380776379253012, test rmse: 0.97627, train rmse: 0.95611\n",
      "Epoch 64 of 2000. Loss per movie: 1.0347458217713836, test rmse: 0.97491, train rmse: 0.95463\n",
      "Epoch 65 of 2000. Loss per movie: 1.0315829535301744, test rmse: 0.97364, train rmse: 0.95321\n",
      "Epoch 66 of 2000. Loss per movie: 1.0285684308314011, test rmse: 0.97245, train rmse: 0.95185\n",
      "Epoch 67 of 2000. Loss per movie: 1.0256880224105436, test rmse: 0.97132, train rmse: 0.95056\n",
      "Epoch 68 of 2000. Loss per movie: 1.022934139979722, test rmse: 0.97026, train rmse: 0.94932\n",
      "Epoch 69 of 2000. Loss per movie: 1.0203016280560941, test rmse: 0.96926, train rmse: 0.94813\n",
      "Epoch 70 of 2000. Loss per movie: 1.01778382303882, test rmse: 0.96832, train rmse: 0.94699\n",
      "Epoch 71 of 2000. Loss per movie: 1.0153723953736948, test rmse: 0.96742, train rmse: 0.94590\n",
      "Epoch 72 of 2000. Loss per movie: 1.0130588224469979, test rmse: 0.96657, train rmse: 0.94484\n",
      "Epoch 73 of 2000. Loss per movie: 1.0108346425962504, test rmse: 0.96575, train rmse: 0.94383\n",
      "Epoch 74 of 2000. Loss per movie: 1.0086918163526355, test rmse: 0.96497, train rmse: 0.94285\n",
      "Epoch 75 of 2000. Loss per movie: 1.0066224474118808, test rmse: 0.96422, train rmse: 0.94190\n",
      "Epoch 76 of 2000. Loss per movie: 1.0046192618811174, test rmse: 0.96350, train rmse: 0.94097\n",
      "Epoch 77 of 2000. Loss per movie: 1.0026759799397091, test rmse: 0.96280, train rmse: 0.94008\n",
      "Epoch 78 of 2000. Loss per movie: 1.000787556951026, test rmse: 0.96212, train rmse: 0.93920\n",
      "Epoch 79 of 2000. Loss per movie: 0.9989495199443894, test rmse: 0.96146, train rmse: 0.93835\n",
      "Epoch 80 of 2000. Loss per movie: 0.9971572589505725, test rmse: 0.96081, train rmse: 0.93751\n",
      "Epoch 81 of 2000. Loss per movie: 0.995407014057554, test rmse: 0.96018, train rmse: 0.93670\n",
      "Epoch 82 of 2000. Loss per movie: 0.9936987357247316, test rmse: 0.95957, train rmse: 0.93590\n",
      "Epoch 83 of 2000. Loss per movie: 0.992035735376383, test rmse: 0.95898, train rmse: 0.93513\n",
      "Epoch 84 of 2000. Loss per movie: 0.9904205714054538, test rmse: 0.95842, train rmse: 0.93438\n",
      "Epoch 85 of 2000. Loss per movie: 0.9888530859057033, test rmse: 0.95788, train rmse: 0.93365\n",
      "Epoch 86 of 2000. Loss per movie: 0.9873304549919155, test rmse: 0.95736, train rmse: 0.93294\n",
      "Epoch 87 of 2000. Loss per movie: 0.9858487732486975, test rmse: 0.95685, train rmse: 0.93225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88 of 2000. Loss per movie: 0.984404605790069, test rmse: 0.95636, train rmse: 0.93157\n",
      "Epoch 89 of 2000. Loss per movie: 0.9829952509167929, test rmse: 0.95588, train rmse: 0.93091\n",
      "Epoch 90 of 2000. Loss per movie: 0.9816186087168355, test rmse: 0.95542, train rmse: 0.93026\n",
      "Epoch 91 of 2000. Loss per movie: 0.9802725431326595, test rmse: 0.95497, train rmse: 0.92962\n",
      "Epoch 92 of 2000. Loss per movie: 0.9789546465902068, test rmse: 0.95453, train rmse: 0.92900\n",
      "Epoch 93 of 2000. Loss per movie: 0.9776622754356665, test rmse: 0.95410, train rmse: 0.92839\n",
      "Epoch 94 of 2000. Loss per movie: 0.9763926380312825, test rmse: 0.95368, train rmse: 0.92779\n",
      "Epoch 95 of 2000. Loss per movie: 0.9751430854786024, test rmse: 0.95327, train rmse: 0.92719\n",
      "Epoch 96 of 2000. Loss per movie: 0.9739110303974038, test rmse: 0.95287, train rmse: 0.92661\n",
      "Epoch 97 of 2000. Loss per movie: 0.9726946849318379, test rmse: 0.95248, train rmse: 0.92603\n",
      "Epoch 98 of 2000. Loss per movie: 0.9714946747534907, test rmse: 0.95209, train rmse: 0.92546\n",
      "Epoch 99 of 2000. Loss per movie: 0.9703134849720705, test rmse: 0.95170, train rmse: 0.92489\n",
      "Epoch 100 of 2000. Loss per movie: 0.9691511900756645, test rmse: 0.95132, train rmse: 0.92434\n",
      "Epoch 101 of 2000. Loss per movie: 0.9680061288594349, test rmse: 0.95094, train rmse: 0.92379\n",
      "Epoch 102 of 2000. Loss per movie: 0.9668763283458532, test rmse: 0.95057, train rmse: 0.92325\n",
      "Epoch 103 of 2000. Loss per movie: 0.9657599716209203, test rmse: 0.95020, train rmse: 0.92272\n",
      "Epoch 104 of 2000. Loss per movie: 0.9646553430489212, test rmse: 0.94983, train rmse: 0.92219\n",
      "Epoch 105 of 2000. Loss per movie: 0.9635609744136597, test rmse: 0.94946, train rmse: 0.92166\n",
      "Epoch 106 of 2000. Loss per movie: 0.9624756357757713, test rmse: 0.94910, train rmse: 0.92114\n",
      "Epoch 107 of 2000. Loss per movie: 0.9613982622603598, test rmse: 0.94875, train rmse: 0.92062\n",
      "Epoch 108 of 2000. Loss per movie: 0.9603280490984106, test rmse: 0.94839, train rmse: 0.92010\n",
      "Epoch 109 of 2000. Loss per movie: 0.9592643803280118, test rmse: 0.94804, train rmse: 0.91959\n",
      "Epoch 110 of 2000. Loss per movie: 0.9582068511903924, test rmse: 0.94770, train rmse: 0.91908\n",
      "Epoch 111 of 2000. Loss per movie: 0.9571550436734303, test rmse: 0.94736, train rmse: 0.91857\n",
      "Epoch 112 of 2000. Loss per movie: 0.9561082902192786, test rmse: 0.94702, train rmse: 0.91806\n",
      "Epoch 113 of 2000. Loss per movie: 0.9550658073209838, test rmse: 0.94668, train rmse: 0.91755\n",
      "Epoch 114 of 2000. Loss per movie: 0.954026630177084, test rmse: 0.94634, train rmse: 0.91705\n",
      "Epoch 115 of 2000. Loss per movie: 0.952989689164156, test rmse: 0.94601, train rmse: 0.91655\n",
      "Epoch 116 of 2000. Loss per movie: 0.9519538413755392, test rmse: 0.94568, train rmse: 0.91604\n",
      "Epoch 117 of 2000. Loss per movie: 0.9509178734562769, test rmse: 0.94535, train rmse: 0.91554\n",
      "Epoch 118 of 2000. Loss per movie: 0.9498807080577302, test rmse: 0.94503, train rmse: 0.91503\n",
      "Epoch 119 of 2000. Loss per movie: 0.9488415594758659, test rmse: 0.94470, train rmse: 0.91452\n",
      "Epoch 120 of 2000. Loss per movie: 0.947800270584052, test rmse: 0.94438, train rmse: 0.91401\n",
      "Epoch 121 of 2000. Loss per movie: 0.9467574769761701, test rmse: 0.94405, train rmse: 0.91351\n",
      "Epoch 122 of 2000. Loss per movie: 0.9457143553655581, test rmse: 0.94372, train rmse: 0.91300\n",
      "Epoch 123 of 2000. Loss per movie: 0.9446718757983196, test rmse: 0.94340, train rmse: 0.91249\n",
      "Epoch 124 of 2000. Loss per movie: 0.9436305526745872, test rmse: 0.94307, train rmse: 0.91198\n",
      "Epoch 125 of 2000. Loss per movie: 0.9425905622568482, test rmse: 0.94274, train rmse: 0.91147\n",
      "Epoch 126 of 2000. Loss per movie: 0.9415517555689273, test rmse: 0.94241, train rmse: 0.91097\n",
      "Epoch 127 of 2000. Loss per movie: 0.9405138154517456, test rmse: 0.94208, train rmse: 0.91046\n",
      "Epoch 128 of 2000. Loss per movie: 0.9394762238906198, test rmse: 0.94175, train rmse: 0.90995\n",
      "Epoch 129 of 2000. Loss per movie: 0.9384380296918073, test rmse: 0.94142, train rmse: 0.90944\n",
      "Epoch 130 of 2000. Loss per movie: 0.9373980784671343, test rmse: 0.94109, train rmse: 0.90893\n",
      "Epoch 131 of 2000. Loss per movie: 0.9363547553630561, test rmse: 0.94076, train rmse: 0.90842\n",
      "Epoch 132 of 2000. Loss per movie: 0.9353068688536654, test rmse: 0.94043, train rmse: 0.90790\n",
      "Epoch 133 of 2000. Loss per movie: 0.9342552427729585, test rmse: 0.94009, train rmse: 0.90739\n",
      "Epoch 134 of 2000. Loss per movie: 0.9332026367238528, test rmse: 0.93975, train rmse: 0.90687\n",
      "Epoch 135 of 2000. Loss per movie: 0.9321505270021282, test rmse: 0.93941, train rmse: 0.90636\n",
      "Epoch 136 of 2000. Loss per movie: 0.9310985636342578, test rmse: 0.93907, train rmse: 0.90584\n",
      "Epoch 137 of 2000. Loss per movie: 0.9300458815378468, test rmse: 0.93873, train rmse: 0.90532\n",
      "Epoch 138 of 2000. Loss per movie: 0.9289924527178479, test rmse: 0.93838, train rmse: 0.90480\n",
      "Epoch 139 of 2000. Loss per movie: 0.9279390991655456, test rmse: 0.93804, train rmse: 0.90429\n",
      "Epoch 140 of 2000. Loss per movie: 0.9268873496231369, test rmse: 0.93769, train rmse: 0.90377\n",
      "Epoch 141 of 2000. Loss per movie: 0.9258384521736118, test rmse: 0.93734, train rmse: 0.90326\n",
      "Epoch 142 of 2000. Loss per movie: 0.924793218318971, test rmse: 0.93699, train rmse: 0.90274\n",
      "Epoch 143 of 2000. Loss per movie: 0.9237520246103176, test rmse: 0.93664, train rmse: 0.90223\n",
      "Epoch 144 of 2000. Loss per movie: 0.9227150283160306, test rmse: 0.93629, train rmse: 0.90172\n",
      "Epoch 145 of 2000. Loss per movie: 0.9216822591321222, test rmse: 0.93594, train rmse: 0.90122\n",
      "Epoch 146 of 2000. Loss per movie: 0.9206536529889147, test rmse: 0.93560, train rmse: 0.90071\n",
      "Epoch 147 of 2000. Loss per movie: 0.9196292397241672, test rmse: 0.93525, train rmse: 0.90021\n",
      "Epoch 148 of 2000. Loss per movie: 0.9186092382662362, test rmse: 0.93491, train rmse: 0.89970\n",
      "Epoch 149 of 2000. Loss per movie: 0.9175940995834387, test rmse: 0.93456, train rmse: 0.89920\n",
      "Epoch 150 of 2000. Loss per movie: 0.9165842134802293, test rmse: 0.93423, train rmse: 0.89871\n",
      "Epoch 151 of 2000. Loss per movie: 0.9155796124166881, test rmse: 0.93389, train rmse: 0.89821\n",
      "Epoch 152 of 2000. Loss per movie: 0.9145801825699142, test rmse: 0.93356, train rmse: 0.89772\n",
      "Epoch 153 of 2000. Loss per movie: 0.9135856937426591, test rmse: 0.93323, train rmse: 0.89723\n",
      "Epoch 154 of 2000. Loss per movie: 0.9125960051800777, test rmse: 0.93290, train rmse: 0.89674\n",
      "Epoch 155 of 2000. Loss per movie: 0.9116110174465973, test rmse: 0.93257, train rmse: 0.89626\n",
      "Epoch 156 of 2000. Loss per movie: 0.9106306518017182, test rmse: 0.93225, train rmse: 0.89577\n",
      "Epoch 157 of 2000. Loss per movie: 0.9096548689106273, test rmse: 0.93193, train rmse: 0.89529\n",
      "Epoch 158 of 2000. Loss per movie: 0.9086837021547606, test rmse: 0.93161, train rmse: 0.89481\n",
      "Epoch 159 of 2000. Loss per movie: 0.907717160251563, test rmse: 0.93129, train rmse: 0.89433\n",
      "Epoch 160 of 2000. Loss per movie: 0.9067553285327734, test rmse: 0.93098, train rmse: 0.89386\n",
      "Epoch 161 of 2000. Loss per movie: 0.9057982460497109, test rmse: 0.93067, train rmse: 0.89338\n",
      "Epoch 162 of 2000. Loss per movie: 0.9048460778668435, test rmse: 0.93036, train rmse: 0.89291\n",
      "Epoch 163 of 2000. Loss per movie: 0.9038988243385387, test rmse: 0.93006, train rmse: 0.89244\n",
      "Epoch 164 of 2000. Loss per movie: 0.9029566622233987, test rmse: 0.92975, train rmse: 0.89198\n",
      "Epoch 165 of 2000. Loss per movie: 0.9020196407076578, test rmse: 0.92945, train rmse: 0.89151\n",
      "Epoch 166 of 2000. Loss per movie: 0.9010878863714538, test rmse: 0.92915, train rmse: 0.89105\n",
      "Epoch 167 of 2000. Loss per movie: 0.9001614889406846, test rmse: 0.92886, train rmse: 0.89059\n",
      "Epoch 168 of 2000. Loss per movie: 0.8992405615295156, test rmse: 0.92856, train rmse: 0.89014\n",
      "Epoch 169 of 2000. Loss per movie: 0.8983251845794116, test rmse: 0.92827, train rmse: 0.88968\n",
      "Epoch 170 of 2000. Loss per movie: 0.8974154568880832, test rmse: 0.92799, train rmse: 0.88923\n",
      "Epoch 171 of 2000. Loss per movie: 0.8965113900787907, test rmse: 0.92770, train rmse: 0.88878\n",
      "Epoch 172 of 2000. Loss per movie: 0.895613125048126, test rmse: 0.92742, train rmse: 0.88834\n",
      "Epoch 173 of 2000. Loss per movie: 0.8947206698047987, test rmse: 0.92714, train rmse: 0.88789\n",
      "Epoch 174 of 2000. Loss per movie: 0.8938340695661257, test rmse: 0.92686, train rmse: 0.88745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175 of 2000. Loss per movie: 0.8929533744396976, test rmse: 0.92659, train rmse: 0.88702\n",
      "Epoch 176 of 2000. Loss per movie: 0.8920786444554005, test rmse: 0.92632, train rmse: 0.88658\n",
      "Epoch 177 of 2000. Loss per movie: 0.8912098502715902, test rmse: 0.92605, train rmse: 0.88615\n",
      "Epoch 178 of 2000. Loss per movie: 0.8903470340580208, test rmse: 0.92578, train rmse: 0.88572\n",
      "Epoch 179 of 2000. Loss per movie: 0.889490255206717, test rmse: 0.92552, train rmse: 0.88529\n",
      "Epoch 180 of 2000. Loss per movie: 0.8886395587223747, test rmse: 0.92526, train rmse: 0.88487\n",
      "Epoch 181 of 2000. Loss per movie: 0.887794920224497, test rmse: 0.92500, train rmse: 0.88445\n",
      "Epoch 182 of 2000. Loss per movie: 0.8869564105866213, test rmse: 0.92475, train rmse: 0.88403\n",
      "Epoch 183 of 2000. Loss per movie: 0.8861240954376429, test rmse: 0.92450, train rmse: 0.88362\n",
      "Epoch 184 of 2000. Loss per movie: 0.8852979371437136, test rmse: 0.92425, train rmse: 0.88321\n",
      "Epoch 185 of 2000. Loss per movie: 0.8844780490316194, test rmse: 0.92400, train rmse: 0.88280\n",
      "Epoch 186 of 2000. Loss per movie: 0.8836643177745742, test rmse: 0.92376, train rmse: 0.88239\n",
      "Epoch 187 of 2000. Loss per movie: 0.8828568301926611, test rmse: 0.92352, train rmse: 0.88199\n",
      "Epoch 188 of 2000. Loss per movie: 0.8820554968434764, test rmse: 0.92328, train rmse: 0.88159\n",
      "Epoch 189 of 2000. Loss per movie: 0.8812603130493663, test rmse: 0.92305, train rmse: 0.88119\n",
      "Epoch 190 of 2000. Loss per movie: 0.8804712116930913, test rmse: 0.92282, train rmse: 0.88080\n",
      "Epoch 191 of 2000. Loss per movie: 0.8796881428796811, test rmse: 0.92259, train rmse: 0.88041\n",
      "Epoch 192 of 2000. Loss per movie: 0.8789111242566466, test rmse: 0.92236, train rmse: 0.88002\n",
      "Epoch 193 of 2000. Loss per movie: 0.8781400788553263, test rmse: 0.92214, train rmse: 0.87963\n",
      "Epoch 194 of 2000. Loss per movie: 0.8773749022081264, test rmse: 0.92192, train rmse: 0.87925\n",
      "Epoch 195 of 2000. Loss per movie: 0.8766155580986694, test rmse: 0.92170, train rmse: 0.87887\n",
      "Epoch 196 of 2000. Loss per movie: 0.8758619778504978, test rmse: 0.92148, train rmse: 0.87849\n",
      "Epoch 197 of 2000. Loss per movie: 0.8751143282999182, test rmse: 0.92127, train rmse: 0.87812\n",
      "Epoch 198 of 2000. Loss per movie: 0.8743730093154273, test rmse: 0.92105, train rmse: 0.87775\n",
      "Epoch 199 of 2000. Loss per movie: 0.8736384023384022, test rmse: 0.92084, train rmse: 0.87738\n",
      "Epoch 200 of 2000. Loss per movie: 0.8729107520242934, test rmse: 0.92063, train rmse: 0.87702\n",
      "Epoch 201 of 2000. Loss per movie: 0.8721899765141654, test rmse: 0.92042, train rmse: 0.87666\n",
      "Epoch 202 of 2000. Loss per movie: 0.8714759162008124, test rmse: 0.92021, train rmse: 0.87630\n",
      "Epoch 203 of 2000. Loss per movie: 0.870768409776064, test rmse: 0.92001, train rmse: 0.87594\n",
      "Epoch 204 of 2000. Loss per movie: 0.8700672675823343, test rmse: 0.91981, train rmse: 0.87559\n",
      "Epoch 205 of 2000. Loss per movie: 0.8693723879869713, test rmse: 0.91960, train rmse: 0.87524\n",
      "Epoch 206 of 2000. Loss per movie: 0.868683650292341, test rmse: 0.91941, train rmse: 0.87490\n",
      "Epoch 207 of 2000. Loss per movie: 0.8680009827744238, test rmse: 0.91921, train rmse: 0.87456\n",
      "Epoch 208 of 2000. Loss per movie: 0.867324362115826, test rmse: 0.91901, train rmse: 0.87422\n",
      "Epoch 209 of 2000. Loss per movie: 0.8666536308355479, test rmse: 0.91882, train rmse: 0.87388\n",
      "Epoch 210 of 2000. Loss per movie: 0.8659887540638093, test rmse: 0.91863, train rmse: 0.87355\n",
      "Epoch 211 of 2000. Loss per movie: 0.8653296057874609, test rmse: 0.91844, train rmse: 0.87321\n",
      "Epoch 212 of 2000. Loss per movie: 0.8646761405056921, test rmse: 0.91825, train rmse: 0.87289\n",
      "Epoch 213 of 2000. Loss per movie: 0.8640281671434463, test rmse: 0.91806, train rmse: 0.87256\n",
      "Epoch 214 of 2000. Loss per movie: 0.8633855866195189, test rmse: 0.91788, train rmse: 0.87224\n",
      "Epoch 215 of 2000. Loss per movie: 0.8627482780945287, test rmse: 0.91769, train rmse: 0.87192\n",
      "Epoch 216 of 2000. Loss per movie: 0.8621160261129227, test rmse: 0.91751, train rmse: 0.87160\n",
      "Epoch 217 of 2000. Loss per movie: 0.8614887620691171, test rmse: 0.91733, train rmse: 0.87128\n",
      "Epoch 218 of 2000. Loss per movie: 0.8608664238070194, test rmse: 0.91716, train rmse: 0.87097\n",
      "Epoch 219 of 2000. Loss per movie: 0.8602490546303612, test rmse: 0.91698, train rmse: 0.87066\n",
      "Epoch 220 of 2000. Loss per movie: 0.8596369616341789, test rmse: 0.91680, train rmse: 0.87035\n",
      "Epoch 221 of 2000. Loss per movie: 0.859030260838453, test rmse: 0.91663, train rmse: 0.87004\n",
      "Epoch 222 of 2000. Loss per movie: 0.8584290226914795, test rmse: 0.91646, train rmse: 0.86974\n",
      "Epoch 223 of 2000. Loss per movie: 0.8578331836196954, test rmse: 0.91629, train rmse: 0.86944\n",
      "Epoch 224 of 2000. Loss per movie: 0.8572426043566002, test rmse: 0.91612, train rmse: 0.86914\n",
      "Epoch 225 of 2000. Loss per movie: 0.8566572442207835, test rmse: 0.91595, train rmse: 0.86885\n",
      "Epoch 226 of 2000. Loss per movie: 0.856076961323424, test rmse: 0.91579, train rmse: 0.86855\n",
      "Epoch 227 of 2000. Loss per movie: 0.8555016825939047, test rmse: 0.91562, train rmse: 0.86826\n",
      "Epoch 228 of 2000. Loss per movie: 0.8549312820190765, test rmse: 0.91546, train rmse: 0.86797\n",
      "Epoch 229 of 2000. Loss per movie: 0.8543656990729388, test rmse: 0.91530, train rmse: 0.86769\n",
      "Epoch 230 of 2000. Loss per movie: 0.8538048593382773, test rmse: 0.91514, train rmse: 0.86741\n",
      "Epoch 231 of 2000. Loss per movie: 0.8532486448106528, test rmse: 0.91498, train rmse: 0.86712\n",
      "Epoch 232 of 2000. Loss per movie: 0.8526970060912097, test rmse: 0.91482, train rmse: 0.86684\n",
      "Epoch 233 of 2000. Loss per movie: 0.8521498328298508, test rmse: 0.91467, train rmse: 0.86657\n",
      "Epoch 234 of 2000. Loss per movie: 0.8516070608860247, test rmse: 0.91451, train rmse: 0.86629\n",
      "Epoch 235 of 2000. Loss per movie: 0.8510686001085923, test rmse: 0.91436, train rmse: 0.86602\n",
      "Epoch 236 of 2000. Loss per movie: 0.8505343824589578, test rmse: 0.91421, train rmse: 0.86575\n",
      "Epoch 237 of 2000. Loss per movie: 0.8500043462062704, test rmse: 0.91406, train rmse: 0.86548\n",
      "Epoch 238 of 2000. Loss per movie: 0.8494783675344606, test rmse: 0.91391, train rmse: 0.86522\n",
      "Epoch 239 of 2000. Loss per movie: 0.848956440206657, test rmse: 0.91376, train rmse: 0.86495\n",
      "Epoch 240 of 2000. Loss per movie: 0.8484384417533875, test rmse: 0.91362, train rmse: 0.86469\n",
      "Epoch 241 of 2000. Loss per movie: 0.8479243067583772, test rmse: 0.91347, train rmse: 0.86443\n",
      "Epoch 242 of 2000. Loss per movie: 0.8474139960994335, test rmse: 0.91333, train rmse: 0.86417\n",
      "Epoch 243 of 2000. Loss per movie: 0.8469074093487543, test rmse: 0.91319, train rmse: 0.86391\n",
      "Epoch 244 of 2000. Loss per movie: 0.8464045341743442, test rmse: 0.91304, train rmse: 0.86366\n",
      "Epoch 245 of 2000. Loss per movie: 0.8459052931823005, test rmse: 0.91290, train rmse: 0.86340\n",
      "Epoch 246 of 2000. Loss per movie: 0.8454096185466481, test rmse: 0.91276, train rmse: 0.86315\n",
      "Epoch 247 of 2000. Loss per movie: 0.8449174785160423, test rmse: 0.91263, train rmse: 0.86290\n",
      "Epoch 248 of 2000. Loss per movie: 0.8444289007311627, test rmse: 0.91249, train rmse: 0.86265\n",
      "Epoch 249 of 2000. Loss per movie: 0.8439437319634219, test rmse: 0.91236, train rmse: 0.86241\n",
      "Epoch 250 of 2000. Loss per movie: 0.8434620250844785, test rmse: 0.91222, train rmse: 0.86216\n",
      "Epoch 251 of 2000. Loss per movie: 0.8429837356566249, test rmse: 0.91209, train rmse: 0.86192\n",
      "Epoch 252 of 2000. Loss per movie: 0.8425088570177485, test rmse: 0.91196, train rmse: 0.86168\n",
      "Epoch 253 of 2000. Loss per movie: 0.8420373355165814, test rmse: 0.91183, train rmse: 0.86144\n",
      "Epoch 254 of 2000. Loss per movie: 0.8415691773899954, test rmse: 0.91170, train rmse: 0.86120\n",
      "Epoch 255 of 2000. Loss per movie: 0.8411043384129029, test rmse: 0.91157, train rmse: 0.86096\n",
      "Epoch 256 of 2000. Loss per movie: 0.8406428416192036, test rmse: 0.91144, train rmse: 0.86073\n",
      "Epoch 257 of 2000. Loss per movie: 0.840184658446862, test rmse: 0.91132, train rmse: 0.86049\n",
      "Epoch 258 of 2000. Loss per movie: 0.8397296966185326, test rmse: 0.91119, train rmse: 0.86026\n",
      "Epoch 259 of 2000. Loss per movie: 0.839278016093228, test rmse: 0.91107, train rmse: 0.86003\n",
      "Epoch 260 of 2000. Loss per movie: 0.8388295513837998, test rmse: 0.91095, train rmse: 0.85980\n",
      "Epoch 261 of 2000. Loss per movie: 0.8383842993009388, test rmse: 0.91082, train rmse: 0.85957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 262 of 2000. Loss per movie: 0.8379421752925151, test rmse: 0.91070, train rmse: 0.85935\n",
      "Epoch 263 of 2000. Loss per movie: 0.8375032221661454, test rmse: 0.91058, train rmse: 0.85913\n",
      "Epoch 264 of 2000. Loss per movie: 0.83706733354065, test rmse: 0.91047, train rmse: 0.85890\n",
      "Epoch 265 of 2000. Loss per movie: 0.8366345187004625, test rmse: 0.91035, train rmse: 0.85868\n",
      "Epoch 266 of 2000. Loss per movie: 0.8362047232147063, test rmse: 0.91023, train rmse: 0.85846\n",
      "Epoch 267 of 2000. Loss per movie: 0.8357779395707864, test rmse: 0.91012, train rmse: 0.85824\n",
      "Epoch 268 of 2000. Loss per movie: 0.8353540845631703, test rmse: 0.91000, train rmse: 0.85803\n",
      "Epoch 269 of 2000. Loss per movie: 0.8349331531598366, test rmse: 0.90989, train rmse: 0.85781\n",
      "Epoch 270 of 2000. Loss per movie: 0.8345150910716561, test rmse: 0.90978, train rmse: 0.85760\n",
      "Epoch 271 of 2000. Loss per movie: 0.8340998460648318, test rmse: 0.90967, train rmse: 0.85739\n",
      "Epoch 272 of 2000. Loss per movie: 0.8336873984365203, test rmse: 0.90956, train rmse: 0.85718\n",
      "Epoch 273 of 2000. Loss per movie: 0.83327772912174, test rmse: 0.90945, train rmse: 0.85697\n",
      "Epoch 274 of 2000. Loss per movie: 0.8328707517256493, test rmse: 0.90934, train rmse: 0.85676\n",
      "Epoch 275 of 2000. Loss per movie: 0.8324664769501522, test rmse: 0.90924, train rmse: 0.85655\n",
      "Epoch 276 of 2000. Loss per movie: 0.8320648500100044, test rmse: 0.90913, train rmse: 0.85635\n",
      "Epoch 277 of 2000. Loss per movie: 0.831665830932531, test rmse: 0.90903, train rmse: 0.85614\n",
      "Epoch 278 of 2000. Loss per movie: 0.8312693564985368, test rmse: 0.90892, train rmse: 0.85594\n",
      "Epoch 279 of 2000. Loss per movie: 0.8308754374807995, test rmse: 0.90882, train rmse: 0.85574\n",
      "Epoch 280 of 2000. Loss per movie: 0.830484010943618, test rmse: 0.90872, train rmse: 0.85554\n",
      "Epoch 281 of 2000. Loss per movie: 0.8300950444269124, test rmse: 0.90862, train rmse: 0.85534\n",
      "Epoch 282 of 2000. Loss per movie: 0.82970856372865, test rmse: 0.90851, train rmse: 0.85514\n",
      "Epoch 283 of 2000. Loss per movie: 0.8293244412055906, test rmse: 0.90842, train rmse: 0.85494\n",
      "Epoch 284 of 2000. Loss per movie: 0.8289427229964067, test rmse: 0.90832, train rmse: 0.85475\n",
      "Epoch 285 of 2000. Loss per movie: 0.8285633322033106, test rmse: 0.90822, train rmse: 0.85455\n",
      "Epoch 286 of 2000. Loss per movie: 0.8281862739291967, test rmse: 0.90812, train rmse: 0.85436\n",
      "Epoch 287 of 2000. Loss per movie: 0.8278114814111934, test rmse: 0.90803, train rmse: 0.85417\n",
      "Epoch 288 of 2000. Loss per movie: 0.8274390005044789, test rmse: 0.90793, train rmse: 0.85397\n",
      "Epoch 289 of 2000. Loss per movie: 0.827068701723101, test rmse: 0.90784, train rmse: 0.85378\n",
      "Epoch 290 of 2000. Loss per movie: 0.8267006342532943, test rmse: 0.90774, train rmse: 0.85360\n",
      "Epoch 291 of 2000. Loss per movie: 0.8263347954018646, test rmse: 0.90765, train rmse: 0.85341\n",
      "Epoch 292 of 2000. Loss per movie: 0.8259710768740469, test rmse: 0.90756, train rmse: 0.85322\n",
      "Epoch 293 of 2000. Loss per movie: 0.8256095088619682, test rmse: 0.90747, train rmse: 0.85304\n",
      "Epoch 294 of 2000. Loss per movie: 0.8252500270124564, test rmse: 0.90737, train rmse: 0.85285\n",
      "Epoch 295 of 2000. Loss per movie: 0.8248926454293458, test rmse: 0.90728, train rmse: 0.85267\n",
      "Epoch 296 of 2000. Loss per movie: 0.8245373284632471, test rmse: 0.90719, train rmse: 0.85248\n",
      "Epoch 297 of 2000. Loss per movie: 0.8241840289832579, test rmse: 0.90711, train rmse: 0.85230\n",
      "Epoch 298 of 2000. Loss per movie: 0.8238327281370175, test rmse: 0.90702, train rmse: 0.85212\n",
      "Epoch 299 of 2000. Loss per movie: 0.8234834270585023, test rmse: 0.90693, train rmse: 0.85194\n",
      "Epoch 300 of 2000. Loss per movie: 0.823136059197461, test rmse: 0.90684, train rmse: 0.85176\n",
      "Epoch 301 of 2000. Loss per movie: 0.8227906083947272, test rmse: 0.90676, train rmse: 0.85159\n",
      "Epoch 302 of 2000. Loss per movie: 0.8224470477183565, test rmse: 0.90667, train rmse: 0.85141\n",
      "Epoch 303 of 2000. Loss per movie: 0.8221053866654032, test rmse: 0.90659, train rmse: 0.85123\n",
      "Epoch 304 of 2000. Loss per movie: 0.8217655289187301, test rmse: 0.90650, train rmse: 0.85106\n",
      "Epoch 305 of 2000. Loss per movie: 0.8214274788016229, test rmse: 0.90642, train rmse: 0.85088\n",
      "Epoch 306 of 2000. Loss per movie: 0.8210912796886866, test rmse: 0.90633, train rmse: 0.85071\n",
      "Epoch 307 of 2000. Loss per movie: 0.8207568181113879, test rmse: 0.90625, train rmse: 0.85054\n",
      "Epoch 308 of 2000. Loss per movie: 0.8204241343967696, test rmse: 0.90617, train rmse: 0.85037\n",
      "Epoch 309 of 2000. Loss per movie: 0.8200931520722851, test rmse: 0.90609, train rmse: 0.85020\n",
      "Epoch 310 of 2000. Loss per movie: 0.8197639108979886, test rmse: 0.90600, train rmse: 0.85003\n",
      "Epoch 311 of 2000. Loss per movie: 0.8194363599866806, test rmse: 0.90592, train rmse: 0.84986\n",
      "Epoch 312 of 2000. Loss per movie: 0.8191104943772134, test rmse: 0.90584, train rmse: 0.84969\n",
      "Epoch 313 of 2000. Loss per movie: 0.8187862873502634, test rmse: 0.90576, train rmse: 0.84952\n",
      "Epoch 314 of 2000. Loss per movie: 0.8184637635698216, test rmse: 0.90568, train rmse: 0.84935\n",
      "Epoch 315 of 2000. Loss per movie: 0.8181428593205782, test rmse: 0.90560, train rmse: 0.84919\n",
      "Epoch 316 of 2000. Loss per movie: 0.8178235952976058, test rmse: 0.90553, train rmse: 0.84902\n",
      "Epoch 317 of 2000. Loss per movie: 0.8175059372689862, test rmse: 0.90545, train rmse: 0.84886\n",
      "Epoch 318 of 2000. Loss per movie: 0.8171898390251732, test rmse: 0.90537, train rmse: 0.84870\n",
      "Epoch 319 of 2000. Loss per movie: 0.8168753625096379, test rmse: 0.90530, train rmse: 0.84853\n",
      "Epoch 320 of 2000. Loss per movie: 0.8165623917024002, test rmse: 0.90522, train rmse: 0.84837\n",
      "Epoch 321 of 2000. Loss per movie: 0.8162509820974397, test rmse: 0.90514, train rmse: 0.84821\n",
      "Epoch 322 of 2000. Loss per movie: 0.8159410918793696, test rmse: 0.90507, train rmse: 0.84805\n",
      "Epoch 323 of 2000. Loss per movie: 0.8156327214734308, test rmse: 0.90499, train rmse: 0.84789\n",
      "Epoch 324 of 2000. Loss per movie: 0.8153258132594378, test rmse: 0.90492, train rmse: 0.84773\n",
      "Epoch 325 of 2000. Loss per movie: 0.815020386444119, test rmse: 0.90484, train rmse: 0.84757\n",
      "Epoch 326 of 2000. Loss per movie: 0.8147164347906033, test rmse: 0.90477, train rmse: 0.84742\n",
      "Epoch 327 of 2000. Loss per movie: 0.8144139046476231, test rmse: 0.90470, train rmse: 0.84726\n",
      "Epoch 328 of 2000. Loss per movie: 0.8141127797851381, test rmse: 0.90463, train rmse: 0.84711\n",
      "Epoch 329 of 2000. Loss per movie: 0.8138130651642962, test rmse: 0.90455, train rmse: 0.84695\n",
      "Epoch 330 of 2000. Loss per movie: 0.8135147634782915, test rmse: 0.90448, train rmse: 0.84680\n",
      "Epoch 331 of 2000. Loss per movie: 0.8132178445349976, test rmse: 0.90441, train rmse: 0.84664\n",
      "Epoch 332 of 2000. Loss per movie: 0.8129222933092242, test rmse: 0.90434, train rmse: 0.84649\n",
      "Epoch 333 of 2000. Loss per movie: 0.8126281131320278, test rmse: 0.90427, train rmse: 0.84634\n",
      "Epoch 334 of 2000. Loss per movie: 0.8123352876316211, test rmse: 0.90420, train rmse: 0.84618\n",
      "Epoch 335 of 2000. Loss per movie: 0.8120437993022406, test rmse: 0.90413, train rmse: 0.84603\n",
      "Epoch 336 of 2000. Loss per movie: 0.8117536387177058, test rmse: 0.90406, train rmse: 0.84588\n",
      "Epoch 337 of 2000. Loss per movie: 0.8114648012712368, test rmse: 0.90400, train rmse: 0.84573\n",
      "Epoch 338 of 2000. Loss per movie: 0.81117731935204, test rmse: 0.90393, train rmse: 0.84558\n",
      "Epoch 339 of 2000. Loss per movie: 0.8108911249215197, test rmse: 0.90386, train rmse: 0.84544\n",
      "Epoch 340 of 2000. Loss per movie: 0.810606225350524, test rmse: 0.90379, train rmse: 0.84529\n",
      "Epoch 341 of 2000. Loss per movie: 0.8103226007944622, test rmse: 0.90373, train rmse: 0.84514\n",
      "Epoch 342 of 2000. Loss per movie: 0.8100403221268716, test rmse: 0.90366, train rmse: 0.84499\n",
      "Epoch 343 of 2000. Loss per movie: 0.8097592802733785, test rmse: 0.90360, train rmse: 0.84485\n",
      "Epoch 344 of 2000. Loss per movie: 0.8094795253415737, test rmse: 0.90353, train rmse: 0.84470\n",
      "Epoch 345 of 2000. Loss per movie: 0.8092010625052254, test rmse: 0.90347, train rmse: 0.84456\n",
      "Epoch 346 of 2000. Loss per movie: 0.8089238027471708, test rmse: 0.90340, train rmse: 0.84441\n",
      "Epoch 347 of 2000. Loss per movie: 0.8086478579058517, test rmse: 0.90334, train rmse: 0.84427\n",
      "Epoch 348 of 2000. Loss per movie: 0.8083731359165433, test rmse: 0.90327, train rmse: 0.84413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349 of 2000. Loss per movie: 0.808099657049077, test rmse: 0.90321, train rmse: 0.84399\n",
      "Epoch 350 of 2000. Loss per movie: 0.807827414570467, test rmse: 0.90315, train rmse: 0.84385\n",
      "Epoch 351 of 2000. Loss per movie: 0.8075563781468394, test rmse: 0.90309, train rmse: 0.84370\n",
      "Epoch 352 of 2000. Loss per movie: 0.8072865677645317, test rmse: 0.90303, train rmse: 0.84356\n",
      "Epoch 353 of 2000. Loss per movie: 0.807017983848785, test rmse: 0.90297, train rmse: 0.84342\n",
      "Epoch 354 of 2000. Loss per movie: 0.8067505981919315, test rmse: 0.90290, train rmse: 0.84329\n",
      "Epoch 355 of 2000. Loss per movie: 0.806484397044505, test rmse: 0.90284, train rmse: 0.84315\n",
      "Epoch 356 of 2000. Loss per movie: 0.8062193648143274, test rmse: 0.90278, train rmse: 0.84301\n",
      "Epoch 357 of 2000. Loss per movie: 0.8059555576332401, test rmse: 0.90273, train rmse: 0.84287\n",
      "Epoch 358 of 2000. Loss per movie: 0.8056928694035579, test rmse: 0.90267, train rmse: 0.84274\n",
      "Epoch 359 of 2000. Loss per movie: 0.8054313500911245, test rmse: 0.90261, train rmse: 0.84260\n",
      "Epoch 360 of 2000. Loss per movie: 0.8051709812688204, test rmse: 0.90255, train rmse: 0.84246\n",
      "Epoch 361 of 2000. Loss per movie: 0.8049117787414442, test rmse: 0.90249, train rmse: 0.84233\n",
      "Epoch 362 of 2000. Loss per movie: 0.8046537101197895, test rmse: 0.90243, train rmse: 0.84219\n",
      "Epoch 363 of 2000. Loss per movie: 0.8043967674660201, test rmse: 0.90238, train rmse: 0.84206\n",
      "Epoch 364 of 2000. Loss per movie: 0.8041409204462621, test rmse: 0.90232, train rmse: 0.84193\n",
      "Epoch 365 of 2000. Loss per movie: 0.8038862075448462, test rmse: 0.90227, train rmse: 0.84179\n",
      "Epoch 366 of 2000. Loss per movie: 0.8036325791502963, test rmse: 0.90221, train rmse: 0.84166\n",
      "Epoch 367 of 2000. Loss per movie: 0.8033800666595895, test rmse: 0.90215, train rmse: 0.84153\n",
      "Epoch 368 of 2000. Loss per movie: 0.8031286232961911, test rmse: 0.90210, train rmse: 0.84140\n",
      "Epoch 369 of 2000. Loss per movie: 0.8028782465086539, test rmse: 0.90205, train rmse: 0.84127\n",
      "Epoch 370 of 2000. Loss per movie: 0.8026289615988306, test rmse: 0.90199, train rmse: 0.84114\n",
      "Epoch 371 of 2000. Loss per movie: 0.8023807016621022, test rmse: 0.90194, train rmse: 0.84101\n",
      "Epoch 372 of 2000. Loss per movie: 0.8021335371467645, test rmse: 0.90188, train rmse: 0.84088\n",
      "Epoch 373 of 2000. Loss per movie: 0.8018873744288749, test rmse: 0.90183, train rmse: 0.84075\n",
      "Epoch 374 of 2000. Loss per movie: 0.8016422708451252, test rmse: 0.90178, train rmse: 0.84063\n",
      "Epoch 375 of 2000. Loss per movie: 0.8013982071887865, test rmse: 0.90173, train rmse: 0.84050\n",
      "Epoch 376 of 2000. Loss per movie: 0.8011551531968587, test rmse: 0.90168, train rmse: 0.84037\n",
      "Epoch 377 of 2000. Loss per movie: 0.800913109578077, test rmse: 0.90162, train rmse: 0.84024\n",
      "Epoch 378 of 2000. Loss per movie: 0.8006720851916337, test rmse: 0.90157, train rmse: 0.84012\n",
      "Epoch 379 of 2000. Loss per movie: 0.800432053034711, test rmse: 0.90152, train rmse: 0.83999\n",
      "Epoch 380 of 2000. Loss per movie: 0.8001930043189903, test rmse: 0.90147, train rmse: 0.83987\n",
      "Epoch 381 of 2000. Loss per movie: 0.7999549975151398, test rmse: 0.90142, train rmse: 0.83974\n",
      "Epoch 382 of 2000. Loss per movie: 0.7997179186585117, test rmse: 0.90137, train rmse: 0.83962\n",
      "Epoch 383 of 2000. Loss per movie: 0.7994818491828002, test rmse: 0.90132, train rmse: 0.83950\n",
      "Epoch 384 of 2000. Loss per movie: 0.799246779661825, test rmse: 0.90128, train rmse: 0.83937\n",
      "Epoch 385 of 2000. Loss per movie: 0.799012676997644, test rmse: 0.90123, train rmse: 0.83925\n",
      "Epoch 386 of 2000. Loss per movie: 0.7987795512542997, test rmse: 0.90118, train rmse: 0.83913\n",
      "Epoch 387 of 2000. Loss per movie: 0.7985473816658456, test rmse: 0.90113, train rmse: 0.83901\n",
      "Epoch 388 of 2000. Loss per movie: 0.7983161812021389, test rmse: 0.90108, train rmse: 0.83889\n",
      "Epoch 389 of 2000. Loss per movie: 0.7980859546825804, test rmse: 0.90104, train rmse: 0.83877\n",
      "Epoch 390 of 2000. Loss per movie: 0.797856684176165, test rmse: 0.90099, train rmse: 0.83865\n",
      "Epoch 391 of 2000. Loss per movie: 0.7976283476412226, test rmse: 0.90095, train rmse: 0.83853\n",
      "Epoch 392 of 2000. Loss per movie: 0.7974009543621866, test rmse: 0.90090, train rmse: 0.83841\n",
      "Epoch 393 of 2000. Loss per movie: 0.7971745183720175, test rmse: 0.90085, train rmse: 0.83829\n",
      "Epoch 394 of 2000. Loss per movie: 0.7969490127387711, test rmse: 0.90081, train rmse: 0.83817\n",
      "Epoch 395 of 2000. Loss per movie: 0.7967244381711827, test rmse: 0.90076, train rmse: 0.83806\n",
      "Epoch 396 of 2000. Loss per movie: 0.7965007828333716, test rmse: 0.90072, train rmse: 0.83794\n",
      "Epoch 397 of 2000. Loss per movie: 0.7962780319127685, test rmse: 0.90068, train rmse: 0.83782\n",
      "Epoch 398 of 2000. Loss per movie: 0.7960561985209779, test rmse: 0.90063, train rmse: 0.83771\n",
      "Epoch 399 of 2000. Loss per movie: 0.7958353010851195, test rmse: 0.90059, train rmse: 0.83759\n",
      "Epoch 400 of 2000. Loss per movie: 0.7956152911985672, test rmse: 0.90055, train rmse: 0.83748\n",
      "Epoch 401 of 2000. Loss per movie: 0.7953961707749067, test rmse: 0.90050, train rmse: 0.83736\n",
      "Epoch 402 of 2000. Loss per movie: 0.7951779631315316, test rmse: 0.90046, train rmse: 0.83725\n",
      "Epoch 403 of 2000. Loss per movie: 0.7949606037026495, test rmse: 0.90042, train rmse: 0.83713\n",
      "Epoch 404 of 2000. Loss per movie: 0.7947441771821375, test rmse: 0.90038, train rmse: 0.83702\n",
      "Epoch 405 of 2000. Loss per movie: 0.7945286272964069, test rmse: 0.90034, train rmse: 0.83691\n",
      "Epoch 406 of 2000. Loss per movie: 0.7943139079067851, test rmse: 0.90029, train rmse: 0.83679\n",
      "Epoch 407 of 2000. Loss per movie: 0.7941001060459758, test rmse: 0.90025, train rmse: 0.83668\n",
      "Epoch 408 of 2000. Loss per movie: 0.7938871571481866, test rmse: 0.90021, train rmse: 0.83657\n",
      "Epoch 409 of 2000. Loss per movie: 0.7936750601503143, test rmse: 0.90017, train rmse: 0.83646\n",
      "Epoch 410 of 2000. Loss per movie: 0.7934638618288936, test rmse: 0.90013, train rmse: 0.83635\n",
      "Epoch 411 of 2000. Loss per movie: 0.7932535159035047, test rmse: 0.90009, train rmse: 0.83624\n",
      "Epoch 412 of 2000. Loss per movie: 0.7930440077033253, test rmse: 0.90005, train rmse: 0.83613\n",
      "Epoch 413 of 2000. Loss per movie: 0.7928353455914329, test rmse: 0.90001, train rmse: 0.83602\n",
      "Epoch 414 of 2000. Loss per movie: 0.7926275494832913, test rmse: 0.89998, train rmse: 0.83591\n",
      "Epoch 415 of 2000. Loss per movie: 0.7924205873440618, test rmse: 0.89994, train rmse: 0.83580\n",
      "Epoch 416 of 2000. Loss per movie: 0.792214451448529, test rmse: 0.89990, train rmse: 0.83569\n",
      "Epoch 417 of 2000. Loss per movie: 0.7920091810606322, test rmse: 0.89986, train rmse: 0.83558\n",
      "Epoch 418 of 2000. Loss per movie: 0.7918047274902517, test rmse: 0.89982, train rmse: 0.83548\n",
      "Epoch 419 of 2000. Loss per movie: 0.7916011057625769, test rmse: 0.89979, train rmse: 0.83537\n",
      "Epoch 420 of 2000. Loss per movie: 0.7913983033329922, test rmse: 0.89975, train rmse: 0.83526\n",
      "Epoch 421 of 2000. Loss per movie: 0.7911963265092421, test rmse: 0.89971, train rmse: 0.83516\n",
      "Epoch 422 of 2000. Loss per movie: 0.7909951665030082, test rmse: 0.89968, train rmse: 0.83505\n",
      "Epoch 423 of 2000. Loss per movie: 0.790794808359974, test rmse: 0.89964, train rmse: 0.83495\n",
      "Epoch 424 of 2000. Loss per movie: 0.7905952595927344, test rmse: 0.89961, train rmse: 0.83484\n",
      "Epoch 425 of 2000. Loss per movie: 0.7903964975926312, test rmse: 0.89957, train rmse: 0.83474\n",
      "Epoch 426 of 2000. Loss per movie: 0.7901985334868097, test rmse: 0.89953, train rmse: 0.83463\n",
      "Epoch 427 of 2000. Loss per movie: 0.7900013759927149, test rmse: 0.89950, train rmse: 0.83453\n",
      "Epoch 428 of 2000. Loss per movie: 0.789804998745391, test rmse: 0.89947, train rmse: 0.83443\n",
      "Epoch 429 of 2000. Loss per movie: 0.7896093977759199, test rmse: 0.89943, train rmse: 0.83432\n",
      "Epoch 430 of 2000. Loss per movie: 0.7894145566416411, test rmse: 0.89940, train rmse: 0.83422\n",
      "Epoch 431 of 2000. Loss per movie: 0.7892204871075615, test rmse: 0.89936, train rmse: 0.83412\n",
      "Epoch 432 of 2000. Loss per movie: 0.7890271803853628, test rmse: 0.89933, train rmse: 0.83402\n",
      "Epoch 433 of 2000. Loss per movie: 0.7888346397352275, test rmse: 0.89930, train rmse: 0.83392\n",
      "Epoch 434 of 2000. Loss per movie: 0.7886428428319915, test rmse: 0.89926, train rmse: 0.83382\n",
      "Epoch 435 of 2000. Loss per movie: 0.7884517743669707, test rmse: 0.89923, train rmse: 0.83372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 436 of 2000. Loss per movie: 0.7882614853691119, test rmse: 0.89920, train rmse: 0.83362\n",
      "Epoch 437 of 2000. Loss per movie: 0.7880719197774472, test rmse: 0.89917, train rmse: 0.83352\n",
      "Epoch 438 of 2000. Loss per movie: 0.7878830785842061, test rmse: 0.89913, train rmse: 0.83342\n",
      "Epoch 439 of 2000. Loss per movie: 0.7876949975805249, test rmse: 0.89910, train rmse: 0.83332\n",
      "Epoch 440 of 2000. Loss per movie: 0.7875075983093979, test rmse: 0.89907, train rmse: 0.83322\n",
      "Epoch 441 of 2000. Loss per movie: 0.787320950156018, test rmse: 0.89904, train rmse: 0.83312\n",
      "Epoch 442 of 2000. Loss per movie: 0.787135012864216, test rmse: 0.89901, train rmse: 0.83302\n",
      "Epoch 443 of 2000. Loss per movie: 0.786949776653444, test rmse: 0.89898, train rmse: 0.83293\n",
      "Epoch 444 of 2000. Loss per movie: 0.7867652588168449, test rmse: 0.89895, train rmse: 0.83283\n",
      "Epoch 445 of 2000. Loss per movie: 0.7865814488651353, test rmse: 0.89892, train rmse: 0.83273\n",
      "Epoch 446 of 2000. Loss per movie: 0.7863983519720833, test rmse: 0.89889, train rmse: 0.83264\n",
      "Epoch 447 of 2000. Loss per movie: 0.7862159530416255, test rmse: 0.89886, train rmse: 0.83254\n",
      "Epoch 448 of 2000. Loss per movie: 0.7860342552630711, test rmse: 0.89883, train rmse: 0.83244\n",
      "Epoch 449 of 2000. Loss per movie: 0.7858532281607992, test rmse: 0.89880, train rmse: 0.83235\n",
      "Epoch 450 of 2000. Loss per movie: 0.7856729591927546, test rmse: 0.89877, train rmse: 0.83225\n",
      "Epoch 451 of 2000. Loss per movie: 0.7854933338981747, test rmse: 0.89874, train rmse: 0.83216\n",
      "Epoch 452 of 2000. Loss per movie: 0.7853144298127092, test rmse: 0.89871, train rmse: 0.83206\n",
      "Epoch 453 of 2000. Loss per movie: 0.7851362013646737, test rmse: 0.89868, train rmse: 0.83197\n",
      "Epoch 454 of 2000. Loss per movie: 0.7849586828568602, test rmse: 0.89865, train rmse: 0.83188\n",
      "Epoch 455 of 2000. Loss per movie: 0.7847818248904134, test rmse: 0.89862, train rmse: 0.83178\n",
      "Epoch 456 of 2000. Loss per movie: 0.7846056747379825, test rmse: 0.89860, train rmse: 0.83169\n",
      "Epoch 457 of 2000. Loss per movie: 0.7844301949074662, test rmse: 0.89857, train rmse: 0.83160\n",
      "Epoch 458 of 2000. Loss per movie: 0.7842554032589961, test rmse: 0.89854, train rmse: 0.83151\n",
      "Epoch 459 of 2000. Loss per movie: 0.7840812687499628, test rmse: 0.89851, train rmse: 0.83141\n",
      "Epoch 460 of 2000. Loss per movie: 0.7839078587102266, test rmse: 0.89849, train rmse: 0.83132\n",
      "Epoch 461 of 2000. Loss per movie: 0.7837350888711518, test rmse: 0.89846, train rmse: 0.83123\n",
      "Epoch 462 of 2000. Loss per movie: 0.7835629916219451, test rmse: 0.89843, train rmse: 0.83114\n",
      "Epoch 463 of 2000. Loss per movie: 0.7833915688761917, test rmse: 0.89841, train rmse: 0.83105\n",
      "Epoch 464 of 2000. Loss per movie: 0.783220817373709, test rmse: 0.89838, train rmse: 0.83096\n",
      "Epoch 465 of 2000. Loss per movie: 0.7830507389572089, test rmse: 0.89835, train rmse: 0.83087\n",
      "Epoch 466 of 2000. Loss per movie: 0.7828813098840566, test rmse: 0.89833, train rmse: 0.83078\n",
      "Epoch 467 of 2000. Loss per movie: 0.7827125240591278, test rmse: 0.89830, train rmse: 0.83069\n",
      "Epoch 468 of 2000. Loss per movie: 0.782544400547404, test rmse: 0.89828, train rmse: 0.83060\n",
      "Epoch 469 of 2000. Loss per movie: 0.7823769197877891, test rmse: 0.89825, train rmse: 0.83051\n",
      "Epoch 470 of 2000. Loss per movie: 0.7822100994986672, test rmse: 0.89823, train rmse: 0.83043\n",
      "Epoch 471 of 2000. Loss per movie: 0.7820439488227245, test rmse: 0.89820, train rmse: 0.83034\n",
      "Epoch 472 of 2000. Loss per movie: 0.7818784031941689, test rmse: 0.89818, train rmse: 0.83025\n",
      "Epoch 473 of 2000. Loss per movie: 0.78171349727016, test rmse: 0.89816, train rmse: 0.83016\n",
      "Epoch 474 of 2000. Loss per movie: 0.7815492423904636, test rmse: 0.89813, train rmse: 0.83008\n",
      "Epoch 475 of 2000. Loss per movie: 0.7813856142454567, test rmse: 0.89811, train rmse: 0.82999\n",
      "Epoch 476 of 2000. Loss per movie: 0.781222606102153, test rmse: 0.89808, train rmse: 0.82990\n",
      "Epoch 477 of 2000. Loss per movie: 0.7810602273867332, test rmse: 0.89806, train rmse: 0.82982\n",
      "Epoch 478 of 2000. Loss per movie: 0.7808984535060797, test rmse: 0.89804, train rmse: 0.82973\n",
      "Epoch 479 of 2000. Loss per movie: 0.7807373130222279, test rmse: 0.89801, train rmse: 0.82965\n",
      "Epoch 480 of 2000. Loss per movie: 0.7805767698605477, test rmse: 0.89799, train rmse: 0.82956\n",
      "Epoch 481 of 2000. Loss per movie: 0.7804168585364515, test rmse: 0.89797, train rmse: 0.82948\n",
      "Epoch 482 of 2000. Loss per movie: 0.7802575390772644, test rmse: 0.89795, train rmse: 0.82939\n",
      "Epoch 483 of 2000. Loss per movie: 0.780098824240223, test rmse: 0.89792, train rmse: 0.82931\n",
      "Epoch 484 of 2000. Loss per movie: 0.7799406937554957, test rmse: 0.89790, train rmse: 0.82923\n",
      "Epoch 485 of 2000. Loss per movie: 0.7797831973763056, test rmse: 0.89788, train rmse: 0.82914\n",
      "Epoch 486 of 2000. Loss per movie: 0.779626258346612, test rmse: 0.89786, train rmse: 0.82906\n",
      "Epoch 487 of 2000. Loss per movie: 0.7794699197575256, test rmse: 0.89783, train rmse: 0.82898\n",
      "Epoch 488 of 2000. Loss per movie: 0.7793141470227599, test rmse: 0.89781, train rmse: 0.82890\n",
      "Epoch 489 of 2000. Loss per movie: 0.7791589725315217, test rmse: 0.89779, train rmse: 0.82881\n",
      "Epoch 490 of 2000. Loss per movie: 0.7790043752343703, test rmse: 0.89777, train rmse: 0.82873\n",
      "Epoch 491 of 2000. Loss per movie: 0.7788503556274206, test rmse: 0.89775, train rmse: 0.82865\n",
      "Epoch 492 of 2000. Loss per movie: 0.7786969103796162, test rmse: 0.89773, train rmse: 0.82857\n",
      "Epoch 493 of 2000. Loss per movie: 0.7785440362307745, test rmse: 0.89771, train rmse: 0.82849\n",
      "Epoch 494 of 2000. Loss per movie: 0.7783917306294481, test rmse: 0.89769, train rmse: 0.82841\n",
      "Epoch 495 of 2000. Loss per movie: 0.778239968769899, test rmse: 0.89766, train rmse: 0.82833\n",
      "Epoch 496 of 2000. Loss per movie: 0.7780887794267833, test rmse: 0.89764, train rmse: 0.82825\n",
      "Epoch 497 of 2000. Loss per movie: 0.7779381565758504, test rmse: 0.89762, train rmse: 0.82817\n",
      "Epoch 498 of 2000. Loss per movie: 0.7777880860423928, test rmse: 0.89760, train rmse: 0.82809\n",
      "Epoch 499 of 2000. Loss per movie: 0.7776385807253943, test rmse: 0.89758, train rmse: 0.82801\n",
      "Epoch 500 of 2000. Loss per movie: 0.7774895959745264, test rmse: 0.89756, train rmse: 0.82793\n",
      "Epoch 501 of 2000. Loss per movie: 0.7773411749517734, test rmse: 0.89754, train rmse: 0.82785\n",
      "Epoch 502 of 2000. Loss per movie: 0.7771932985921536, test rmse: 0.89752, train rmse: 0.82777\n",
      "Epoch 503 of 2000. Loss per movie: 0.7770459501695123, test rmse: 0.89750, train rmse: 0.82769\n",
      "Epoch 504 of 2000. Loss per movie: 0.7768991446381659, test rmse: 0.89748, train rmse: 0.82762\n",
      "Epoch 505 of 2000. Loss per movie: 0.7767529084339436, test rmse: 0.89747, train rmse: 0.82754\n",
      "Epoch 506 of 2000. Loss per movie: 0.7766071827318098, test rmse: 0.89745, train rmse: 0.82746\n",
      "Epoch 507 of 2000. Loss per movie: 0.7764619848249075, test rmse: 0.89743, train rmse: 0.82738\n",
      "Epoch 508 of 2000. Loss per movie: 0.7763173131540188, test rmse: 0.89741, train rmse: 0.82731\n",
      "Epoch 509 of 2000. Loss per movie: 0.7761731795550243, test rmse: 0.89739, train rmse: 0.82723\n",
      "Epoch 510 of 2000. Loss per movie: 0.7760295580173361, test rmse: 0.89737, train rmse: 0.82716\n",
      "Epoch 511 of 2000. Loss per movie: 0.7758864789457015, test rmse: 0.89735, train rmse: 0.82708\n",
      "Epoch 512 of 2000. Loss per movie: 0.7757439038557898, test rmse: 0.89733, train rmse: 0.82700\n",
      "Epoch 513 of 2000. Loss per movie: 0.7756018338815778, test rmse: 0.89731, train rmse: 0.82693\n",
      "Epoch 514 of 2000. Loss per movie: 0.7754603273519863, test rmse: 0.89730, train rmse: 0.82685\n",
      "Epoch 515 of 2000. Loss per movie: 0.7753192772479743, test rmse: 0.89728, train rmse: 0.82678\n",
      "Epoch 516 of 2000. Loss per movie: 0.7751787676255569, test rmse: 0.89726, train rmse: 0.82670\n",
      "Epoch 517 of 2000. Loss per movie: 0.7750387774352937, test rmse: 0.89724, train rmse: 0.82663\n",
      "Epoch 518 of 2000. Loss per movie: 0.7748992597589028, test rmse: 0.89722, train rmse: 0.82656\n",
      "Epoch 519 of 2000. Loss per movie: 0.7747602805796475, test rmse: 0.89721, train rmse: 0.82648\n",
      "Epoch 520 of 2000. Loss per movie: 0.7746217741977587, test rmse: 0.89719, train rmse: 0.82641\n",
      "Epoch 521 of 2000. Loss per movie: 0.7744837744907873, test rmse: 0.89717, train rmse: 0.82634\n",
      "Epoch 522 of 2000. Loss per movie: 0.7743462611889017, test rmse: 0.89716, train rmse: 0.82626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 523 of 2000. Loss per movie: 0.7742092406707202, test rmse: 0.89714, train rmse: 0.82619\n",
      "Epoch 524 of 2000. Loss per movie: 0.7740727171886549, test rmse: 0.89712, train rmse: 0.82612\n",
      "Epoch 525 of 2000. Loss per movie: 0.7739366701185065, test rmse: 0.89710, train rmse: 0.82604\n",
      "Epoch 526 of 2000. Loss per movie: 0.773801109595191, test rmse: 0.89709, train rmse: 0.82597\n",
      "Epoch 527 of 2000. Loss per movie: 0.7736660377449145, test rmse: 0.89707, train rmse: 0.82590\n",
      "Epoch 528 of 2000. Loss per movie: 0.773531436778419, test rmse: 0.89705, train rmse: 0.82583\n",
      "Epoch 529 of 2000. Loss per movie: 0.7733972979782594, test rmse: 0.89704, train rmse: 0.82576\n",
      "Epoch 530 of 2000. Loss per movie: 0.7732636382832113, test rmse: 0.89702, train rmse: 0.82569\n",
      "Epoch 531 of 2000. Loss per movie: 0.7731304759077736, test rmse: 0.89700, train rmse: 0.82562\n",
      "Epoch 532 of 2000. Loss per movie: 0.7729977359386176, test rmse: 0.89699, train rmse: 0.82555\n",
      "Epoch 533 of 2000. Loss per movie: 0.7728655044870909, test rmse: 0.89697, train rmse: 0.82548\n",
      "Epoch 534 of 2000. Loss per movie: 0.7727337092621857, test rmse: 0.89696, train rmse: 0.82541\n",
      "Epoch 535 of 2000. Loss per movie: 0.7726023813773846, test rmse: 0.89694, train rmse: 0.82534\n",
      "Epoch 536 of 2000. Loss per movie: 0.772471538551072, test rmse: 0.89692, train rmse: 0.82527\n",
      "Epoch 537 of 2000. Loss per movie: 0.772341125643636, test rmse: 0.89691, train rmse: 0.82520\n",
      "Epoch 538 of 2000. Loss per movie: 0.7722111792258218, test rmse: 0.89689, train rmse: 0.82513\n",
      "Epoch 539 of 2000. Loss per movie: 0.7720816744918914, test rmse: 0.89688, train rmse: 0.82506\n",
      "Epoch 540 of 2000. Loss per movie: 0.771952625333058, test rmse: 0.89686, train rmse: 0.82499\n",
      "Epoch 541 of 2000. Loss per movie: 0.7718240250163356, test rmse: 0.89685, train rmse: 0.82492\n",
      "Epoch 542 of 2000. Loss per movie: 0.7716958626271995, test rmse: 0.89683, train rmse: 0.82485\n",
      "Epoch 543 of 2000. Loss per movie: 0.7715681531908396, test rmse: 0.89682, train rmse: 0.82479\n",
      "Epoch 544 of 2000. Loss per movie: 0.7714409017392769, test rmse: 0.89680, train rmse: 0.82472\n",
      "Epoch 545 of 2000. Loss per movie: 0.7713140781512584, test rmse: 0.89679, train rmse: 0.82465\n",
      "Epoch 546 of 2000. Loss per movie: 0.7711877138237607, test rmse: 0.89677, train rmse: 0.82458\n",
      "Epoch 547 of 2000. Loss per movie: 0.7710617486560245, test rmse: 0.89676, train rmse: 0.82452\n",
      "Epoch 548 of 2000. Loss per movie: 0.7709362383546501, test rmse: 0.89674, train rmse: 0.82445\n",
      "Epoch 549 of 2000. Loss per movie: 0.7708111593187493, test rmse: 0.89673, train rmse: 0.82438\n",
      "Epoch 550 of 2000. Loss per movie: 0.7706865324560158, test rmse: 0.89671, train rmse: 0.82432\n",
      "Epoch 551 of 2000. Loss per movie: 0.7705623424577657, test rmse: 0.89670, train rmse: 0.82425\n",
      "Epoch 552 of 2000. Loss per movie: 0.7704385700463965, test rmse: 0.89668, train rmse: 0.82418\n",
      "Epoch 553 of 2000. Loss per movie: 0.7703152107568757, test rmse: 0.89667, train rmse: 0.82412\n",
      "Epoch 554 of 2000. Loss per movie: 0.7701922889696999, test rmse: 0.89666, train rmse: 0.82405\n",
      "Epoch 555 of 2000. Loss per movie: 0.7700698106382463, test rmse: 0.89664, train rmse: 0.82399\n",
      "Epoch 556 of 2000. Loss per movie: 0.769947726434533, test rmse: 0.89663, train rmse: 0.82392\n",
      "Epoch 557 of 2000. Loss per movie: 0.7698260858991626, test rmse: 0.89661, train rmse: 0.82386\n",
      "Epoch 558 of 2000. Loss per movie: 0.7697048446653005, test rmse: 0.89660, train rmse: 0.82379\n",
      "Epoch 559 of 2000. Loss per movie: 0.7695840267590762, test rmse: 0.89659, train rmse: 0.82373\n",
      "Epoch 560 of 2000. Loss per movie: 0.7694636416066699, test rmse: 0.89657, train rmse: 0.82367\n",
      "Epoch 561 of 2000. Loss per movie: 0.7693436731906623, test rmse: 0.89656, train rmse: 0.82360\n",
      "Epoch 562 of 2000. Loss per movie: 0.7692241125101141, test rmse: 0.89654, train rmse: 0.82354\n",
      "Epoch 563 of 2000. Loss per movie: 0.769104960699002, test rmse: 0.89653, train rmse: 0.82347\n",
      "Epoch 564 of 2000. Loss per movie: 0.768986223568956, test rmse: 0.89652, train rmse: 0.82341\n",
      "Epoch 565 of 2000. Loss per movie: 0.7688678808501445, test rmse: 0.89650, train rmse: 0.82335\n",
      "Epoch 566 of 2000. Loss per movie: 0.7687499679793359, test rmse: 0.89649, train rmse: 0.82329\n",
      "Epoch 567 of 2000. Loss per movie: 0.7686324444877407, test rmse: 0.89648, train rmse: 0.82322\n",
      "Epoch 568 of 2000. Loss per movie: 0.7685153098083705, test rmse: 0.89646, train rmse: 0.82316\n",
      "Epoch 569 of 2000. Loss per movie: 0.7683986250342143, test rmse: 0.89645, train rmse: 0.82310\n",
      "Epoch 570 of 2000. Loss per movie: 0.768282296186962, test rmse: 0.89644, train rmse: 0.82304\n",
      "Epoch 571 of 2000. Loss per movie: 0.7681663579946466, test rmse: 0.89643, train rmse: 0.82297\n",
      "Epoch 572 of 2000. Loss per movie: 0.7680508401532803, test rmse: 0.89641, train rmse: 0.82291\n",
      "Epoch 573 of 2000. Loss per movie: 0.76793569411449, test rmse: 0.89640, train rmse: 0.82285\n",
      "Epoch 574 of 2000. Loss per movie: 0.7678209602761921, test rmse: 0.89639, train rmse: 0.82279\n",
      "Epoch 575 of 2000. Loss per movie: 0.7677065951220345, test rmse: 0.89638, train rmse: 0.82273\n",
      "Epoch 576 of 2000. Loss per movie: 0.7675925987228911, test rmse: 0.89636, train rmse: 0.82267\n",
      "Epoch 577 of 2000. Loss per movie: 0.7674790189892727, test rmse: 0.89635, train rmse: 0.82261\n",
      "Epoch 578 of 2000. Loss per movie: 0.7673658109164834, test rmse: 0.89634, train rmse: 0.82255\n",
      "Epoch 579 of 2000. Loss per movie: 0.7672529811666355, test rmse: 0.89633, train rmse: 0.82249\n",
      "Epoch 580 of 2000. Loss per movie: 0.7671405310154528, test rmse: 0.89632, train rmse: 0.82243\n",
      "Epoch 581 of 2000. Loss per movie: 0.7670284552182935, test rmse: 0.89630, train rmse: 0.82237\n",
      "Epoch 582 of 2000. Loss per movie: 0.7669167425062654, test rmse: 0.89629, train rmse: 0.82231\n",
      "Epoch 583 of 2000. Loss per movie: 0.7668054215831507, test rmse: 0.89628, train rmse: 0.82225\n",
      "Epoch 584 of 2000. Loss per movie: 0.7666944487908507, test rmse: 0.89627, train rmse: 0.82219\n",
      "Epoch 585 of 2000. Loss per movie: 0.7665838354691313, test rmse: 0.89626, train rmse: 0.82213\n",
      "Epoch 586 of 2000. Loss per movie: 0.7664736140780727, test rmse: 0.89625, train rmse: 0.82207\n",
      "Epoch 587 of 2000. Loss per movie: 0.766363759599316, test rmse: 0.89623, train rmse: 0.82201\n",
      "Epoch 588 of 2000. Loss per movie: 0.7662542464475144, test rmse: 0.89622, train rmse: 0.82195\n",
      "Epoch 589 of 2000. Loss per movie: 0.7661450903565932, test rmse: 0.89621, train rmse: 0.82190\n",
      "Epoch 590 of 2000. Loss per movie: 0.7660363064226158, test rmse: 0.89620, train rmse: 0.82184\n",
      "Epoch 591 of 2000. Loss per movie: 0.7659278862116311, test rmse: 0.89619, train rmse: 0.82178\n",
      "Epoch 592 of 2000. Loss per movie: 0.7658197941451237, test rmse: 0.89618, train rmse: 0.82172\n",
      "Epoch 593 of 2000. Loss per movie: 0.7657120825986374, test rmse: 0.89617, train rmse: 0.82166\n",
      "Epoch 594 of 2000. Loss per movie: 0.7656047087645559, test rmse: 0.89616, train rmse: 0.82161\n",
      "Epoch 595 of 2000. Loss per movie: 0.7654977146708866, test rmse: 0.89615, train rmse: 0.82155\n",
      "Epoch 596 of 2000. Loss per movie: 0.7653910503517859, test rmse: 0.89614, train rmse: 0.82149\n",
      "Epoch 597 of 2000. Loss per movie: 0.7652847494013103, test rmse: 0.89613, train rmse: 0.82144\n",
      "Epoch 598 of 2000. Loss per movie: 0.7651787982826143, test rmse: 0.89612, train rmse: 0.82138\n",
      "Epoch 599 of 2000. Loss per movie: 0.7650731750957748, test rmse: 0.89611, train rmse: 0.82132\n",
      "Epoch 600 of 2000. Loss per movie: 0.764967931720221, test rmse: 0.89609, train rmse: 0.82127\n",
      "Epoch 601 of 2000. Loss per movie: 0.7648629983455191, test rmse: 0.89608, train rmse: 0.82121\n",
      "Epoch 602 of 2000. Loss per movie: 0.7647584612956371, test rmse: 0.89607, train rmse: 0.82116\n",
      "Epoch 603 of 2000. Loss per movie: 0.7646542416883281, test rmse: 0.89606, train rmse: 0.82110\n",
      "Epoch 604 of 2000. Loss per movie: 0.7645503729050281, test rmse: 0.89605, train rmse: 0.82104\n",
      "Epoch 605 of 2000. Loss per movie: 0.7644468667107445, test rmse: 0.89604, train rmse: 0.82099\n",
      "Epoch 606 of 2000. Loss per movie: 0.7643437180734559, test rmse: 0.89604, train rmse: 0.82093\n",
      "Epoch 607 of 2000. Loss per movie: 0.7642408578205903, test rmse: 0.89603, train rmse: 0.82088\n",
      "Epoch 608 of 2000. Loss per movie: 0.7641383933255562, test rmse: 0.89602, train rmse: 0.82082\n",
      "Epoch 609 of 2000. Loss per movie: 0.7640362610147909, test rmse: 0.89601, train rmse: 0.82077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 610 of 2000. Loss per movie: 0.7639344428155425, test rmse: 0.89600, train rmse: 0.82071\n",
      "Epoch 611 of 2000. Loss per movie: 0.7638329870635633, test rmse: 0.89599, train rmse: 0.82066\n",
      "Epoch 612 of 2000. Loss per movie: 0.7637318795841458, test rmse: 0.89598, train rmse: 0.82061\n",
      "Epoch 613 of 2000. Loss per movie: 0.763631115628763, test rmse: 0.89597, train rmse: 0.82055\n",
      "Epoch 614 of 2000. Loss per movie: 0.763530660115014, test rmse: 0.89596, train rmse: 0.82050\n",
      "Epoch 615 of 2000. Loss per movie: 0.763430557905848, test rmse: 0.89595, train rmse: 0.82045\n",
      "Epoch 616 of 2000. Loss per movie: 0.7633307790217587, test rmse: 0.89594, train rmse: 0.82039\n",
      "Epoch 617 of 2000. Loss per movie: 0.7632313472762544, test rmse: 0.89593, train rmse: 0.82034\n",
      "Epoch 618 of 2000. Loss per movie: 0.7631322577081877, test rmse: 0.89592, train rmse: 0.82029\n",
      "Epoch 619 of 2000. Loss per movie: 0.7630334666594597, test rmse: 0.89591, train rmse: 0.82023\n",
      "Epoch 620 of 2000. Loss per movie: 0.7629350423104131, test rmse: 0.89590, train rmse: 0.82018\n",
      "Epoch 621 of 2000. Loss per movie: 0.7628369178981759, test rmse: 0.89590, train rmse: 0.82013\n",
      "Epoch 622 of 2000. Loss per movie: 0.7627391245362308, test rmse: 0.89589, train rmse: 0.82008\n",
      "Epoch 623 of 2000. Loss per movie: 0.7626416535780064, test rmse: 0.89588, train rmse: 0.82002\n",
      "Epoch 624 of 2000. Loss per movie: 0.7625445223875194, test rmse: 0.89587, train rmse: 0.81997\n",
      "Epoch 625 of 2000. Loss per movie: 0.7624477112619261, test rmse: 0.89586, train rmse: 0.81992\n",
      "Epoch 626 of 2000. Loss per movie: 0.7623512008527513, test rmse: 0.89585, train rmse: 0.81987\n",
      "Epoch 627 of 2000. Loss per movie: 0.7622550104375968, test rmse: 0.89584, train rmse: 0.81982\n",
      "Epoch 628 of 2000. Loss per movie: 0.7621591458280929, test rmse: 0.89584, train rmse: 0.81977\n",
      "Epoch 629 of 2000. Loss per movie: 0.7620635818641336, test rmse: 0.89583, train rmse: 0.81971\n",
      "Epoch 630 of 2000. Loss per movie: 0.7619683319408176, test rmse: 0.89582, train rmse: 0.81966\n",
      "Epoch 631 of 2000. Loss per movie: 0.7618734096658641, test rmse: 0.89581, train rmse: 0.81961\n",
      "Epoch 632 of 2000. Loss per movie: 0.7617787574899608, test rmse: 0.89580, train rmse: 0.81956\n",
      "Epoch 633 of 2000. Loss per movie: 0.7616844463575184, test rmse: 0.89579, train rmse: 0.81951\n",
      "Epoch 634 of 2000. Loss per movie: 0.7615903966775547, test rmse: 0.89579, train rmse: 0.81946\n",
      "Epoch 635 of 2000. Loss per movie: 0.7614967010109093, test rmse: 0.89578, train rmse: 0.81941\n",
      "Epoch 636 of 2000. Loss per movie: 0.7614032546773677, test rmse: 0.89577, train rmse: 0.81936\n",
      "Epoch 637 of 2000. Loss per movie: 0.7613101482533104, test rmse: 0.89576, train rmse: 0.81931\n",
      "Epoch 638 of 2000. Loss per movie: 0.761217296690493, test rmse: 0.89575, train rmse: 0.81926\n",
      "Epoch 639 of 2000. Loss per movie: 0.7611247522935856, test rmse: 0.89575, train rmse: 0.81921\n",
      "Epoch 640 of 2000. Loss per movie: 0.7610325091092113, test rmse: 0.89574, train rmse: 0.81916\n",
      "Epoch 641 of 2000. Loss per movie: 0.7609405269520745, test rmse: 0.89573, train rmse: 0.81911\n",
      "Epoch 642 of 2000. Loss per movie: 0.7608488787510449, test rmse: 0.89572, train rmse: 0.81906\n",
      "Epoch 643 of 2000. Loss per movie: 0.7607574745676038, test rmse: 0.89572, train rmse: 0.81901\n",
      "Epoch 644 of 2000. Loss per movie: 0.7606663817316116, test rmse: 0.89571, train rmse: 0.81897\n",
      "Epoch 645 of 2000. Loss per movie: 0.7605755260384749, test rmse: 0.89570, train rmse: 0.81892\n",
      "Epoch 646 of 2000. Loss per movie: 0.7604849731879626, test rmse: 0.89569, train rmse: 0.81887\n",
      "Epoch 647 of 2000. Loss per movie: 0.7603947135412735, test rmse: 0.89569, train rmse: 0.81882\n",
      "Epoch 648 of 2000. Loss per movie: 0.7603046921005431, test rmse: 0.89568, train rmse: 0.81877\n",
      "Epoch 649 of 2000. Loss per movie: 0.760214975628643, test rmse: 0.89567, train rmse: 0.81872\n",
      "Epoch 650 of 2000. Loss per movie: 0.7601254926141746, test rmse: 0.89566, train rmse: 0.81868\n",
      "Epoch 651 of 2000. Loss per movie: 0.7600362553182595, test rmse: 0.89566, train rmse: 0.81863\n",
      "Epoch 652 of 2000. Loss per movie: 0.7599473313542524, test rmse: 0.89565, train rmse: 0.81858\n",
      "Epoch 653 of 2000. Loss per movie: 0.7598586723155272, test rmse: 0.89564, train rmse: 0.81853\n",
      "Epoch 654 of 2000. Loss per movie: 0.7597702315672966, test rmse: 0.89563, train rmse: 0.81849\n",
      "Epoch 655 of 2000. Loss per movie: 0.7596820668006199, test rmse: 0.89563, train rmse: 0.81844\n",
      "Epoch 656 of 2000. Loss per movie: 0.7595941257816999, test rmse: 0.89562, train rmse: 0.81839\n",
      "Epoch 657 of 2000. Loss per movie: 0.7595064686112966, test rmse: 0.89561, train rmse: 0.81834\n",
      "Epoch 658 of 2000. Loss per movie: 0.7594190451818187, test rmse: 0.89561, train rmse: 0.81830\n",
      "Epoch 659 of 2000. Loss per movie: 0.7593318576903463, test rmse: 0.89560, train rmse: 0.81825\n",
      "Epoch 660 of 2000. Loss per movie: 0.7592449343445469, test rmse: 0.89559, train rmse: 0.81820\n",
      "Epoch 661 of 2000. Loss per movie: 0.7591582096572722, test rmse: 0.89559, train rmse: 0.81816\n",
      "Epoch 662 of 2000. Loss per movie: 0.7590717405399726, test rmse: 0.89558, train rmse: 0.81811\n",
      "Epoch 663 of 2000. Loss per movie: 0.7589855058014605, test rmse: 0.89557, train rmse: 0.81806\n",
      "Epoch 664 of 2000. Loss per movie: 0.7588994726272882, test rmse: 0.89556, train rmse: 0.81802\n",
      "Epoch 665 of 2000. Loss per movie: 0.758813668232894, test rmse: 0.89556, train rmse: 0.81797\n",
      "Epoch 666 of 2000. Loss per movie: 0.7587280776639614, test rmse: 0.89555, train rmse: 0.81793\n",
      "Epoch 667 of 2000. Loss per movie: 0.7586426891554834, test rmse: 0.89554, train rmse: 0.81788\n",
      "Epoch 668 of 2000. Loss per movie: 0.758557520000603, test rmse: 0.89554, train rmse: 0.81784\n",
      "Epoch 669 of 2000. Loss per movie: 0.7584725246276357, test rmse: 0.89553, train rmse: 0.81779\n",
      "Epoch 670 of 2000. Loss per movie: 0.7583877236607809, test rmse: 0.89552, train rmse: 0.81774\n",
      "Epoch 671 of 2000. Loss per movie: 0.7583031207854626, test rmse: 0.89552, train rmse: 0.81770\n",
      "Epoch 672 of 2000. Loss per movie: 0.7582186644766192, test rmse: 0.89551, train rmse: 0.81765\n",
      "Epoch 673 of 2000. Loss per movie: 0.7581344063301858, test rmse: 0.89550, train rmse: 0.81761\n",
      "Epoch 674 of 2000. Loss per movie: 0.758050310555026, test rmse: 0.89550, train rmse: 0.81756\n",
      "Epoch 675 of 2000. Loss per movie: 0.7579663783559901, test rmse: 0.89549, train rmse: 0.81752\n",
      "Epoch 676 of 2000. Loss per movie: 0.7578826210019705, test rmse: 0.89549, train rmse: 0.81747\n",
      "Epoch 677 of 2000. Loss per movie: 0.7577990437376088, test rmse: 0.89548, train rmse: 0.81743\n",
      "Epoch 678 of 2000. Loss per movie: 0.7577156334513008, test rmse: 0.89547, train rmse: 0.81738\n",
      "Epoch 679 of 2000. Loss per movie: 0.757632418988576, test rmse: 0.89547, train rmse: 0.81734\n",
      "Epoch 680 of 2000. Loss per movie: 0.7575493704408017, test rmse: 0.89546, train rmse: 0.81729\n",
      "Epoch 681 of 2000. Loss per movie: 0.75746654294759, test rmse: 0.89545, train rmse: 0.81725\n",
      "Epoch 682 of 2000. Loss per movie: 0.7573839040488607, test rmse: 0.89545, train rmse: 0.81720\n",
      "Epoch 683 of 2000. Loss per movie: 0.7573015191608887, test rmse: 0.89544, train rmse: 0.81716\n",
      "Epoch 684 of 2000. Loss per movie: 0.7572193509333199, test rmse: 0.89543, train rmse: 0.81712\n",
      "Epoch 685 of 2000. Loss per movie: 0.7571374031933252, test rmse: 0.89543, train rmse: 0.81707\n",
      "Epoch 686 of 2000. Loss per movie: 0.757055684800097, test rmse: 0.89542, train rmse: 0.81703\n",
      "Epoch 687 of 2000. Loss per movie: 0.7569742236778086, test rmse: 0.89542, train rmse: 0.81698\n",
      "Epoch 688 of 2000. Loss per movie: 0.7568930075653385, test rmse: 0.89541, train rmse: 0.81694\n",
      "Epoch 689 of 2000. Loss per movie: 0.7568119977657349, test rmse: 0.89540, train rmse: 0.81690\n",
      "Epoch 690 of 2000. Loss per movie: 0.7567312567185845, test rmse: 0.89540, train rmse: 0.81685\n",
      "Epoch 691 of 2000. Loss per movie: 0.7566507504754628, test rmse: 0.89539, train rmse: 0.81681\n",
      "Epoch 692 of 2000. Loss per movie: 0.7565704572073203, test rmse: 0.89539, train rmse: 0.81677\n",
      "Epoch 693 of 2000. Loss per movie: 0.7564904248955417, test rmse: 0.89538, train rmse: 0.81672\n",
      "Epoch 694 of 2000. Loss per movie: 0.7564106061257306, test rmse: 0.89537, train rmse: 0.81668\n",
      "Epoch 695 of 2000. Loss per movie: 0.7563310296725433, test rmse: 0.89537, train rmse: 0.81664\n",
      "Epoch 696 of 2000. Loss per movie: 0.7562516874563964, test rmse: 0.89536, train rmse: 0.81660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 697 of 2000. Loss per movie: 0.7561725733112921, test rmse: 0.89536, train rmse: 0.81655\n",
      "Epoch 698 of 2000. Loss per movie: 0.7560936756848439, test rmse: 0.89535, train rmse: 0.81651\n",
      "Epoch 699 of 2000. Loss per movie: 0.7560150039323587, test rmse: 0.89535, train rmse: 0.81647\n",
      "Epoch 700 of 2000. Loss per movie: 0.7559365581955836, test rmse: 0.89534, train rmse: 0.81643\n",
      "Epoch 701 of 2000. Loss per movie: 0.7558583262133967, test rmse: 0.89534, train rmse: 0.81639\n",
      "Epoch 702 of 2000. Loss per movie: 0.7557802915431373, test rmse: 0.89533, train rmse: 0.81634\n",
      "Epoch 703 of 2000. Loss per movie: 0.7557025025205578, test rmse: 0.89532, train rmse: 0.81630\n",
      "Epoch 704 of 2000. Loss per movie: 0.755624910172044, test rmse: 0.89532, train rmse: 0.81626\n",
      "Epoch 705 of 2000. Loss per movie: 0.7555475658809104, test rmse: 0.89531, train rmse: 0.81622\n",
      "Epoch 706 of 2000. Loss per movie: 0.7554703989862404, test rmse: 0.89531, train rmse: 0.81618\n",
      "Epoch 707 of 2000. Loss per movie: 0.7553934598082452, test rmse: 0.89530, train rmse: 0.81614\n",
      "Epoch 708 of 2000. Loss per movie: 0.7553167091538592, test rmse: 0.89530, train rmse: 0.81609\n",
      "Epoch 709 of 2000. Loss per movie: 0.7552401832394594, test rmse: 0.89529, train rmse: 0.81605\n",
      "Epoch 710 of 2000. Loss per movie: 0.7551638722136245, test rmse: 0.89529, train rmse: 0.81601\n",
      "Epoch 711 of 2000. Loss per movie: 0.7550877795491576, test rmse: 0.89528, train rmse: 0.81597\n",
      "Epoch 712 of 2000. Loss per movie: 0.7550118707306461, test rmse: 0.89528, train rmse: 0.81593\n",
      "Epoch 713 of 2000. Loss per movie: 0.7549361726191608, test rmse: 0.89527, train rmse: 0.81589\n",
      "Epoch 714 of 2000. Loss per movie: 0.7548606642361345, test rmse: 0.89527, train rmse: 0.81585\n",
      "Epoch 715 of 2000. Loss per movie: 0.7547853659931459, test rmse: 0.89526, train rmse: 0.81581\n",
      "Epoch 716 of 2000. Loss per movie: 0.7547102626523847, test rmse: 0.89526, train rmse: 0.81577\n",
      "Epoch 717 of 2000. Loss per movie: 0.7546353873826661, test rmse: 0.89525, train rmse: 0.81573\n",
      "Epoch 718 of 2000. Loss per movie: 0.754560699927821, test rmse: 0.89525, train rmse: 0.81569\n",
      "Epoch 719 of 2000. Loss per movie: 0.7544861918538986, test rmse: 0.89524, train rmse: 0.81565\n",
      "Epoch 720 of 2000. Loss per movie: 0.7544118998733911, test rmse: 0.89524, train rmse: 0.81561\n",
      "Epoch 721 of 2000. Loss per movie: 0.7543377933689304, test rmse: 0.89523, train rmse: 0.81557\n",
      "Epoch 722 of 2000. Loss per movie: 0.7542638819793175, test rmse: 0.89523, train rmse: 0.81553\n",
      "Epoch 723 of 2000. Loss per movie: 0.7541901943374616, test rmse: 0.89522, train rmse: 0.81549\n",
      "Epoch 724 of 2000. Loss per movie: 0.7541166568057573, test rmse: 0.89522, train rmse: 0.81545\n",
      "Epoch 725 of 2000. Loss per movie: 0.75404334762859, test rmse: 0.89521, train rmse: 0.81541\n",
      "Epoch 726 of 2000. Loss per movie: 0.7539701929557338, test rmse: 0.89521, train rmse: 0.81537\n",
      "Epoch 727 of 2000. Loss per movie: 0.7538972434617677, test rmse: 0.89520, train rmse: 0.81533\n",
      "Epoch 728 of 2000. Loss per movie: 0.7538244851846049, test rmse: 0.89520, train rmse: 0.81529\n",
      "Epoch 729 of 2000. Loss per movie: 0.7537519308106085, test rmse: 0.89519, train rmse: 0.81525\n",
      "Epoch 730 of 2000. Loss per movie: 0.753679535122462, test rmse: 0.89519, train rmse: 0.81521\n",
      "Epoch 731 of 2000. Loss per movie: 0.7536073448258263, test rmse: 0.89518, train rmse: 0.81517\n",
      "Epoch 732 of 2000. Loss per movie: 0.7535353280984831, test rmse: 0.89518, train rmse: 0.81514\n",
      "Epoch 733 of 2000. Loss per movie: 0.7534634808297673, test rmse: 0.89517, train rmse: 0.81510\n",
      "Epoch 734 of 2000. Loss per movie: 0.7533918496544665, test rmse: 0.89517, train rmse: 0.81506\n",
      "Epoch 735 of 2000. Loss per movie: 0.7533203768106477, test rmse: 0.89516, train rmse: 0.81502\n",
      "Epoch 736 of 2000. Loss per movie: 0.7532491039010772, test rmse: 0.89516, train rmse: 0.81498\n",
      "Epoch 737 of 2000. Loss per movie: 0.7531779838657265, test rmse: 0.89516, train rmse: 0.81494\n",
      "Epoch 738 of 2000. Loss per movie: 0.753107044061781, test rmse: 0.89515, train rmse: 0.81490\n",
      "Epoch 739 of 2000. Loss per movie: 0.7530362856940905, test rmse: 0.89515, train rmse: 0.81487\n",
      "Epoch 740 of 2000. Loss per movie: 0.7529657152121473, test rmse: 0.89514, train rmse: 0.81483\n",
      "Epoch 741 of 2000. Loss per movie: 0.7528953167402787, test rmse: 0.89514, train rmse: 0.81479\n",
      "Epoch 742 of 2000. Loss per movie: 0.7528250901367378, test rmse: 0.89513, train rmse: 0.81475\n",
      "Epoch 743 of 2000. Loss per movie: 0.7527550327792037, test rmse: 0.89513, train rmse: 0.81471\n",
      "Epoch 744 of 2000. Loss per movie: 0.7526851489909622, test rmse: 0.89513, train rmse: 0.81468\n",
      "Epoch 745 of 2000. Loss per movie: 0.752615430550683, test rmse: 0.89512, train rmse: 0.81464\n",
      "Epoch 746 of 2000. Loss per movie: 0.7525458968068417, test rmse: 0.89512, train rmse: 0.81460\n",
      "Epoch 747 of 2000. Loss per movie: 0.7524765308206629, test rmse: 0.89511, train rmse: 0.81456\n",
      "Epoch 748 of 2000. Loss per movie: 0.7524073303950669, test rmse: 0.89511, train rmse: 0.81453\n",
      "Epoch 749 of 2000. Loss per movie: 0.752338301483431, test rmse: 0.89510, train rmse: 0.81449\n",
      "Epoch 750 of 2000. Loss per movie: 0.7522694255168884, test rmse: 0.89510, train rmse: 0.81445\n",
      "Epoch 751 of 2000. Loss per movie: 0.7522007393496782, test rmse: 0.89510, train rmse: 0.81442\n",
      "Epoch 752 of 2000. Loss per movie: 0.7521321872752005, test rmse: 0.89509, train rmse: 0.81438\n",
      "Epoch 753 of 2000. Loss per movie: 0.7520638329378915, test rmse: 0.89509, train rmse: 0.81434\n",
      "Epoch 754 of 2000. Loss per movie: 0.7519956389165237, test rmse: 0.89508, train rmse: 0.81431\n",
      "Epoch 755 of 2000. Loss per movie: 0.7519275961392842, test rmse: 0.89508, train rmse: 0.81427\n",
      "Epoch 756 of 2000. Loss per movie: 0.7518596997158989, test rmse: 0.89508, train rmse: 0.81423\n",
      "Epoch 757 of 2000. Loss per movie: 0.7517920066995655, test rmse: 0.89507, train rmse: 0.81420\n",
      "Epoch 758 of 2000. Loss per movie: 0.7517244385624046, test rmse: 0.89507, train rmse: 0.81416\n",
      "Epoch 759 of 2000. Loss per movie: 0.751657056255658, test rmse: 0.89506, train rmse: 0.81412\n",
      "Epoch 760 of 2000. Loss per movie: 0.751589813711527, test rmse: 0.89506, train rmse: 0.81409\n",
      "Epoch 761 of 2000. Loss per movie: 0.7515227518949157, test rmse: 0.89506, train rmse: 0.81405\n",
      "Epoch 762 of 2000. Loss per movie: 0.7514558328884818, test rmse: 0.89505, train rmse: 0.81401\n",
      "Epoch 763 of 2000. Loss per movie: 0.7513890761824482, test rmse: 0.89505, train rmse: 0.81398\n",
      "Epoch 764 of 2000. Loss per movie: 0.7513224582468003, test rmse: 0.89504, train rmse: 0.81394\n",
      "Epoch 765 of 2000. Loss per movie: 0.7512560267794288, test rmse: 0.89504, train rmse: 0.81391\n",
      "Epoch 766 of 2000. Loss per movie: 0.7511897260028598, test rmse: 0.89504, train rmse: 0.81387\n",
      "Epoch 767 of 2000. Loss per movie: 0.7511235977324805, test rmse: 0.89503, train rmse: 0.81384\n",
      "Epoch 768 of 2000. Loss per movie: 0.7510576285023185, test rmse: 0.89503, train rmse: 0.81380\n",
      "Epoch 769 of 2000. Loss per movie: 0.7509918098075494, test rmse: 0.89502, train rmse: 0.81376\n",
      "Epoch 770 of 2000. Loss per movie: 0.7509261264103629, test rmse: 0.89502, train rmse: 0.81373\n",
      "Epoch 771 of 2000. Loss per movie: 0.7508606021951411, test rmse: 0.89502, train rmse: 0.81369\n",
      "Epoch 772 of 2000. Loss per movie: 0.7507952425482723, test rmse: 0.89501, train rmse: 0.81366\n",
      "Epoch 773 of 2000. Loss per movie: 0.7507300123873563, test rmse: 0.89501, train rmse: 0.81362\n",
      "Epoch 774 of 2000. Loss per movie: 0.7506649504093438, test rmse: 0.89501, train rmse: 0.81359\n",
      "Epoch 775 of 2000. Loss per movie: 0.7506000253590052, test rmse: 0.89500, train rmse: 0.81355\n",
      "Epoch 776 of 2000. Loss per movie: 0.7505352594906312, test rmse: 0.89500, train rmse: 0.81352\n",
      "Epoch 777 of 2000. Loss per movie: 0.7504706360780669, test rmse: 0.89500, train rmse: 0.81348\n",
      "Epoch 778 of 2000. Loss per movie: 0.7504061471126026, test rmse: 0.89499, train rmse: 0.81345\n",
      "Epoch 779 of 2000. Loss per movie: 0.7503418060602104, test rmse: 0.89499, train rmse: 0.81341\n",
      "Epoch 780 of 2000. Loss per movie: 0.7502776380809961, test rmse: 0.89499, train rmse: 0.81338\n",
      "Epoch 781 of 2000. Loss per movie: 0.7502135898780596, test rmse: 0.89498, train rmse: 0.81334\n",
      "Epoch 782 of 2000. Loss per movie: 0.7501496996522375, test rmse: 0.89498, train rmse: 0.81331\n",
      "Epoch 783 of 2000. Loss per movie: 0.7500859451492391, test rmse: 0.89497, train rmse: 0.81328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 784 of 2000. Loss per movie: 0.7500223481981139, test rmse: 0.89497, train rmse: 0.81324\n",
      "Epoch 785 of 2000. Loss per movie: 0.7499588844183651, test rmse: 0.89497, train rmse: 0.81321\n",
      "Epoch 786 of 2000. Loss per movie: 0.7498955470770066, test rmse: 0.89496, train rmse: 0.81317\n",
      "Epoch 787 of 2000. Loss per movie: 0.7498323637438444, test rmse: 0.89496, train rmse: 0.81314\n",
      "Epoch 788 of 2000. Loss per movie: 0.7497693262684217, test rmse: 0.89496, train rmse: 0.81310\n",
      "Epoch 789 of 2000. Loss per movie: 0.7497064248701906, test rmse: 0.89495, train rmse: 0.81307\n",
      "Epoch 790 of 2000. Loss per movie: 0.749643665006413, test rmse: 0.89495, train rmse: 0.81304\n",
      "Epoch 791 of 2000. Loss per movie: 0.7495810229344544, test rmse: 0.89495, train rmse: 0.81300\n",
      "Epoch 792 of 2000. Loss per movie: 0.7495185309017739, test rmse: 0.89494, train rmse: 0.81297\n",
      "Epoch 793 of 2000. Loss per movie: 0.7494562036500675, test rmse: 0.89494, train rmse: 0.81294\n",
      "Epoch 794 of 2000. Loss per movie: 0.7493939563437109, test rmse: 0.89494, train rmse: 0.81290\n",
      "Epoch 795 of 2000. Loss per movie: 0.7493318667309746, test rmse: 0.89493, train rmse: 0.81287\n",
      "Epoch 796 of 2000. Loss per movie: 0.74926993644195, test rmse: 0.89493, train rmse: 0.81283\n",
      "Epoch 797 of 2000. Loss per movie: 0.7492081144476901, test rmse: 0.89493, train rmse: 0.81280\n",
      "Epoch 798 of 2000. Loss per movie: 0.7491464193879354, test rmse: 0.89492, train rmse: 0.81277\n",
      "Epoch 799 of 2000. Loss per movie: 0.7490848767771591, test rmse: 0.89492, train rmse: 0.81273\n",
      "Epoch 800 of 2000. Loss per movie: 0.749023450824225, test rmse: 0.89492, train rmse: 0.81270\n",
      "Epoch 801 of 2000. Loss per movie: 0.7489621534358873, test rmse: 0.89492, train rmse: 0.81267\n",
      "Epoch 802 of 2000. Loss per movie: 0.7489010091343902, test rmse: 0.89491, train rmse: 0.81264\n",
      "Epoch 803 of 2000. Loss per movie: 0.7488399657568098, test rmse: 0.89491, train rmse: 0.81260\n",
      "Epoch 804 of 2000. Loss per movie: 0.748779056755456, test rmse: 0.89491, train rmse: 0.81257\n",
      "Epoch 805 of 2000. Loss per movie: 0.7487182963050362, test rmse: 0.89490, train rmse: 0.81254\n",
      "Epoch 806 of 2000. Loss per movie: 0.7486576422357956, test rmse: 0.89490, train rmse: 0.81250\n",
      "Epoch 807 of 2000. Loss per movie: 0.7485970872477601, test rmse: 0.89490, train rmse: 0.81247\n",
      "Epoch 808 of 2000. Loss per movie: 0.7485367161765535, test rmse: 0.89489, train rmse: 0.81244\n",
      "Epoch 809 of 2000. Loss per movie: 0.7484764335555214, test rmse: 0.89489, train rmse: 0.81241\n",
      "Epoch 810 of 2000. Loss per movie: 0.748416261851575, test rmse: 0.89489, train rmse: 0.81237\n",
      "Epoch 811 of 2000. Loss per movie: 0.748356246423778, test rmse: 0.89488, train rmse: 0.81234\n",
      "Epoch 812 of 2000. Loss per movie: 0.7482963396451136, test rmse: 0.89488, train rmse: 0.81231\n",
      "Epoch 813 of 2000. Loss per movie: 0.7482365583834836, test rmse: 0.89488, train rmse: 0.81228\n",
      "Epoch 814 of 2000. Loss per movie: 0.7481768783292646, test rmse: 0.89488, train rmse: 0.81224\n",
      "Epoch 815 of 2000. Loss per movie: 0.7481173600084575, test rmse: 0.89487, train rmse: 0.81221\n",
      "Epoch 816 of 2000. Loss per movie: 0.7480579487066915, test rmse: 0.89487, train rmse: 0.81218\n",
      "Epoch 817 of 2000. Loss per movie: 0.74799861543669, test rmse: 0.89487, train rmse: 0.81215\n",
      "Epoch 818 of 2000. Loss per movie: 0.7479394485068803, test rmse: 0.89486, train rmse: 0.81212\n",
      "Epoch 819 of 2000. Loss per movie: 0.747880387036894, test rmse: 0.89486, train rmse: 0.81208\n",
      "Epoch 820 of 2000. Loss per movie: 0.7478214172063912, test rmse: 0.89486, train rmse: 0.81205\n",
      "Epoch 821 of 2000. Loss per movie: 0.7477626000374874, test rmse: 0.89485, train rmse: 0.81202\n",
      "Epoch 822 of 2000. Loss per movie: 0.7477038985341965, test rmse: 0.89485, train rmse: 0.81199\n",
      "Epoch 823 of 2000. Loss per movie: 0.7476453090819678, test rmse: 0.89485, train rmse: 0.81196\n",
      "Epoch 824 of 2000. Loss per movie: 0.7475868376341784, test rmse: 0.89485, train rmse: 0.81192\n",
      "Epoch 825 of 2000. Loss per movie: 0.7475284780957044, test rmse: 0.89484, train rmse: 0.81189\n",
      "Epoch 826 of 2000. Loss per movie: 0.7474702413101968, test rmse: 0.89484, train rmse: 0.81186\n",
      "Epoch 827 of 2000. Loss per movie: 0.7474121423737189, test rmse: 0.89484, train rmse: 0.81183\n",
      "Epoch 828 of 2000. Loss per movie: 0.747354126146659, test rmse: 0.89483, train rmse: 0.81180\n",
      "Epoch 829 of 2000. Loss per movie: 0.7472962391929309, test rmse: 0.89483, train rmse: 0.81177\n",
      "Epoch 830 of 2000. Loss per movie: 0.747238489733865, test rmse: 0.89483, train rmse: 0.81174\n",
      "Epoch 831 of 2000. Loss per movie: 0.7471808346783505, test rmse: 0.89482, train rmse: 0.81171\n",
      "Epoch 832 of 2000. Loss per movie: 0.7471233254097022, test rmse: 0.89482, train rmse: 0.81167\n",
      "Epoch 833 of 2000. Loss per movie: 0.7470659114659612, test rmse: 0.89482, train rmse: 0.81164\n",
      "Epoch 834 of 2000. Loss per movie: 0.7470086204169338, test rmse: 0.89482, train rmse: 0.81161\n",
      "Epoch 835 of 2000. Loss per movie: 0.7469514551684349, test rmse: 0.89481, train rmse: 0.81158\n",
      "Epoch 836 of 2000. Loss per movie: 0.7468944287611954, test rmse: 0.89481, train rmse: 0.81155\n",
      "Epoch 837 of 2000. Loss per movie: 0.746837484567259, test rmse: 0.89481, train rmse: 0.81152\n",
      "Epoch 838 of 2000. Loss per movie: 0.7467807091232146, test rmse: 0.89480, train rmse: 0.81149\n",
      "Epoch 839 of 2000. Loss per movie: 0.7467240179478059, test rmse: 0.89480, train rmse: 0.81146\n",
      "Epoch 840 of 2000. Loss per movie: 0.746667484536891, test rmse: 0.89480, train rmse: 0.81143\n",
      "Epoch 841 of 2000. Loss per movie: 0.7466110461673895, test rmse: 0.89480, train rmse: 0.81140\n",
      "Epoch 842 of 2000. Loss per movie: 0.7465547376382081, test rmse: 0.89479, train rmse: 0.81137\n",
      "Epoch 843 of 2000. Loss per movie: 0.7464985595872088, test rmse: 0.89479, train rmse: 0.81134\n",
      "Epoch 844 of 2000. Loss per movie: 0.746442498264925, test rmse: 0.89479, train rmse: 0.81131\n",
      "Epoch 845 of 2000. Loss per movie: 0.7463865814537838, test rmse: 0.89478, train rmse: 0.81128\n",
      "Epoch 846 of 2000. Loss per movie: 0.746330769606351, test rmse: 0.89478, train rmse: 0.81125\n",
      "Epoch 847 of 2000. Loss per movie: 0.746275081078873, test rmse: 0.89478, train rmse: 0.81121\n",
      "Epoch 848 of 2000. Loss per movie: 0.7462195304712983, test rmse: 0.89478, train rmse: 0.81118\n",
      "Epoch 849 of 2000. Loss per movie: 0.7461641121846175, test rmse: 0.89477, train rmse: 0.81115\n",
      "Epoch 850 of 2000. Loss per movie: 0.7461088033975517, test rmse: 0.89477, train rmse: 0.81112\n",
      "Epoch 851 of 2000. Loss per movie: 0.7460536343022277, test rmse: 0.89477, train rmse: 0.81110\n",
      "Epoch 852 of 2000. Loss per movie: 0.7459985872511348, test rmse: 0.89476, train rmse: 0.81107\n",
      "Epoch 853 of 2000. Loss per movie: 0.7459436642287319, test rmse: 0.89476, train rmse: 0.81104\n",
      "Epoch 854 of 2000. Loss per movie: 0.7458888853631037, test rmse: 0.89476, train rmse: 0.81101\n",
      "Epoch 855 of 2000. Loss per movie: 0.7458342137291372, test rmse: 0.89476, train rmse: 0.81098\n",
      "Epoch 856 of 2000. Loss per movie: 0.7457796717228702, test rmse: 0.89475, train rmse: 0.81095\n",
      "Epoch 857 of 2000. Loss per movie: 0.7457252672112654, test rmse: 0.89475, train rmse: 0.81092\n",
      "Epoch 858 of 2000. Loss per movie: 0.7456709938157045, test rmse: 0.89475, train rmse: 0.81089\n",
      "Epoch 859 of 2000. Loss per movie: 0.7456168153906833, test rmse: 0.89475, train rmse: 0.81086\n",
      "Epoch 860 of 2000. Loss per movie: 0.7455627989116945, test rmse: 0.89474, train rmse: 0.81083\n",
      "Epoch 861 of 2000. Loss per movie: 0.7455088596139878, test rmse: 0.89474, train rmse: 0.81080\n",
      "Epoch 862 of 2000. Loss per movie: 0.7454550790730045, test rmse: 0.89474, train rmse: 0.81077\n",
      "Epoch 863 of 2000. Loss per movie: 0.7454014226315846, test rmse: 0.89473, train rmse: 0.81074\n",
      "Epoch 864 of 2000. Loss per movie: 0.7453479010625061, test rmse: 0.89473, train rmse: 0.81071\n",
      "Epoch 865 of 2000. Loss per movie: 0.7452944931037078, test rmse: 0.89473, train rmse: 0.81068\n",
      "Epoch 866 of 2000. Loss per movie: 0.7452412000309131, test rmse: 0.89473, train rmse: 0.81066\n",
      "Epoch 867 of 2000. Loss per movie: 0.7451880204975251, test rmse: 0.89472, train rmse: 0.81063\n",
      "Epoch 868 of 2000. Loss per movie: 0.7451349853335323, test rmse: 0.89472, train rmse: 0.81060\n",
      "Epoch 869 of 2000. Loss per movie: 0.7450820709380469, test rmse: 0.89472, train rmse: 0.81057\n",
      "Epoch 870 of 2000. Loss per movie: 0.7450292657586822, test rmse: 0.89472, train rmse: 0.81054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 871 of 2000. Loss per movie: 0.7449765949555441, test rmse: 0.89471, train rmse: 0.81051\n",
      "Epoch 872 of 2000. Loss per movie: 0.7449240440704309, test rmse: 0.89471, train rmse: 0.81048\n",
      "Epoch 873 of 2000. Loss per movie: 0.7448715887228459, test rmse: 0.89471, train rmse: 0.81045\n",
      "Epoch 874 of 2000. Loss per movie: 0.744819280508724, test rmse: 0.89471, train rmse: 0.81043\n",
      "Epoch 875 of 2000. Loss per movie: 0.7447670739272545, test rmse: 0.89470, train rmse: 0.81040\n",
      "Epoch 876 of 2000. Loss per movie: 0.7447149786881119, test rmse: 0.89470, train rmse: 0.81037\n",
      "Epoch 877 of 2000. Loss per movie: 0.7446630188174254, test rmse: 0.89470, train rmse: 0.81034\n",
      "Epoch 878 of 2000. Loss per movie: 0.7446111792191313, test rmse: 0.89470, train rmse: 0.81031\n",
      "Epoch 879 of 2000. Loss per movie: 0.7445594350166185, test rmse: 0.89469, train rmse: 0.81029\n",
      "Epoch 880 of 2000. Loss per movie: 0.7445078083224302, test rmse: 0.89469, train rmse: 0.81026\n",
      "Epoch 881 of 2000. Loss per movie: 0.7444563012627727, test rmse: 0.89469, train rmse: 0.81023\n",
      "Epoch 882 of 2000. Loss per movie: 0.7444049210667468, test rmse: 0.89469, train rmse: 0.81020\n",
      "Epoch 883 of 2000. Loss per movie: 0.7443536305257454, test rmse: 0.89468, train rmse: 0.81017\n",
      "Epoch 884 of 2000. Loss per movie: 0.7443024440270967, test rmse: 0.89468, train rmse: 0.81015\n",
      "Epoch 885 of 2000. Loss per movie: 0.7442514189783657, test rmse: 0.89468, train rmse: 0.81012\n",
      "Epoch 886 of 2000. Loss per movie: 0.7442004553061178, test rmse: 0.89468, train rmse: 0.81009\n",
      "Epoch 887 of 2000. Loss per movie: 0.744149627923682, test rmse: 0.89468, train rmse: 0.81006\n",
      "Epoch 888 of 2000. Loss per movie: 0.7440988772972869, test rmse: 0.89467, train rmse: 0.81004\n",
      "Epoch 889 of 2000. Loss per movie: 0.7440482620393478, test rmse: 0.89467, train rmse: 0.81001\n",
      "Epoch 890 of 2000. Loss per movie: 0.7439977646441012, test rmse: 0.89467, train rmse: 0.80998\n",
      "Epoch 891 of 2000. Loss per movie: 0.7439473632824973, test rmse: 0.89467, train rmse: 0.80995\n",
      "Epoch 892 of 2000. Loss per movie: 0.7438970528516418, test rmse: 0.89466, train rmse: 0.80993\n",
      "Epoch 893 of 2000. Loss per movie: 0.7438468504320569, test rmse: 0.89466, train rmse: 0.80990\n",
      "Epoch 894 of 2000. Loss per movie: 0.7437967606305226, test rmse: 0.89466, train rmse: 0.80987\n",
      "Epoch 895 of 2000. Loss per movie: 0.7437467828091769, test rmse: 0.89466, train rmse: 0.80985\n",
      "Epoch 896 of 2000. Loss per movie: 0.7436968993205094, test rmse: 0.89466, train rmse: 0.80982\n",
      "Epoch 897 of 2000. Loss per movie: 0.7436471043528898, test rmse: 0.89465, train rmse: 0.80979\n",
      "Epoch 898 of 2000. Loss per movie: 0.7435974066946368, test rmse: 0.89465, train rmse: 0.80976\n",
      "Epoch 899 of 2000. Loss per movie: 0.7435478344116709, test rmse: 0.89465, train rmse: 0.80974\n",
      "Epoch 900 of 2000. Loss per movie: 0.7434983429954112, test rmse: 0.89465, train rmse: 0.80971\n",
      "Epoch 901 of 2000. Loss per movie: 0.7434489615040075, test rmse: 0.89465, train rmse: 0.80968\n",
      "Epoch 902 of 2000. Loss per movie: 0.7433996613754246, test rmse: 0.89464, train rmse: 0.80966\n",
      "Epoch 903 of 2000. Loss per movie: 0.7433504647930794, test rmse: 0.89464, train rmse: 0.80963\n",
      "Epoch 904 of 2000. Loss per movie: 0.7433013767889932, test rmse: 0.89464, train rmse: 0.80960\n",
      "Epoch 905 of 2000. Loss per movie: 0.7432523606506738, test rmse: 0.89464, train rmse: 0.80958\n",
      "Epoch 906 of 2000. Loss per movie: 0.7432034491925688, test rmse: 0.89464, train rmse: 0.80955\n",
      "Epoch 907 of 2000. Loss per movie: 0.7431546324923829, test rmse: 0.89463, train rmse: 0.80952\n",
      "Epoch 908 of 2000. Loss per movie: 0.7431059045258657, test rmse: 0.89463, train rmse: 0.80950\n",
      "Epoch 909 of 2000. Loss per movie: 0.7430572748609445, test rmse: 0.89463, train rmse: 0.80947\n",
      "Epoch 910 of 2000. Loss per movie: 0.7430087094783216, test rmse: 0.89463, train rmse: 0.80944\n",
      "Epoch 911 of 2000. Loss per movie: 0.7429602601865526, test rmse: 0.89463, train rmse: 0.80942\n",
      "Epoch 912 of 2000. Loss per movie: 0.7429118911236277, test rmse: 0.89463, train rmse: 0.80939\n",
      "Epoch 913 of 2000. Loss per movie: 0.7428635988167435, test rmse: 0.89462, train rmse: 0.80937\n",
      "Epoch 914 of 2000. Loss per movie: 0.7428154302550553, test rmse: 0.89462, train rmse: 0.80934\n",
      "Epoch 915 of 2000. Loss per movie: 0.7427673165494848, test rmse: 0.89462, train rmse: 0.80931\n",
      "Epoch 916 of 2000. Loss per movie: 0.7427192914358359, test rmse: 0.89462, train rmse: 0.80929\n",
      "Epoch 917 of 2000. Loss per movie: 0.7426713604422444, test rmse: 0.89462, train rmse: 0.80926\n",
      "Epoch 918 of 2000. Loss per movie: 0.7426234969202602, test rmse: 0.89462, train rmse: 0.80924\n",
      "Epoch 919 of 2000. Loss per movie: 0.7425757638055844, test rmse: 0.89461, train rmse: 0.80921\n",
      "Epoch 920 of 2000. Loss per movie: 0.7425280656315626, test rmse: 0.89461, train rmse: 0.80918\n",
      "Epoch 921 of 2000. Loss per movie: 0.7424804722086286, test rmse: 0.89461, train rmse: 0.80916\n",
      "Epoch 922 of 2000. Loss per movie: 0.7424329751028317, test rmse: 0.89461, train rmse: 0.80913\n",
      "Epoch 923 of 2000. Loss per movie: 0.7423855549656962, test rmse: 0.89461, train rmse: 0.80911\n",
      "Epoch 924 of 2000. Loss per movie: 0.7423382208690349, test rmse: 0.89461, train rmse: 0.80908\n",
      "Epoch 925 of 2000. Loss per movie: 0.7422909740176977, test rmse: 0.89460, train rmse: 0.80906\n",
      "Epoch 926 of 2000. Loss per movie: 0.7422438122854789, test rmse: 0.89460, train rmse: 0.80903\n",
      "Epoch 927 of 2000. Loss per movie: 0.7421967158277877, test rmse: 0.89460, train rmse: 0.80900\n",
      "Epoch 928 of 2000. Loss per movie: 0.7421496996698141, test rmse: 0.89460, train rmse: 0.80898\n",
      "Epoch 929 of 2000. Loss per movie: 0.7421028093123692, test rmse: 0.89460, train rmse: 0.80895\n",
      "Epoch 930 of 2000. Loss per movie: 0.7420559889779789, test rmse: 0.89460, train rmse: 0.80893\n",
      "Epoch 931 of 2000. Loss per movie: 0.7420092373909196, test rmse: 0.89460, train rmse: 0.80890\n",
      "Epoch 932 of 2000. Loss per movie: 0.7419625800656648, test rmse: 0.89459, train rmse: 0.80888\n",
      "Epoch 933 of 2000. Loss per movie: 0.7419160301846925, test rmse: 0.89459, train rmse: 0.80885\n",
      "Epoch 934 of 2000. Loss per movie: 0.7418695327501377, test rmse: 0.89459, train rmse: 0.80883\n",
      "Epoch 935 of 2000. Loss per movie: 0.7418231325540758, test rmse: 0.89459, train rmse: 0.80880\n",
      "Epoch 936 of 2000. Loss per movie: 0.7417768123742374, test rmse: 0.89459, train rmse: 0.80878\n",
      "Epoch 937 of 2000. Loss per movie: 0.7417306323822557, test rmse: 0.89459, train rmse: 0.80875\n",
      "Epoch 938 of 2000. Loss per movie: 0.7416845046240708, test rmse: 0.89459, train rmse: 0.80873\n",
      "Epoch 939 of 2000. Loss per movie: 0.7416384667335311, test rmse: 0.89459, train rmse: 0.80870\n",
      "Epoch 940 of 2000. Loss per movie: 0.7415925238135311, test rmse: 0.89458, train rmse: 0.80868\n",
      "Epoch 941 of 2000. Loss per movie: 0.741546665162167, test rmse: 0.89458, train rmse: 0.80865\n",
      "Epoch 942 of 2000. Loss per movie: 0.7415009089939376, test rmse: 0.89458, train rmse: 0.80863\n",
      "Epoch 943 of 2000. Loss per movie: 0.7414552214312922, test rmse: 0.89458, train rmse: 0.80860\n",
      "Epoch 944 of 2000. Loss per movie: 0.7414096388323553, test rmse: 0.89458, train rmse: 0.80858\n",
      "Epoch 945 of 2000. Loss per movie: 0.7413641607010124, test rmse: 0.89458, train rmse: 0.80855\n",
      "Epoch 946 of 2000. Loss per movie: 0.7413187472772088, test rmse: 0.89458, train rmse: 0.80853\n",
      "Epoch 947 of 2000. Loss per movie: 0.7412734595121867, test rmse: 0.89458, train rmse: 0.80850\n",
      "Epoch 948 of 2000. Loss per movie: 0.741228263103154, test rmse: 0.89457, train rmse: 0.80848\n",
      "Epoch 949 of 2000. Loss per movie: 0.7411831158094826, test rmse: 0.89457, train rmse: 0.80845\n",
      "Epoch 950 of 2000. Loss per movie: 0.7411380975765224, test rmse: 0.89457, train rmse: 0.80843\n",
      "Epoch 951 of 2000. Loss per movie: 0.7410931590762915, test rmse: 0.89457, train rmse: 0.80840\n",
      "Epoch 952 of 2000. Loss per movie: 0.7410483304300433, test rmse: 0.89457, train rmse: 0.80838\n",
      "Epoch 953 of 2000. Loss per movie: 0.7410035973213233, test rmse: 0.89457, train rmse: 0.80836\n",
      "Epoch 954 of 2000. Loss per movie: 0.7409589314007163, test rmse: 0.89457, train rmse: 0.80833\n",
      "Epoch 955 of 2000. Loss per movie: 0.7409143820670783, test rmse: 0.89457, train rmse: 0.80831\n",
      "Epoch 956 of 2000. Loss per movie: 0.7408699223175911, test rmse: 0.89456, train rmse: 0.80828\n",
      "Epoch 957 of 2000. Loss per movie: 0.7408255447105335, test rmse: 0.89456, train rmse: 0.80826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 958 of 2000. Loss per movie: 0.7407812654759456, test rmse: 0.89456, train rmse: 0.80824\n",
      "Epoch 959 of 2000. Loss per movie: 0.7407370622177896, test rmse: 0.89456, train rmse: 0.80821\n",
      "Epoch 960 of 2000. Loss per movie: 0.7406929831300706, test rmse: 0.89456, train rmse: 0.80819\n",
      "Epoch 961 of 2000. Loss per movie: 0.7406489721518209, test rmse: 0.89456, train rmse: 0.80816\n",
      "Epoch 962 of 2000. Loss per movie: 0.7406050467888041, test rmse: 0.89456, train rmse: 0.80814\n",
      "Epoch 963 of 2000. Loss per movie: 0.7405612290118169, test rmse: 0.89456, train rmse: 0.80812\n",
      "Epoch 964 of 2000. Loss per movie: 0.7405175051422663, test rmse: 0.89456, train rmse: 0.80809\n",
      "Epoch 965 of 2000. Loss per movie: 0.7404738550520681, test rmse: 0.89455, train rmse: 0.80807\n",
      "Epoch 966 of 2000. Loss per movie: 0.7404303183595292, test rmse: 0.89455, train rmse: 0.80804\n",
      "Epoch 967 of 2000. Loss per movie: 0.7403868348221433, test rmse: 0.89455, train rmse: 0.80802\n",
      "Epoch 968 of 2000. Loss per movie: 0.740343449657227, test rmse: 0.89455, train rmse: 0.80800\n",
      "Epoch 969 of 2000. Loss per movie: 0.7403001731414433, test rmse: 0.89455, train rmse: 0.80797\n",
      "Epoch 970 of 2000. Loss per movie: 0.7402569482215945, test rmse: 0.89455, train rmse: 0.80795\n",
      "Epoch 971 of 2000. Loss per movie: 0.7402138289741897, test rmse: 0.89455, train rmse: 0.80793\n",
      "Epoch 972 of 2000. Loss per movie: 0.7401707721663712, test rmse: 0.89455, train rmse: 0.80790\n",
      "Epoch 973 of 2000. Loss per movie: 0.7401278231572027, test rmse: 0.89455, train rmse: 0.80788\n",
      "Epoch 974 of 2000. Loss per movie: 0.7400849660001385, test rmse: 0.89455, train rmse: 0.80786\n",
      "Epoch 975 of 2000. Loss per movie: 0.7400421705739252, test rmse: 0.89454, train rmse: 0.80783\n",
      "Epoch 976 of 2000. Loss per movie: 0.7399994531794762, test rmse: 0.89454, train rmse: 0.80781\n",
      "Epoch 977 of 2000. Loss per movie: 0.7399568318895438, test rmse: 0.89454, train rmse: 0.80779\n",
      "Epoch 978 of 2000. Loss per movie: 0.7399142697790149, test rmse: 0.89454, train rmse: 0.80776\n",
      "Epoch 979 of 2000. Loss per movie: 0.7398718324767849, test rmse: 0.89454, train rmse: 0.80774\n",
      "Epoch 980 of 2000. Loss per movie: 0.7398294235948434, test rmse: 0.89454, train rmse: 0.80772\n",
      "Epoch 981 of 2000. Loss per movie: 0.7397871141484744, test rmse: 0.89454, train rmse: 0.80770\n",
      "Epoch 982 of 2000. Loss per movie: 0.7397448808202846, test rmse: 0.89454, train rmse: 0.80767\n",
      "Epoch 983 of 2000. Loss per movie: 0.7397027360131426, test rmse: 0.89454, train rmse: 0.80765\n",
      "Epoch 984 of 2000. Loss per movie: 0.7396606143816473, test rmse: 0.89454, train rmse: 0.80763\n",
      "Epoch 985 of 2000. Loss per movie: 0.739618617700198, test rmse: 0.89454, train rmse: 0.80760\n",
      "Epoch 986 of 2000. Loss per movie: 0.7395766923884007, test rmse: 0.89453, train rmse: 0.80758\n",
      "Epoch 987 of 2000. Loss per movie: 0.7395348218618477, test rmse: 0.89453, train rmse: 0.80756\n",
      "Epoch 988 of 2000. Loss per movie: 0.7394930285165766, test rmse: 0.89453, train rmse: 0.80754\n",
      "Epoch 989 of 2000. Loss per movie: 0.7394513032807748, test rmse: 0.89453, train rmse: 0.80751\n",
      "Epoch 990 of 2000. Loss per movie: 0.7394096470049247, test rmse: 0.89453, train rmse: 0.80749\n",
      "Epoch 991 of 2000. Loss per movie: 0.7393680464356748, test rmse: 0.89453, train rmse: 0.80747\n",
      "Epoch 992 of 2000. Loss per movie: 0.7393265633747496, test rmse: 0.89453, train rmse: 0.80744\n",
      "Epoch 993 of 2000. Loss per movie: 0.7392850764866533, test rmse: 0.89453, train rmse: 0.80742\n",
      "Epoch 994 of 2000. Loss per movie: 0.7392437071068817, test rmse: 0.89453, train rmse: 0.80740\n",
      "Epoch 995 of 2000. Loss per movie: 0.7392023779832793, test rmse: 0.89453, train rmse: 0.80738\n",
      "Epoch 996 of 2000. Loss per movie: 0.7391611360341275, test rmse: 0.89453, train rmse: 0.80735\n",
      "Epoch 997 of 2000. Loss per movie: 0.7391199387353042, test rmse: 0.89453, train rmse: 0.80733\n",
      "Epoch 998 of 2000. Loss per movie: 0.7390788275478286, test rmse: 0.89452, train rmse: 0.80731\n",
      "Epoch 999 of 2000. Loss per movie: 0.7390377712873443, test rmse: 0.89452, train rmse: 0.80729\n",
      "Epoch 1000 of 2000. Loss per movie: 0.7389967846955471, test rmse: 0.89452, train rmse: 0.80727\n",
      "Epoch 1001 of 2000. Loss per movie: 0.7389558475734788, test rmse: 0.89452, train rmse: 0.80724\n",
      "Epoch 1002 of 2000. Loss per movie: 0.7389149793404887, test rmse: 0.89452, train rmse: 0.80722\n",
      "Epoch 1003 of 2000. Loss per movie: 0.7388741843198624, test rmse: 0.89452, train rmse: 0.80720\n",
      "Epoch 1004 of 2000. Loss per movie: 0.7388334483368927, test rmse: 0.89452, train rmse: 0.80718\n",
      "Epoch 1005 of 2000. Loss per movie: 0.7387927798964039, test rmse: 0.89452, train rmse: 0.80715\n",
      "Epoch 1006 of 2000. Loss per movie: 0.7387521697139626, test rmse: 0.89452, train rmse: 0.80713\n",
      "Epoch 1007 of 2000. Loss per movie: 0.7387116210497517, test rmse: 0.89452, train rmse: 0.80711\n",
      "Epoch 1008 of 2000. Loss per movie: 0.7386711606939681, test rmse: 0.89452, train rmse: 0.80709\n",
      "Epoch 1009 of 2000. Loss per movie: 0.738630732727391, test rmse: 0.89452, train rmse: 0.80707\n",
      "Epoch 1010 of 2000. Loss per movie: 0.7385903823673371, test rmse: 0.89452, train rmse: 0.80704\n",
      "Epoch 1011 of 2000. Loss per movie: 0.7385501159924247, test rmse: 0.89451, train rmse: 0.80702\n",
      "Epoch 1012 of 2000. Loss per movie: 0.7385098822902131, test rmse: 0.89451, train rmse: 0.80700\n",
      "Epoch 1013 of 2000. Loss per movie: 0.7384697251314215, test rmse: 0.89451, train rmse: 0.80698\n",
      "Epoch 1014 of 2000. Loss per movie: 0.7384296611713315, test rmse: 0.89451, train rmse: 0.80696\n",
      "Epoch 1015 of 2000. Loss per movie: 0.7383896273324946, test rmse: 0.89451, train rmse: 0.80693\n",
      "Epoch 1016 of 2000. Loss per movie: 0.7383496670603894, test rmse: 0.89451, train rmse: 0.80691\n",
      "Epoch 1017 of 2000. Loss per movie: 0.7383097526434628, test rmse: 0.89451, train rmse: 0.80689\n",
      "Epoch 1018 of 2000. Loss per movie: 0.7382699098088087, test rmse: 0.89451, train rmse: 0.80687\n",
      "Epoch 1019 of 2000. Loss per movie: 0.7382301458564015, test rmse: 0.89451, train rmse: 0.80685\n",
      "Epoch 1020 of 2000. Loss per movie: 0.7381904206009455, test rmse: 0.89451, train rmse: 0.80683\n",
      "Epoch 1021 of 2000. Loss per movie: 0.738150747366666, test rmse: 0.89451, train rmse: 0.80680\n",
      "Epoch 1022 of 2000. Loss per movie: 0.7381111535816215, test rmse: 0.89451, train rmse: 0.80678\n",
      "Epoch 1023 of 2000. Loss per movie: 0.7380715949498515, test rmse: 0.89451, train rmse: 0.80676\n",
      "Epoch 1024 of 2000. Loss per movie: 0.7380321049945391, test rmse: 0.89450, train rmse: 0.80674\n",
      "Epoch 1025 of 2000. Loss per movie: 0.7379926550119017, test rmse: 0.89450, train rmse: 0.80672\n",
      "Epoch 1026 of 2000. Loss per movie: 0.7379532360718735, test rmse: 0.89450, train rmse: 0.80670\n",
      "Epoch 1027 of 2000. Loss per movie: 0.7379138878636354, test rmse: 0.89450, train rmse: 0.80668\n",
      "Epoch 1028 of 2000. Loss per movie: 0.7378745543262196, test rmse: 0.89450, train rmse: 0.80665\n",
      "Epoch 1029 of 2000. Loss per movie: 0.7378352769915187, test rmse: 0.89450, train rmse: 0.80663\n",
      "Epoch 1030 of 2000. Loss per movie: 0.7377960178004433, test rmse: 0.89450, train rmse: 0.80661\n",
      "Epoch 1031 of 2000. Loss per movie: 0.7377568122606357, test rmse: 0.89450, train rmse: 0.80659\n",
      "Epoch 1032 of 2000. Loss per movie: 0.7377176186984558, test rmse: 0.89450, train rmse: 0.80657\n",
      "Epoch 1033 of 2000. Loss per movie: 0.7376784349168239, test rmse: 0.89450, train rmse: 0.80655\n",
      "Epoch 1034 of 2000. Loss per movie: 0.7376392650264055, test rmse: 0.89450, train rmse: 0.80653\n",
      "Epoch 1035 of 2000. Loss per movie: 0.7376001071136148, test rmse: 0.89450, train rmse: 0.80650\n",
      "Epoch 1036 of 2000. Loss per movie: 0.7375609433183206, test rmse: 0.89450, train rmse: 0.80648\n",
      "Epoch 1037 of 2000. Loss per movie: 0.7375217809404969, test rmse: 0.89450, train rmse: 0.80646\n",
      "Epoch 1038 of 2000. Loss per movie: 0.7374826099869755, test rmse: 0.89450, train rmse: 0.80644\n",
      "Epoch 1039 of 2000. Loss per movie: 0.7374434105422919, test rmse: 0.89450, train rmse: 0.80642\n",
      "Epoch 1040 of 2000. Loss per movie: 0.7374042440538032, test rmse: 0.89450, train rmse: 0.80640\n",
      "Epoch 1041 of 2000. Loss per movie: 0.7373650495702673, test rmse: 0.89450, train rmse: 0.80638\n",
      "Epoch 1042 of 2000. Loss per movie: 0.7373258820186755, test rmse: 0.89449, train rmse: 0.80636\n",
      "Epoch 1043 of 2000. Loss per movie: 0.7372867390602013, test rmse: 0.89449, train rmse: 0.80633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1044 of 2000. Loss per movie: 0.7372475958891063, test rmse: 0.89449, train rmse: 0.80631\n",
      "Epoch 1045 of 2000. Loss per movie: 0.7372084856033325, test rmse: 0.89449, train rmse: 0.80629\n",
      "Epoch 1046 of 2000. Loss per movie: 0.7371694703589722, test rmse: 0.89449, train rmse: 0.80627\n",
      "Epoch 1047 of 2000. Loss per movie: 0.7371305167037157, test rmse: 0.89449, train rmse: 0.80625\n",
      "Epoch 1048 of 2000. Loss per movie: 0.7370916499394157, test rmse: 0.89449, train rmse: 0.80623\n",
      "Epoch 1049 of 2000. Loss per movie: 0.7370528583010652, test rmse: 0.89449, train rmse: 0.80621\n",
      "Epoch 1050 of 2000. Loss per movie: 0.7370141724769057, test rmse: 0.89449, train rmse: 0.80619\n",
      "Epoch 1051 of 2000. Loss per movie: 0.7369756002630263, test rmse: 0.89449, train rmse: 0.80616\n",
      "Epoch 1052 of 2000. Loss per movie: 0.7369370820547823, test rmse: 0.89449, train rmse: 0.80614\n",
      "Epoch 1053 of 2000. Loss per movie: 0.7368986761102212, test rmse: 0.89449, train rmse: 0.80612\n",
      "Epoch 1054 of 2000. Loss per movie: 0.7368603671206588, test rmse: 0.89449, train rmse: 0.80610\n",
      "Epoch 1055 of 2000. Loss per movie: 0.7368221342492756, test rmse: 0.89449, train rmse: 0.80608\n",
      "Epoch 1056 of 2000. Loss per movie: 0.7367840087513011, test rmse: 0.89449, train rmse: 0.80606\n",
      "Epoch 1057 of 2000. Loss per movie: 0.7367459434249597, test rmse: 0.89449, train rmse: 0.80604\n",
      "Epoch 1058 of 2000. Loss per movie: 0.736707966194425, test rmse: 0.89448, train rmse: 0.80602\n",
      "Epoch 1059 of 2000. Loss per movie: 0.7366700543092916, test rmse: 0.89448, train rmse: 0.80600\n",
      "Epoch 1060 of 2000. Loss per movie: 0.7366322337801476, test rmse: 0.89448, train rmse: 0.80598\n",
      "Epoch 1061 of 2000. Loss per movie: 0.7365944631459739, test rmse: 0.89448, train rmse: 0.80596\n",
      "Epoch 1062 of 2000. Loss per movie: 0.7365567665746464, test rmse: 0.89448, train rmse: 0.80594\n",
      "Epoch 1063 of 2000. Loss per movie: 0.7365191359157086, test rmse: 0.89448, train rmse: 0.80592\n",
      "Epoch 1064 of 2000. Loss per movie: 0.736481576272055, test rmse: 0.89448, train rmse: 0.80590\n",
      "Epoch 1065 of 2000. Loss per movie: 0.736444051002067, test rmse: 0.89448, train rmse: 0.80587\n",
      "Epoch 1066 of 2000. Loss per movie: 0.7364066170880684, test rmse: 0.89448, train rmse: 0.80585\n",
      "Epoch 1067 of 2000. Loss per movie: 0.7363692365418433, test rmse: 0.89448, train rmse: 0.80583\n",
      "Epoch 1068 of 2000. Loss per movie: 0.7363318980236258, test rmse: 0.89448, train rmse: 0.80581\n",
      "Epoch 1069 of 2000. Loss per movie: 0.7362946214488797, test rmse: 0.89447, train rmse: 0.80579\n",
      "Epoch 1070 of 2000. Loss per movie: 0.7362573965409422, test rmse: 0.89447, train rmse: 0.80577\n",
      "Epoch 1071 of 2000. Loss per movie: 0.7362202170629422, test rmse: 0.89447, train rmse: 0.80575\n",
      "Epoch 1072 of 2000. Loss per movie: 0.7361831100176971, test rmse: 0.89447, train rmse: 0.80573\n",
      "Epoch 1073 of 2000. Loss per movie: 0.7361460436538624, test rmse: 0.89447, train rmse: 0.80571\n",
      "Epoch 1074 of 2000. Loss per movie: 0.736109031295663, test rmse: 0.89447, train rmse: 0.80569\n",
      "Epoch 1075 of 2000. Loss per movie: 0.7360720916537127, test rmse: 0.89447, train rmse: 0.80567\n",
      "Epoch 1076 of 2000. Loss per movie: 0.7360351893621165, test rmse: 0.89447, train rmse: 0.80565\n",
      "Epoch 1077 of 2000. Loss per movie: 0.7359983208063241, test rmse: 0.89447, train rmse: 0.80563\n",
      "Epoch 1078 of 2000. Loss per movie: 0.7359614878999209, test rmse: 0.89447, train rmse: 0.80561\n",
      "Epoch 1079 of 2000. Loss per movie: 0.735924755704814, test rmse: 0.89446, train rmse: 0.80559\n",
      "Epoch 1080 of 2000. Loss per movie: 0.7358880558989136, test rmse: 0.89446, train rmse: 0.80557\n",
      "Epoch 1081 of 2000. Loss per movie: 0.7358514190287142, test rmse: 0.89446, train rmse: 0.80555\n",
      "Epoch 1082 of 2000. Loss per movie: 0.7358147983885549, test rmse: 0.89446, train rmse: 0.80553\n",
      "Epoch 1083 of 2000. Loss per movie: 0.7357782644267314, test rmse: 0.89446, train rmse: 0.80551\n",
      "Epoch 1084 of 2000. Loss per movie: 0.7357417517269692, test rmse: 0.89446, train rmse: 0.80549\n",
      "Epoch 1085 of 2000. Loss per movie: 0.7357053240045781, test rmse: 0.89446, train rmse: 0.80547\n",
      "Epoch 1086 of 2000. Loss per movie: 0.7356689137879506, test rmse: 0.89446, train rmse: 0.80545\n",
      "Epoch 1087 of 2000. Loss per movie: 0.7356325561594877, test rmse: 0.89446, train rmse: 0.80543\n",
      "Epoch 1088 of 2000. Loss per movie: 0.7355962684123326, test rmse: 0.89446, train rmse: 0.80541\n",
      "Epoch 1089 of 2000. Loss per movie: 0.7355600146844753, test rmse: 0.89445, train rmse: 0.80539\n",
      "Epoch 1090 of 2000. Loss per movie: 0.7355238391301294, test rmse: 0.89445, train rmse: 0.80537\n",
      "Epoch 1091 of 2000. Loss per movie: 0.7354876806563061, test rmse: 0.89445, train rmse: 0.80535\n",
      "Epoch 1092 of 2000. Loss per movie: 0.7354515752667621, test rmse: 0.89445, train rmse: 0.80533\n",
      "Epoch 1093 of 2000. Loss per movie: 0.7354155082906753, test rmse: 0.89445, train rmse: 0.80531\n",
      "Epoch 1094 of 2000. Loss per movie: 0.7353795242366269, test rmse: 0.89445, train rmse: 0.80529\n",
      "Epoch 1095 of 2000. Loss per movie: 0.7353436002124646, test rmse: 0.89445, train rmse: 0.80527\n",
      "Epoch 1096 of 2000. Loss per movie: 0.7353076842678855, test rmse: 0.89445, train rmse: 0.80526\n",
      "Epoch 1097 of 2000. Loss per movie: 0.7352718396220848, test rmse: 0.89445, train rmse: 0.80524\n",
      "Epoch 1098 of 2000. Loss per movie: 0.7352360267985023, test rmse: 0.89445, train rmse: 0.80522\n",
      "Epoch 1099 of 2000. Loss per movie: 0.7352002869746631, test rmse: 0.89444, train rmse: 0.80520\n",
      "Epoch 1100 of 2000. Loss per movie: 0.735164595061335, test rmse: 0.89444, train rmse: 0.80518\n",
      "Epoch 1101 of 2000. Loss per movie: 0.7351289330566397, test rmse: 0.89444, train rmse: 0.80516\n",
      "Epoch 1102 of 2000. Loss per movie: 0.7350933239944767, test rmse: 0.89444, train rmse: 0.80514\n",
      "Epoch 1103 of 2000. Loss per movie: 0.7350577766631644, test rmse: 0.89444, train rmse: 0.80512\n",
      "Epoch 1104 of 2000. Loss per movie: 0.7350222845423378, test rmse: 0.89444, train rmse: 0.80510\n",
      "Epoch 1105 of 2000. Loss per movie: 0.7349868252359587, test rmse: 0.89444, train rmse: 0.80508\n",
      "Epoch 1106 of 2000. Loss per movie: 0.7349514037051751, test rmse: 0.89444, train rmse: 0.80506\n",
      "Epoch 1107 of 2000. Loss per movie: 0.7349160546780199, test rmse: 0.89444, train rmse: 0.80504\n",
      "Epoch 1108 of 2000. Loss per movie: 0.7348807239362374, test rmse: 0.89443, train rmse: 0.80502\n",
      "Epoch 1109 of 2000. Loss per movie: 0.7348454534369615, test rmse: 0.89443, train rmse: 0.80500\n",
      "Epoch 1110 of 2000. Loss per movie: 0.7348102345336206, test rmse: 0.89443, train rmse: 0.80498\n",
      "Epoch 1111 of 2000. Loss per movie: 0.7347750424913504, test rmse: 0.89443, train rmse: 0.80496\n",
      "Epoch 1112 of 2000. Loss per movie: 0.7347398933984437, test rmse: 0.89443, train rmse: 0.80495\n",
      "Epoch 1113 of 2000. Loss per movie: 0.7347047889558653, test rmse: 0.89443, train rmse: 0.80493\n",
      "Epoch 1114 of 2000. Loss per movie: 0.7346697517722739, test rmse: 0.89443, train rmse: 0.80491\n",
      "Epoch 1115 of 2000. Loss per movie: 0.7346347278420339, test rmse: 0.89443, train rmse: 0.80489\n",
      "Epoch 1116 of 2000. Loss per movie: 0.7345997429631128, test rmse: 0.89443, train rmse: 0.80487\n",
      "Epoch 1117 of 2000. Loss per movie: 0.7345648058529558, test rmse: 0.89442, train rmse: 0.80485\n",
      "Epoch 1118 of 2000. Loss per movie: 0.7345299068018882, test rmse: 0.89442, train rmse: 0.80483\n",
      "Epoch 1119 of 2000. Loss per movie: 0.7344950411322567, test rmse: 0.89442, train rmse: 0.80481\n",
      "Epoch 1120 of 2000. Loss per movie: 0.7344602126003588, test rmse: 0.89442, train rmse: 0.80479\n",
      "Epoch 1121 of 2000. Loss per movie: 0.7344254382158432, test rmse: 0.89442, train rmse: 0.80477\n",
      "Epoch 1122 of 2000. Loss per movie: 0.7343906644691893, test rmse: 0.89442, train rmse: 0.80475\n",
      "Epoch 1123 of 2000. Loss per movie: 0.7343559339553932, test rmse: 0.89442, train rmse: 0.80474\n",
      "Epoch 1124 of 2000. Loss per movie: 0.7343212407919513, test rmse: 0.89442, train rmse: 0.80472\n",
      "Epoch 1125 of 2000. Loss per movie: 0.7342865597478841, test rmse: 0.89441, train rmse: 0.80470\n",
      "Epoch 1126 of 2000. Loss per movie: 0.734251918818239, test rmse: 0.89441, train rmse: 0.80468\n",
      "Epoch 1127 of 2000. Loss per movie: 0.7342173153098216, test rmse: 0.89441, train rmse: 0.80466\n",
      "Epoch 1128 of 2000. Loss per movie: 0.734182702942212, test rmse: 0.89441, train rmse: 0.80464\n",
      "Epoch 1129 of 2000. Loss per movie: 0.7341481461394556, test rmse: 0.89441, train rmse: 0.80462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1130 of 2000. Loss per movie: 0.7341135828163338, test rmse: 0.89441, train rmse: 0.80460\n",
      "Epoch 1131 of 2000. Loss per movie: 0.7340790655610111, test rmse: 0.89441, train rmse: 0.80458\n",
      "Epoch 1132 of 2000. Loss per movie: 0.734044569496876, test rmse: 0.89441, train rmse: 0.80457\n",
      "Epoch 1133 of 2000. Loss per movie: 0.7340100878909425, test rmse: 0.89440, train rmse: 0.80455\n",
      "Epoch 1134 of 2000. Loss per movie: 0.7339756051510324, test rmse: 0.89440, train rmse: 0.80453\n",
      "Epoch 1135 of 2000. Loss per movie: 0.7339411759915164, test rmse: 0.89440, train rmse: 0.80451\n",
      "Epoch 1136 of 2000. Loss per movie: 0.7339067409494969, test rmse: 0.89440, train rmse: 0.80449\n",
      "Epoch 1137 of 2000. Loss per movie: 0.7338723514791617, test rmse: 0.89440, train rmse: 0.80447\n",
      "Epoch 1138 of 2000. Loss per movie: 0.7338379724272366, test rmse: 0.89440, train rmse: 0.80445\n",
      "Epoch 1139 of 2000. Loss per movie: 0.7338036056364334, test rmse: 0.89439, train rmse: 0.80443\n",
      "Epoch 1140 of 2000. Loss per movie: 0.7337692866852678, test rmse: 0.89439, train rmse: 0.80442\n",
      "Epoch 1141 of 2000. Loss per movie: 0.7337349953747816, test rmse: 0.89439, train rmse: 0.80440\n",
      "Epoch 1142 of 2000. Loss per movie: 0.7337007256098508, test rmse: 0.89439, train rmse: 0.80438\n",
      "Epoch 1143 of 2000. Loss per movie: 0.7336664868166557, test rmse: 0.89439, train rmse: 0.80436\n",
      "Epoch 1144 of 2000. Loss per movie: 0.7336323108882882, test rmse: 0.89439, train rmse: 0.80434\n",
      "Epoch 1145 of 2000. Loss per movie: 0.7335981263842225, test rmse: 0.89439, train rmse: 0.80432\n",
      "Epoch 1146 of 2000. Loss per movie: 0.7335640050993522, test rmse: 0.89438, train rmse: 0.80430\n",
      "Epoch 1147 of 2000. Loss per movie: 0.7335299483802741, test rmse: 0.89438, train rmse: 0.80428\n",
      "Epoch 1148 of 2000. Loss per movie: 0.7334958928660462, test rmse: 0.89438, train rmse: 0.80427\n",
      "Epoch 1149 of 2000. Loss per movie: 0.7334619088632173, test rmse: 0.89438, train rmse: 0.80425\n",
      "Epoch 1150 of 2000. Loss per movie: 0.7334279652583047, test rmse: 0.89438, train rmse: 0.80423\n",
      "Epoch 1151 of 2000. Loss per movie: 0.7333940445455445, test rmse: 0.89438, train rmse: 0.80421\n",
      "Epoch 1152 of 2000. Loss per movie: 0.7333601722394102, test rmse: 0.89437, train rmse: 0.80419\n",
      "Epoch 1153 of 2000. Loss per movie: 0.7333263415360423, test rmse: 0.89437, train rmse: 0.80417\n",
      "Epoch 1154 of 2000. Loss per movie: 0.7332925737683753, test rmse: 0.89437, train rmse: 0.80415\n",
      "Epoch 1155 of 2000. Loss per movie: 0.7332588550451962, test rmse: 0.89437, train rmse: 0.80414\n",
      "Epoch 1156 of 2000. Loss per movie: 0.733225147945277, test rmse: 0.89437, train rmse: 0.80412\n",
      "Epoch 1157 of 2000. Loss per movie: 0.7331914964810846, test rmse: 0.89437, train rmse: 0.80410\n",
      "Epoch 1158 of 2000. Loss per movie: 0.7331578785400753, test rmse: 0.89436, train rmse: 0.80408\n",
      "Epoch 1159 of 2000. Loss per movie: 0.7331243133998513, test rmse: 0.89436, train rmse: 0.80406\n",
      "Epoch 1160 of 2000. Loss per movie: 0.733090767891597, test rmse: 0.89436, train rmse: 0.80404\n",
      "Epoch 1161 of 2000. Loss per movie: 0.7330572589540878, test rmse: 0.89436, train rmse: 0.80403\n",
      "Epoch 1162 of 2000. Loss per movie: 0.7330237919737127, test rmse: 0.89436, train rmse: 0.80401\n",
      "Epoch 1163 of 2000. Loss per movie: 0.7329903605009798, test rmse: 0.89435, train rmse: 0.80399\n",
      "Epoch 1164 of 2000. Loss per movie: 0.7329569736785753, test rmse: 0.89435, train rmse: 0.80397\n",
      "Epoch 1165 of 2000. Loss per movie: 0.7329235891949974, test rmse: 0.89435, train rmse: 0.80395\n",
      "Epoch 1166 of 2000. Loss per movie: 0.7328902276035721, test rmse: 0.89435, train rmse: 0.80393\n",
      "Epoch 1167 of 2000. Loss per movie: 0.732856928309986, test rmse: 0.89435, train rmse: 0.80392\n",
      "Epoch 1168 of 2000. Loss per movie: 0.7328236443959575, test rmse: 0.89435, train rmse: 0.80390\n",
      "Epoch 1169 of 2000. Loss per movie: 0.7327903850041728, test rmse: 0.89434, train rmse: 0.80388\n",
      "Epoch 1170 of 2000. Loss per movie: 0.7327571362434188, test rmse: 0.89434, train rmse: 0.80386\n",
      "Epoch 1171 of 2000. Loss per movie: 0.7327239404960705, test rmse: 0.89434, train rmse: 0.80384\n",
      "Epoch 1172 of 2000. Loss per movie: 0.7326907549545115, test rmse: 0.89434, train rmse: 0.80383\n",
      "Epoch 1173 of 2000. Loss per movie: 0.7326575747284679, test rmse: 0.89434, train rmse: 0.80381\n",
      "Epoch 1174 of 2000. Loss per movie: 0.732624449996404, test rmse: 0.89434, train rmse: 0.80379\n",
      "Epoch 1175 of 2000. Loss per movie: 0.732591319594457, test rmse: 0.89433, train rmse: 0.80377\n",
      "Epoch 1176 of 2000. Loss per movie: 0.7325582193846368, test rmse: 0.89433, train rmse: 0.80375\n",
      "Epoch 1177 of 2000. Loss per movie: 0.7325251468154962, test rmse: 0.89433, train rmse: 0.80374\n",
      "Epoch 1178 of 2000. Loss per movie: 0.7324920930278429, test rmse: 0.89433, train rmse: 0.80372\n",
      "Epoch 1179 of 2000. Loss per movie: 0.7324590519265528, test rmse: 0.89433, train rmse: 0.80370\n",
      "Epoch 1180 of 2000. Loss per movie: 0.7324260233698787, test rmse: 0.89432, train rmse: 0.80368\n",
      "Epoch 1181 of 2000. Loss per movie: 0.732393042085854, test rmse: 0.89432, train rmse: 0.80366\n",
      "Epoch 1182 of 2000. Loss per movie: 0.7323600537144755, test rmse: 0.89432, train rmse: 0.80364\n",
      "Epoch 1183 of 2000. Loss per movie: 0.7323270927711559, test rmse: 0.89432, train rmse: 0.80363\n",
      "Epoch 1184 of 2000. Loss per movie: 0.7322941610277337, test rmse: 0.89432, train rmse: 0.80361\n",
      "Epoch 1185 of 2000. Loss per movie: 0.7322612449473631, test rmse: 0.89431, train rmse: 0.80359\n",
      "Epoch 1186 of 2000. Loss per movie: 0.7322283467979975, test rmse: 0.89431, train rmse: 0.80357\n",
      "Epoch 1187 of 2000. Loss per movie: 0.7321954626815923, test rmse: 0.89431, train rmse: 0.80355\n",
      "Epoch 1188 of 2000. Loss per movie: 0.7321626087573139, test rmse: 0.89431, train rmse: 0.80354\n",
      "Epoch 1189 of 2000. Loss per movie: 0.7321297752446142, test rmse: 0.89431, train rmse: 0.80352\n",
      "Epoch 1190 of 2000. Loss per movie: 0.7320969481814064, test rmse: 0.89430, train rmse: 0.80350\n",
      "Epoch 1191 of 2000. Loss per movie: 0.73206416683163, test rmse: 0.89430, train rmse: 0.80348\n",
      "Epoch 1192 of 2000. Loss per movie: 0.7320313771896498, test rmse: 0.89430, train rmse: 0.80347\n",
      "Epoch 1193 of 2000. Loss per movie: 0.7319986295048037, test rmse: 0.89430, train rmse: 0.80345\n",
      "Epoch 1194 of 2000. Loss per movie: 0.731965880827728, test rmse: 0.89430, train rmse: 0.80343\n",
      "Epoch 1195 of 2000. Loss per movie: 0.7319331946611121, test rmse: 0.89430, train rmse: 0.80341\n",
      "Epoch 1196 of 2000. Loss per movie: 0.7319005082818756, test rmse: 0.89429, train rmse: 0.80339\n",
      "Epoch 1197 of 2000. Loss per movie: 0.731867857126787, test rmse: 0.89429, train rmse: 0.80338\n",
      "Epoch 1198 of 2000. Loss per movie: 0.731835211570708, test rmse: 0.89429, train rmse: 0.80336\n",
      "Epoch 1199 of 2000. Loss per movie: 0.7318026000339267, test rmse: 0.89429, train rmse: 0.80334\n",
      "Epoch 1200 of 2000. Loss per movie: 0.7317700057194151, test rmse: 0.89429, train rmse: 0.80332\n",
      "Epoch 1201 of 2000. Loss per movie: 0.731737440250433, test rmse: 0.89428, train rmse: 0.80330\n",
      "Epoch 1202 of 2000. Loss per movie: 0.7317049154628613, test rmse: 0.89428, train rmse: 0.80329\n",
      "Epoch 1203 of 2000. Loss per movie: 0.731672398117011, test rmse: 0.89428, train rmse: 0.80327\n",
      "Epoch 1204 of 2000. Loss per movie: 0.7316399123098848, test rmse: 0.89428, train rmse: 0.80325\n",
      "Epoch 1205 of 2000. Loss per movie: 0.7316074670424217, test rmse: 0.89428, train rmse: 0.80323\n",
      "Epoch 1206 of 2000. Loss per movie: 0.7315750398477108, test rmse: 0.89428, train rmse: 0.80322\n",
      "Epoch 1207 of 2000. Loss per movie: 0.7315426270403278, test rmse: 0.89427, train rmse: 0.80320\n",
      "Epoch 1208 of 2000. Loss per movie: 0.731510249740587, test rmse: 0.89427, train rmse: 0.80318\n",
      "Epoch 1209 of 2000. Loss per movie: 0.7314779196426219, test rmse: 0.89427, train rmse: 0.80316\n",
      "Epoch 1210 of 2000. Loss per movie: 0.7314456134999124, test rmse: 0.89427, train rmse: 0.80314\n",
      "Epoch 1211 of 2000. Loss per movie: 0.7314133234454959, test rmse: 0.89427, train rmse: 0.80313\n",
      "Epoch 1212 of 2000. Loss per movie: 0.7313810723006513, test rmse: 0.89426, train rmse: 0.80311\n",
      "Epoch 1213 of 2000. Loss per movie: 0.7313488447566945, test rmse: 0.89426, train rmse: 0.80309\n",
      "Epoch 1214 of 2000. Loss per movie: 0.7313166183467144, test rmse: 0.89426, train rmse: 0.80307\n",
      "Epoch 1215 of 2000. Loss per movie: 0.7312844607549388, test rmse: 0.89426, train rmse: 0.80306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1216 of 2000. Loss per movie: 0.7312523080534373, test rmse: 0.89426, train rmse: 0.80304\n",
      "Epoch 1217 of 2000. Loss per movie: 0.7312202041129294, test rmse: 0.89426, train rmse: 0.80302\n",
      "Epoch 1218 of 2000. Loss per movie: 0.7311881107325786, test rmse: 0.89425, train rmse: 0.80300\n",
      "Epoch 1219 of 2000. Loss per movie: 0.7311560443550454, test rmse: 0.89425, train rmse: 0.80299\n",
      "Epoch 1220 of 2000. Loss per movie: 0.731124020147267, test rmse: 0.89425, train rmse: 0.80297\n",
      "Epoch 1221 of 2000. Loss per movie: 0.7310920087675986, test rmse: 0.89425, train rmse: 0.80295\n",
      "Epoch 1222 of 2000. Loss per movie: 0.7310600336043078, test rmse: 0.89425, train rmse: 0.80293\n",
      "Epoch 1223 of 2000. Loss per movie: 0.7310280838846168, test rmse: 0.89425, train rmse: 0.80292\n",
      "Epoch 1224 of 2000. Loss per movie: 0.7309961785317601, test rmse: 0.89424, train rmse: 0.80290\n",
      "Epoch 1225 of 2000. Loss per movie: 0.7309642860778872, test rmse: 0.89424, train rmse: 0.80288\n",
      "Epoch 1226 of 2000. Loss per movie: 0.7309324208394525, test rmse: 0.89424, train rmse: 0.80286\n",
      "Epoch 1227 of 2000. Loss per movie: 0.7309006099610209, test rmse: 0.89424, train rmse: 0.80285\n",
      "Epoch 1228 of 2000. Loss per movie: 0.7308687691739566, test rmse: 0.89424, train rmse: 0.80283\n",
      "Epoch 1229 of 2000. Loss per movie: 0.7308369996856707, test rmse: 0.89424, train rmse: 0.80281\n",
      "Epoch 1230 of 2000. Loss per movie: 0.7308052506089636, test rmse: 0.89423, train rmse: 0.80279\n",
      "Epoch 1231 of 2000. Loss per movie: 0.7307735198885025, test rmse: 0.89423, train rmse: 0.80278\n",
      "Epoch 1232 of 2000. Loss per movie: 0.7307418139737795, test rmse: 0.89423, train rmse: 0.80276\n",
      "Epoch 1233 of 2000. Loss per movie: 0.7307101481026049, test rmse: 0.89423, train rmse: 0.80274\n",
      "Epoch 1234 of 2000. Loss per movie: 0.73067852610215, test rmse: 0.89423, train rmse: 0.80272\n",
      "Epoch 1235 of 2000. Loss per movie: 0.7306469253637561, test rmse: 0.89423, train rmse: 0.80271\n",
      "Epoch 1236 of 2000. Loss per movie: 0.7306153391544373, test rmse: 0.89423, train rmse: 0.80269\n",
      "Epoch 1237 of 2000. Loss per movie: 0.73058378703529, test rmse: 0.89422, train rmse: 0.80267\n",
      "Epoch 1238 of 2000. Loss per movie: 0.7305522449093115, test rmse: 0.89422, train rmse: 0.80266\n",
      "Epoch 1239 of 2000. Loss per movie: 0.7305207579229448, test rmse: 0.89422, train rmse: 0.80264\n",
      "Epoch 1240 of 2000. Loss per movie: 0.7304892875209859, test rmse: 0.89422, train rmse: 0.80262\n",
      "Epoch 1241 of 2000. Loss per movie: 0.7304578499334746, test rmse: 0.89422, train rmse: 0.80260\n",
      "Epoch 1242 of 2000. Loss per movie: 0.7304264540904768, test rmse: 0.89422, train rmse: 0.80259\n",
      "Epoch 1243 of 2000. Loss per movie: 0.7303950730600481, test rmse: 0.89421, train rmse: 0.80257\n",
      "Epoch 1244 of 2000. Loss per movie: 0.7303637257654233, test rmse: 0.89421, train rmse: 0.80255\n",
      "Epoch 1245 of 2000. Loss per movie: 0.7303324240424829, test rmse: 0.89421, train rmse: 0.80253\n",
      "Epoch 1246 of 2000. Loss per movie: 0.7303011398961796, test rmse: 0.89421, train rmse: 0.80252\n",
      "Epoch 1247 of 2000. Loss per movie: 0.730269893950713, test rmse: 0.89421, train rmse: 0.80250\n",
      "Epoch 1248 of 2000. Loss per movie: 0.730238670401284, test rmse: 0.89421, train rmse: 0.80248\n",
      "Epoch 1249 of 2000. Loss per movie: 0.7302075003613757, test rmse: 0.89420, train rmse: 0.80247\n",
      "Epoch 1250 of 2000. Loss per movie: 0.7301763348573737, test rmse: 0.89420, train rmse: 0.80245\n",
      "Epoch 1251 of 2000. Loss per movie: 0.7301452364706115, test rmse: 0.89420, train rmse: 0.80243\n",
      "Epoch 1252 of 2000. Loss per movie: 0.7301141489275004, test rmse: 0.89420, train rmse: 0.80242\n",
      "Epoch 1253 of 2000. Loss per movie: 0.7300831192171956, test rmse: 0.89420, train rmse: 0.80240\n",
      "Epoch 1254 of 2000. Loss per movie: 0.7300520875933054, test rmse: 0.89420, train rmse: 0.80238\n",
      "Epoch 1255 of 2000. Loss per movie: 0.7300211388914537, test rmse: 0.89419, train rmse: 0.80236\n",
      "Epoch 1256 of 2000. Loss per movie: 0.7299902039390682, test rmse: 0.89419, train rmse: 0.80235\n",
      "Epoch 1257 of 2000. Loss per movie: 0.7299593088884841, test rmse: 0.89419, train rmse: 0.80233\n",
      "Epoch 1258 of 2000. Loss per movie: 0.7299284456601183, test rmse: 0.89419, train rmse: 0.80231\n",
      "Epoch 1259 of 2000. Loss per movie: 0.7298976023472163, test rmse: 0.89419, train rmse: 0.80230\n",
      "Epoch 1260 of 2000. Loss per movie: 0.7298668408223763, test rmse: 0.89419, train rmse: 0.80228\n",
      "Epoch 1261 of 2000. Loss per movie: 0.7298360744781358, test rmse: 0.89419, train rmse: 0.80226\n",
      "Epoch 1262 of 2000. Loss per movie: 0.7298053697229991, test rmse: 0.89418, train rmse: 0.80225\n",
      "Epoch 1263 of 2000. Loss per movie: 0.7297746780085932, test rmse: 0.89418, train rmse: 0.80223\n",
      "Epoch 1264 of 2000. Loss per movie: 0.7297440542619095, test rmse: 0.89418, train rmse: 0.80221\n",
      "Epoch 1265 of 2000. Loss per movie: 0.7297134381695678, test rmse: 0.89418, train rmse: 0.80220\n",
      "Epoch 1266 of 2000. Loss per movie: 0.7296828839498241, test rmse: 0.89418, train rmse: 0.80218\n",
      "Epoch 1267 of 2000. Loss per movie: 0.729652362544528, test rmse: 0.89418, train rmse: 0.80216\n",
      "Epoch 1268 of 2000. Loss per movie: 0.7296218699138882, test rmse: 0.89417, train rmse: 0.80215\n",
      "Epoch 1269 of 2000. Loss per movie: 0.7295914268238508, test rmse: 0.89417, train rmse: 0.80213\n",
      "Epoch 1270 of 2000. Loss per movie: 0.7295610159103992, test rmse: 0.89417, train rmse: 0.80211\n",
      "Epoch 1271 of 2000. Loss per movie: 0.7295306414968193, test rmse: 0.89417, train rmse: 0.80210\n",
      "Epoch 1272 of 2000. Loss per movie: 0.729500299472446, test rmse: 0.89417, train rmse: 0.80208\n",
      "Epoch 1273 of 2000. Loss per movie: 0.7294700017440333, test rmse: 0.89417, train rmse: 0.80206\n",
      "Epoch 1274 of 2000. Loss per movie: 0.7294397411533542, test rmse: 0.89417, train rmse: 0.80205\n",
      "Epoch 1275 of 2000. Loss per movie: 0.7294095159994436, test rmse: 0.89416, train rmse: 0.80203\n",
      "Epoch 1276 of 2000. Loss per movie: 0.7293793299677256, test rmse: 0.89416, train rmse: 0.80201\n",
      "Epoch 1277 of 2000. Loss per movie: 0.7293491631427361, test rmse: 0.89416, train rmse: 0.80200\n",
      "Epoch 1278 of 2000. Loss per movie: 0.7293190529457029, test rmse: 0.89416, train rmse: 0.80198\n",
      "Epoch 1279 of 2000. Loss per movie: 0.7292889616010305, test rmse: 0.89416, train rmse: 0.80196\n",
      "Epoch 1280 of 2000. Loss per movie: 0.7292589267425673, test rmse: 0.89416, train rmse: 0.80195\n",
      "Epoch 1281 of 2000. Loss per movie: 0.7292289162646008, test rmse: 0.89416, train rmse: 0.80193\n",
      "Epoch 1282 of 2000. Loss per movie: 0.7291989539806397, test rmse: 0.89415, train rmse: 0.80191\n",
      "Epoch 1283 of 2000. Loss per movie: 0.7291690182033815, test rmse: 0.89415, train rmse: 0.80190\n",
      "Epoch 1284 of 2000. Loss per movie: 0.7291391070901143, test rmse: 0.89415, train rmse: 0.80188\n",
      "Epoch 1285 of 2000. Loss per movie: 0.7291092491319999, test rmse: 0.89415, train rmse: 0.80186\n",
      "Epoch 1286 of 2000. Loss per movie: 0.7290793981194922, test rmse: 0.89415, train rmse: 0.80185\n",
      "Epoch 1287 of 2000. Loss per movie: 0.7290496106096738, test rmse: 0.89415, train rmse: 0.80183\n",
      "Epoch 1288 of 2000. Loss per movie: 0.7290198383376658, test rmse: 0.89415, train rmse: 0.80182\n",
      "Epoch 1289 of 2000. Loss per movie: 0.728990138781907, test rmse: 0.89415, train rmse: 0.80180\n",
      "Epoch 1290 of 2000. Loss per movie: 0.7289604155543868, test rmse: 0.89414, train rmse: 0.80178\n",
      "Epoch 1291 of 2000. Loss per movie: 0.7289307448441577, test rmse: 0.89414, train rmse: 0.80177\n",
      "Epoch 1292 of 2000. Loss per movie: 0.7289011559219905, test rmse: 0.89414, train rmse: 0.80175\n",
      "Epoch 1293 of 2000. Loss per movie: 0.7288715434698089, test rmse: 0.89414, train rmse: 0.80173\n",
      "Epoch 1294 of 2000. Loss per movie: 0.728841965816534, test rmse: 0.89414, train rmse: 0.80172\n",
      "Epoch 1295 of 2000. Loss per movie: 0.7288124513115809, test rmse: 0.89414, train rmse: 0.80170\n",
      "Epoch 1296 of 2000. Loss per movie: 0.7287829584230566, test rmse: 0.89414, train rmse: 0.80169\n",
      "Epoch 1297 of 2000. Loss per movie: 0.7287534836781577, test rmse: 0.89413, train rmse: 0.80167\n",
      "Epoch 1298 of 2000. Loss per movie: 0.7287240521661167, test rmse: 0.89413, train rmse: 0.80165\n",
      "Epoch 1299 of 2000. Loss per movie: 0.7286946443967105, test rmse: 0.89413, train rmse: 0.80164\n",
      "Epoch 1300 of 2000. Loss per movie: 0.7286652920504104, test rmse: 0.89413, train rmse: 0.80162\n",
      "Epoch 1301 of 2000. Loss per movie: 0.7286359516108646, test rmse: 0.89413, train rmse: 0.80161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1302 of 2000. Loss per movie: 0.7286066556799, test rmse: 0.89413, train rmse: 0.80159\n",
      "Epoch 1303 of 2000. Loss per movie: 0.7285773482674226, test rmse: 0.89413, train rmse: 0.80157\n",
      "Epoch 1304 of 2000. Loss per movie: 0.7285481305099696, test rmse: 0.89413, train rmse: 0.80156\n",
      "Epoch 1305 of 2000. Loss per movie: 0.7285189099175752, test rmse: 0.89412, train rmse: 0.80154\n",
      "Epoch 1306 of 2000. Loss per movie: 0.7284897305027058, test rmse: 0.89412, train rmse: 0.80153\n",
      "Epoch 1307 of 2000. Loss per movie: 0.7284605651916704, test rmse: 0.89412, train rmse: 0.80151\n",
      "Epoch 1308 of 2000. Loss per movie: 0.728431428797038, test rmse: 0.89412, train rmse: 0.80149\n",
      "Epoch 1309 of 2000. Loss per movie: 0.7284023299653805, test rmse: 0.89412, train rmse: 0.80148\n",
      "Epoch 1310 of 2000. Loss per movie: 0.7283732876199321, test rmse: 0.89412, train rmse: 0.80146\n",
      "Epoch 1311 of 2000. Loss per movie: 0.7283442569686173, test rmse: 0.89412, train rmse: 0.80145\n",
      "Epoch 1312 of 2000. Loss per movie: 0.7283152343968857, test rmse: 0.89412, train rmse: 0.80143\n",
      "Epoch 1313 of 2000. Loss per movie: 0.7282862547745177, test rmse: 0.89411, train rmse: 0.80141\n",
      "Epoch 1314 of 2000. Loss per movie: 0.7282572929414075, test rmse: 0.89411, train rmse: 0.80140\n",
      "Epoch 1315 of 2000. Loss per movie: 0.7282283578984944, test rmse: 0.89411, train rmse: 0.80138\n",
      "Epoch 1316 of 2000. Loss per movie: 0.728199468994254, test rmse: 0.89411, train rmse: 0.80137\n",
      "Epoch 1317 of 2000. Loss per movie: 0.7281705924928826, test rmse: 0.89411, train rmse: 0.80135\n",
      "Epoch 1318 of 2000. Loss per movie: 0.7281417537671065, test rmse: 0.89411, train rmse: 0.80133\n",
      "Epoch 1319 of 2000. Loss per movie: 0.7281129212073282, test rmse: 0.89411, train rmse: 0.80132\n",
      "Epoch 1320 of 2000. Loss per movie: 0.7280841511580096, test rmse: 0.89411, train rmse: 0.80130\n",
      "Epoch 1321 of 2000. Loss per movie: 0.7280553634611804, test rmse: 0.89410, train rmse: 0.80129\n",
      "Epoch 1322 of 2000. Loss per movie: 0.7280266481262325, test rmse: 0.89410, train rmse: 0.80127\n",
      "Epoch 1323 of 2000. Loss per movie: 0.7279979347757437, test rmse: 0.89410, train rmse: 0.80126\n",
      "Epoch 1324 of 2000. Loss per movie: 0.7279692497746698, test rmse: 0.89410, train rmse: 0.80124\n",
      "Epoch 1325 of 2000. Loss per movie: 0.7279406169365192, test rmse: 0.89410, train rmse: 0.80122\n",
      "Epoch 1326 of 2000. Loss per movie: 0.7279119856575865, test rmse: 0.89410, train rmse: 0.80121\n",
      "Epoch 1327 of 2000. Loss per movie: 0.7278833862717455, test rmse: 0.89410, train rmse: 0.80119\n",
      "Epoch 1328 of 2000. Loss per movie: 0.7278548389779543, test rmse: 0.89410, train rmse: 0.80118\n",
      "Epoch 1329 of 2000. Loss per movie: 0.7278262743910201, test rmse: 0.89410, train rmse: 0.80116\n",
      "Epoch 1330 of 2000. Loss per movie: 0.7277977864183796, test rmse: 0.89409, train rmse: 0.80115\n",
      "Epoch 1331 of 2000. Loss per movie: 0.7277693272912686, test rmse: 0.89409, train rmse: 0.80113\n",
      "Epoch 1332 of 2000. Loss per movie: 0.7277408812757621, test rmse: 0.89409, train rmse: 0.80111\n",
      "Epoch 1333 of 2000. Loss per movie: 0.7277124509233074, test rmse: 0.89409, train rmse: 0.80110\n",
      "Epoch 1334 of 2000. Loss per movie: 0.7276841074618091, test rmse: 0.89409, train rmse: 0.80108\n",
      "Epoch 1335 of 2000. Loss per movie: 0.7276557484790063, test rmse: 0.89409, train rmse: 0.80107\n",
      "Epoch 1336 of 2000. Loss per movie: 0.7276274553377194, test rmse: 0.89409, train rmse: 0.80105\n",
      "Epoch 1337 of 2000. Loss per movie: 0.7275991896244915, test rmse: 0.89409, train rmse: 0.80104\n",
      "Epoch 1338 of 2000. Loss per movie: 0.727570935180156, test rmse: 0.89409, train rmse: 0.80102\n",
      "Epoch 1339 of 2000. Loss per movie: 0.7275427418288096, test rmse: 0.89408, train rmse: 0.80101\n",
      "Epoch 1340 of 2000. Loss per movie: 0.7275146031918338, test rmse: 0.89408, train rmse: 0.80099\n",
      "Epoch 1341 of 2000. Loss per movie: 0.7274864864547811, test rmse: 0.89408, train rmse: 0.80098\n",
      "Epoch 1342 of 2000. Loss per movie: 0.7274583969331666, test rmse: 0.89408, train rmse: 0.80096\n",
      "Epoch 1343 of 2000. Loss per movie: 0.7274303611336934, test rmse: 0.89408, train rmse: 0.80094\n",
      "Epoch 1344 of 2000. Loss per movie: 0.727402363322436, test rmse: 0.89408, train rmse: 0.80093\n",
      "Epoch 1345 of 2000. Loss per movie: 0.7273744254701912, test rmse: 0.89408, train rmse: 0.80091\n",
      "Epoch 1346 of 2000. Loss per movie: 0.7273465205032675, test rmse: 0.89408, train rmse: 0.80090\n",
      "Epoch 1347 of 2000. Loss per movie: 0.7273186556507659, test rmse: 0.89408, train rmse: 0.80088\n",
      "Epoch 1348 of 2000. Loss per movie: 0.7272908289991009, test rmse: 0.89408, train rmse: 0.80087\n",
      "Epoch 1349 of 2000. Loss per movie: 0.7272630357997454, test rmse: 0.89408, train rmse: 0.80085\n",
      "Epoch 1350 of 2000. Loss per movie: 0.7272353143244096, test rmse: 0.89407, train rmse: 0.80084\n",
      "Epoch 1351 of 2000. Loss per movie: 0.7272076242460507, test rmse: 0.89407, train rmse: 0.80082\n",
      "Epoch 1352 of 2000. Loss per movie: 0.7271799618792448, test rmse: 0.89407, train rmse: 0.80081\n",
      "Epoch 1353 of 2000. Loss per movie: 0.72715235663651, test rmse: 0.89407, train rmse: 0.80079\n",
      "Epoch 1354 of 2000. Loss per movie: 0.7271247893819911, test rmse: 0.89407, train rmse: 0.80078\n",
      "Epoch 1355 of 2000. Loss per movie: 0.7270972939223653, test rmse: 0.89407, train rmse: 0.80076\n",
      "Epoch 1356 of 2000. Loss per movie: 0.7270698152597679, test rmse: 0.89407, train rmse: 0.80075\n",
      "Epoch 1357 of 2000. Loss per movie: 0.7270423915241618, test rmse: 0.89407, train rmse: 0.80073\n",
      "Epoch 1358 of 2000. Loss per movie: 0.7270150264718445, test rmse: 0.89407, train rmse: 0.80072\n",
      "Epoch 1359 of 2000. Loss per movie: 0.7269876882097241, test rmse: 0.89407, train rmse: 0.80070\n",
      "Epoch 1360 of 2000. Loss per movie: 0.726960393889197, test rmse: 0.89406, train rmse: 0.80069\n",
      "Epoch 1361 of 2000. Loss per movie: 0.7269331529364432, test rmse: 0.89406, train rmse: 0.80067\n",
      "Epoch 1362 of 2000. Loss per movie: 0.7269059304816827, test rmse: 0.89406, train rmse: 0.80066\n",
      "Epoch 1363 of 2000. Loss per movie: 0.726878792437305, test rmse: 0.89406, train rmse: 0.80064\n",
      "Epoch 1364 of 2000. Loss per movie: 0.7268516698433585, test rmse: 0.89406, train rmse: 0.80063\n",
      "Epoch 1365 of 2000. Loss per movie: 0.7268246248559351, test rmse: 0.89406, train rmse: 0.80061\n",
      "Epoch 1366 of 2000. Loss per movie: 0.7267975851131534, test rmse: 0.89406, train rmse: 0.80060\n",
      "Epoch 1367 of 2000. Loss per movie: 0.7267705791061755, test rmse: 0.89406, train rmse: 0.80058\n",
      "Epoch 1368 of 2000. Loss per movie: 0.7267436628959693, test rmse: 0.89406, train rmse: 0.80057\n",
      "Epoch 1369 of 2000. Loss per movie: 0.7267167683021918, test rmse: 0.89406, train rmse: 0.80055\n",
      "Epoch 1370 of 2000. Loss per movie: 0.7266899176500073, test rmse: 0.89406, train rmse: 0.80054\n",
      "Epoch 1371 of 2000. Loss per movie: 0.7266630968355821, test rmse: 0.89405, train rmse: 0.80052\n",
      "Epoch 1372 of 2000. Loss per movie: 0.7266363437762584, test rmse: 0.89405, train rmse: 0.80051\n",
      "Epoch 1373 of 2000. Loss per movie: 0.7266096241692442, test rmse: 0.89405, train rmse: 0.80049\n",
      "Epoch 1374 of 2000. Loss per movie: 0.7265829194456729, test rmse: 0.89405, train rmse: 0.80048\n",
      "Epoch 1375 of 2000. Loss per movie: 0.7265562894936832, test rmse: 0.89405, train rmse: 0.80046\n",
      "Epoch 1376 of 2000. Loss per movie: 0.7265296921435208, test rmse: 0.89405, train rmse: 0.80045\n",
      "Epoch 1377 of 2000. Loss per movie: 0.7265031335611831, test rmse: 0.89405, train rmse: 0.80044\n",
      "Epoch 1378 of 2000. Loss per movie: 0.7264766360009609, test rmse: 0.89405, train rmse: 0.80042\n",
      "Epoch 1379 of 2000. Loss per movie: 0.7264501528280669, test rmse: 0.89405, train rmse: 0.80041\n",
      "Epoch 1380 of 2000. Loss per movie: 0.726423726283129, test rmse: 0.89404, train rmse: 0.80039\n",
      "Epoch 1381 of 2000. Loss per movie: 0.72639734339629, test rmse: 0.89404, train rmse: 0.80038\n",
      "Epoch 1382 of 2000. Loss per movie: 0.72637096710069, test rmse: 0.89404, train rmse: 0.80036\n",
      "Epoch 1383 of 2000. Loss per movie: 0.7263446462281961, test rmse: 0.89404, train rmse: 0.80035\n",
      "Epoch 1384 of 2000. Loss per movie: 0.7263183722031102, test rmse: 0.89404, train rmse: 0.80033\n",
      "Epoch 1385 of 2000. Loss per movie: 0.7262921472225121, test rmse: 0.89404, train rmse: 0.80032\n",
      "Epoch 1386 of 2000. Loss per movie: 0.726265945063193, test rmse: 0.89404, train rmse: 0.80031\n",
      "Epoch 1387 of 2000. Loss per movie: 0.7262397989648418, test rmse: 0.89404, train rmse: 0.80029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1388 of 2000. Loss per movie: 0.7262136838382263, test rmse: 0.89404, train rmse: 0.80028\n",
      "Epoch 1389 of 2000. Loss per movie: 0.7261875817523417, test rmse: 0.89404, train rmse: 0.80026\n",
      "Epoch 1390 of 2000. Loss per movie: 0.7261615754874793, test rmse: 0.89403, train rmse: 0.80025\n",
      "Epoch 1391 of 2000. Loss per movie: 0.7261355388178695, test rmse: 0.89403, train rmse: 0.80023\n",
      "Epoch 1392 of 2000. Loss per movie: 0.7261095811013801, test rmse: 0.89403, train rmse: 0.80022\n",
      "Epoch 1393 of 2000. Loss per movie: 0.7260836351498979, test rmse: 0.89403, train rmse: 0.80021\n",
      "Epoch 1394 of 2000. Loss per movie: 0.726057755465173, test rmse: 0.89403, train rmse: 0.80019\n",
      "Epoch 1395 of 2000. Loss per movie: 0.726031902428898, test rmse: 0.89403, train rmse: 0.80018\n",
      "Epoch 1396 of 2000. Loss per movie: 0.7260060661187778, test rmse: 0.89403, train rmse: 0.80016\n",
      "Epoch 1397 of 2000. Loss per movie: 0.7259802806958574, test rmse: 0.89403, train rmse: 0.80015\n",
      "Epoch 1398 of 2000. Loss per movie: 0.7259545321271763, test rmse: 0.89403, train rmse: 0.80013\n",
      "Epoch 1399 of 2000. Loss per movie: 0.7259288058127858, test rmse: 0.89402, train rmse: 0.80012\n",
      "Epoch 1400 of 2000. Loss per movie: 0.7259031237234825, test rmse: 0.89402, train rmse: 0.80011\n",
      "Epoch 1401 of 2000. Loss per movie: 0.7258774580059664, test rmse: 0.89402, train rmse: 0.80009\n",
      "Epoch 1402 of 2000. Loss per movie: 0.7258518484911652, test rmse: 0.89402, train rmse: 0.80008\n",
      "Epoch 1403 of 2000. Loss per movie: 0.7258262774606947, test rmse: 0.89402, train rmse: 0.80006\n",
      "Epoch 1404 of 2000. Loss per movie: 0.7258007172738754, test rmse: 0.89402, train rmse: 0.80005\n",
      "Epoch 1405 of 2000. Loss per movie: 0.7257751806170705, test rmse: 0.89402, train rmse: 0.80004\n",
      "Epoch 1406 of 2000. Loss per movie: 0.7257497122823554, test rmse: 0.89402, train rmse: 0.80002\n",
      "Epoch 1407 of 2000. Loss per movie: 0.7257242462155935, test rmse: 0.89401, train rmse: 0.80001\n",
      "Epoch 1408 of 2000. Loss per movie: 0.7256988221059656, test rmse: 0.89401, train rmse: 0.79999\n",
      "Epoch 1409 of 2000. Loss per movie: 0.7256734426466662, test rmse: 0.89401, train rmse: 0.79998\n",
      "Epoch 1410 of 2000. Loss per movie: 0.7256480589349544, test rmse: 0.89401, train rmse: 0.79997\n",
      "Epoch 1411 of 2000. Loss per movie: 0.7256227191648359, test rmse: 0.89401, train rmse: 0.79995\n",
      "Epoch 1412 of 2000. Loss per movie: 0.7255974119256708, test rmse: 0.89401, train rmse: 0.79994\n",
      "Epoch 1413 of 2000. Loss per movie: 0.725572128074773, test rmse: 0.89401, train rmse: 0.79992\n",
      "Epoch 1414 of 2000. Loss per movie: 0.7255468811489881, test rmse: 0.89401, train rmse: 0.79991\n",
      "Epoch 1415 of 2000. Loss per movie: 0.7255216627143651, test rmse: 0.89400, train rmse: 0.79990\n",
      "Epoch 1416 of 2000. Loss per movie: 0.7254964634155971, test rmse: 0.89400, train rmse: 0.79988\n",
      "Epoch 1417 of 2000. Loss per movie: 0.7254712782915367, test rmse: 0.89400, train rmse: 0.79987\n",
      "Epoch 1418 of 2000. Loss per movie: 0.7254461337071395, test rmse: 0.89400, train rmse: 0.79986\n",
      "Epoch 1419 of 2000. Loss per movie: 0.725421015204204, test rmse: 0.89400, train rmse: 0.79984\n",
      "Epoch 1420 of 2000. Loss per movie: 0.7253959241293273, test rmse: 0.89400, train rmse: 0.79983\n",
      "Epoch 1421 of 2000. Loss per movie: 0.7253708380155983, test rmse: 0.89400, train rmse: 0.79981\n",
      "Epoch 1422 of 2000. Loss per movie: 0.7253457896774645, test rmse: 0.89399, train rmse: 0.79980\n",
      "Epoch 1423 of 2000. Loss per movie: 0.7253207545926823, test rmse: 0.89399, train rmse: 0.79979\n",
      "Epoch 1424 of 2000. Loss per movie: 0.7252957406990876, test rmse: 0.89399, train rmse: 0.79977\n",
      "Epoch 1425 of 2000. Loss per movie: 0.7252707642267207, test rmse: 0.89399, train rmse: 0.79976\n",
      "Epoch 1426 of 2000. Loss per movie: 0.7252457876126066, test rmse: 0.89399, train rmse: 0.79974\n",
      "Epoch 1427 of 2000. Loss per movie: 0.7252208472857435, test rmse: 0.89399, train rmse: 0.79973\n",
      "Epoch 1428 of 2000. Loss per movie: 0.7251959124161429, test rmse: 0.89399, train rmse: 0.79972\n",
      "Epoch 1429 of 2000. Loss per movie: 0.7251709831455516, test rmse: 0.89398, train rmse: 0.79970\n",
      "Epoch 1430 of 2000. Loss per movie: 0.7251461050456542, test rmse: 0.89398, train rmse: 0.79969\n",
      "Epoch 1431 of 2000. Loss per movie: 0.7251212393486259, test rmse: 0.89398, train rmse: 0.79968\n",
      "Epoch 1432 of 2000. Loss per movie: 0.7250963747855741, test rmse: 0.89398, train rmse: 0.79966\n",
      "Epoch 1433 of 2000. Loss per movie: 0.725071537083593, test rmse: 0.89398, train rmse: 0.79965\n",
      "Epoch 1434 of 2000. Loss per movie: 0.7250467054767359, test rmse: 0.89398, train rmse: 0.79964\n",
      "Epoch 1435 of 2000. Loss per movie: 0.7250218948484459, test rmse: 0.89398, train rmse: 0.79962\n",
      "Epoch 1436 of 2000. Loss per movie: 0.7249970798968702, test rmse: 0.89397, train rmse: 0.79961\n",
      "Epoch 1437 of 2000. Loss per movie: 0.7249723017286602, test rmse: 0.89397, train rmse: 0.79959\n",
      "Epoch 1438 of 2000. Loss per movie: 0.724947549641912, test rmse: 0.89397, train rmse: 0.79958\n",
      "Epoch 1439 of 2000. Loss per movie: 0.7249227764348496, test rmse: 0.89397, train rmse: 0.79957\n",
      "Epoch 1440 of 2000. Loss per movie: 0.7248980407198884, test rmse: 0.89397, train rmse: 0.79955\n",
      "Epoch 1441 of 2000. Loss per movie: 0.7248733174786698, test rmse: 0.89397, train rmse: 0.79954\n",
      "Epoch 1442 of 2000. Loss per movie: 0.7248485932452217, test rmse: 0.89397, train rmse: 0.79953\n",
      "Epoch 1443 of 2000. Loss per movie: 0.7248238836117221, test rmse: 0.89396, train rmse: 0.79951\n",
      "Epoch 1444 of 2000. Loss per movie: 0.7247991717811431, test rmse: 0.89396, train rmse: 0.79950\n",
      "Epoch 1445 of 2000. Loss per movie: 0.7247744857485314, test rmse: 0.89396, train rmse: 0.79949\n",
      "Epoch 1446 of 2000. Loss per movie: 0.7247498284905759, test rmse: 0.89396, train rmse: 0.79947\n",
      "Epoch 1447 of 2000. Loss per movie: 0.7247251781073535, test rmse: 0.89396, train rmse: 0.79946\n",
      "Epoch 1448 of 2000. Loss per movie: 0.7247005249600632, test rmse: 0.89396, train rmse: 0.79945\n",
      "Epoch 1449 of 2000. Loss per movie: 0.7246758831525387, test rmse: 0.89396, train rmse: 0.79943\n",
      "Epoch 1450 of 2000. Loss per movie: 0.7246512753643122, test rmse: 0.89395, train rmse: 0.79942\n",
      "Epoch 1451 of 2000. Loss per movie: 0.7246266783488633, test rmse: 0.89395, train rmse: 0.79940\n",
      "Epoch 1452 of 2000. Loss per movie: 0.7246020801285643, test rmse: 0.89395, train rmse: 0.79939\n",
      "Epoch 1453 of 2000. Loss per movie: 0.724577508131474, test rmse: 0.89395, train rmse: 0.79938\n",
      "Epoch 1454 of 2000. Loss per movie: 0.724552962428466, test rmse: 0.89395, train rmse: 0.79936\n",
      "Epoch 1455 of 2000. Loss per movie: 0.7245284321758891, test rmse: 0.89395, train rmse: 0.79935\n",
      "Epoch 1456 of 2000. Loss per movie: 0.7245039005767151, test rmse: 0.89395, train rmse: 0.79934\n",
      "Epoch 1457 of 2000. Loss per movie: 0.7244794066822628, test rmse: 0.89395, train rmse: 0.79932\n",
      "Epoch 1458 of 2000. Loss per movie: 0.7244549364595719, test rmse: 0.89394, train rmse: 0.79931\n",
      "Epoch 1459 of 2000. Loss per movie: 0.7244304686465813, test rmse: 0.89394, train rmse: 0.79930\n",
      "Epoch 1460 of 2000. Loss per movie: 0.7244060396722891, test rmse: 0.89394, train rmse: 0.79928\n",
      "Epoch 1461 of 2000. Loss per movie: 0.7243816130368236, test rmse: 0.89394, train rmse: 0.79927\n",
      "Epoch 1462 of 2000. Loss per movie: 0.7243572593302279, test rmse: 0.89394, train rmse: 0.79926\n",
      "Epoch 1463 of 2000. Loss per movie: 0.7243328826606061, test rmse: 0.89394, train rmse: 0.79924\n",
      "Epoch 1464 of 2000. Loss per movie: 0.7243085497908303, test rmse: 0.89394, train rmse: 0.79923\n",
      "Epoch 1465 of 2000. Loss per movie: 0.7242842415141719, test rmse: 0.89394, train rmse: 0.79922\n",
      "Epoch 1466 of 2000. Loss per movie: 0.7242599583976193, test rmse: 0.89393, train rmse: 0.79920\n",
      "Epoch 1467 of 2000. Loss per movie: 0.7242357175925682, test rmse: 0.89393, train rmse: 0.79919\n",
      "Epoch 1468 of 2000. Loss per movie: 0.7242115051369321, test rmse: 0.89393, train rmse: 0.79918\n",
      "Epoch 1469 of 2000. Loss per movie: 0.7241873133763689, test rmse: 0.89393, train rmse: 0.79916\n",
      "Epoch 1470 of 2000. Loss per movie: 0.7241631426652462, test rmse: 0.89393, train rmse: 0.79915\n",
      "Epoch 1471 of 2000. Loss per movie: 0.7241390219908407, test rmse: 0.89393, train rmse: 0.79914\n",
      "Epoch 1472 of 2000. Loss per movie: 0.724114932288171, test rmse: 0.89393, train rmse: 0.79912\n",
      "Epoch 1473 of 2000. Loss per movie: 0.7240908668242511, test rmse: 0.89393, train rmse: 0.79911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1474 of 2000. Loss per movie: 0.7240668616737111, test rmse: 0.89393, train rmse: 0.79910\n",
      "Epoch 1475 of 2000. Loss per movie: 0.7240428724697172, test rmse: 0.89393, train rmse: 0.79908\n",
      "Epoch 1476 of 2000. Loss per movie: 0.7240189106937821, test rmse: 0.89392, train rmse: 0.79907\n",
      "Epoch 1477 of 2000. Loss per movie: 0.7239949911584752, test rmse: 0.89392, train rmse: 0.79906\n",
      "Epoch 1478 of 2000. Loss per movie: 0.7239710916803793, test rmse: 0.89392, train rmse: 0.79904\n",
      "Epoch 1479 of 2000. Loss per movie: 0.7239472309701083, test rmse: 0.89392, train rmse: 0.79903\n",
      "Epoch 1480 of 2000. Loss per movie: 0.7239234304314703, test rmse: 0.89392, train rmse: 0.79902\n",
      "Epoch 1481 of 2000. Loss per movie: 0.7238996339326239, test rmse: 0.89392, train rmse: 0.79900\n",
      "Epoch 1482 of 2000. Loss per movie: 0.7238758749967523, test rmse: 0.89392, train rmse: 0.79899\n",
      "Epoch 1483 of 2000. Loss per movie: 0.7238521693577806, test rmse: 0.89392, train rmse: 0.79898\n",
      "Epoch 1484 of 2000. Loss per movie: 0.7238285025575073, test rmse: 0.89392, train rmse: 0.79897\n",
      "Epoch 1485 of 2000. Loss per movie: 0.7238048400805198, test rmse: 0.89392, train rmse: 0.79895\n",
      "Epoch 1486 of 2000. Loss per movie: 0.7237812525877346, test rmse: 0.89392, train rmse: 0.79894\n",
      "Epoch 1487 of 2000. Loss per movie: 0.7237576695599822, test rmse: 0.89392, train rmse: 0.79893\n",
      "Epoch 1488 of 2000. Loss per movie: 0.7237341424514507, test rmse: 0.89391, train rmse: 0.79891\n",
      "Epoch 1489 of 2000. Loss per movie: 0.7237106400777837, test rmse: 0.89391, train rmse: 0.79890\n",
      "Epoch 1490 of 2000. Loss per movie: 0.7236871898670401, test rmse: 0.89391, train rmse: 0.79889\n",
      "Epoch 1491 of 2000. Loss per movie: 0.7236637341281605, test rmse: 0.89391, train rmse: 0.79888\n",
      "Epoch 1492 of 2000. Loss per movie: 0.7236403453647736, test rmse: 0.89391, train rmse: 0.79886\n",
      "Epoch 1493 of 2000. Loss per movie: 0.7236170064254833, test rmse: 0.89391, train rmse: 0.79885\n",
      "Epoch 1494 of 2000. Loss per movie: 0.72359368846476, test rmse: 0.89391, train rmse: 0.79884\n",
      "Epoch 1495 of 2000. Loss per movie: 0.7235703977194751, test rmse: 0.89391, train rmse: 0.79882\n",
      "Epoch 1496 of 2000. Loss per movie: 0.7235471444662913, test rmse: 0.89391, train rmse: 0.79881\n",
      "Epoch 1497 of 2000. Loss per movie: 0.7235239435886515, test rmse: 0.89391, train rmse: 0.79880\n",
      "Epoch 1498 of 2000. Loss per movie: 0.7235007521371921, test rmse: 0.89391, train rmse: 0.79879\n",
      "Epoch 1499 of 2000. Loss per movie: 0.7234776042020845, test rmse: 0.89391, train rmse: 0.79877\n",
      "Epoch 1500 of 2000. Loss per movie: 0.7234545032561321, test rmse: 0.89391, train rmse: 0.79876\n",
      "Epoch 1501 of 2000. Loss per movie: 0.7234314398731545, test rmse: 0.89391, train rmse: 0.79875\n",
      "Epoch 1502 of 2000. Loss per movie: 0.7234084046269711, test rmse: 0.89391, train rmse: 0.79873\n",
      "Epoch 1503 of 2000. Loss per movie: 0.7233853900758604, test rmse: 0.89391, train rmse: 0.79872\n",
      "Epoch 1504 of 2000. Loss per movie: 0.7233624208838138, test rmse: 0.89391, train rmse: 0.79871\n",
      "Epoch 1505 of 2000. Loss per movie: 0.723339509878941, test rmse: 0.89391, train rmse: 0.79870\n",
      "Epoch 1506 of 2000. Loss per movie: 0.7233166016381363, test rmse: 0.89390, train rmse: 0.79868\n",
      "Epoch 1507 of 2000. Loss per movie: 0.7232937552699295, test rmse: 0.89390, train rmse: 0.79867\n",
      "Epoch 1508 of 2000. Loss per movie: 0.7232709263366127, test rmse: 0.89390, train rmse: 0.79866\n",
      "Epoch 1509 of 2000. Loss per movie: 0.7232481084595679, test rmse: 0.89390, train rmse: 0.79865\n",
      "Epoch 1510 of 2000. Loss per movie: 0.7232253588337394, test rmse: 0.89390, train rmse: 0.79863\n",
      "Epoch 1511 of 2000. Loss per movie: 0.7232026432980823, test rmse: 0.89390, train rmse: 0.79862\n",
      "Epoch 1512 of 2000. Loss per movie: 0.7231799636953083, test rmse: 0.89390, train rmse: 0.79861\n",
      "Epoch 1513 of 2000. Loss per movie: 0.723157294298324, test rmse: 0.89390, train rmse: 0.79860\n",
      "Epoch 1514 of 2000. Loss per movie: 0.723134685002099, test rmse: 0.89390, train rmse: 0.79858\n",
      "Epoch 1515 of 2000. Loss per movie: 0.7231121004407386, test rmse: 0.89390, train rmse: 0.79857\n",
      "Epoch 1516 of 2000. Loss per movie: 0.7230895438035518, test rmse: 0.89390, train rmse: 0.79856\n",
      "Epoch 1517 of 2000. Loss per movie: 0.7230670247293397, test rmse: 0.89390, train rmse: 0.79855\n",
      "Epoch 1518 of 2000. Loss per movie: 0.7230445381860813, test rmse: 0.89390, train rmse: 0.79853\n",
      "Epoch 1519 of 2000. Loss per movie: 0.7230220648961742, test rmse: 0.89390, train rmse: 0.79852\n",
      "Epoch 1520 of 2000. Loss per movie: 0.7229996736069497, test rmse: 0.89390, train rmse: 0.79851\n",
      "Epoch 1521 of 2000. Loss per movie: 0.7229772728206711, test rmse: 0.89390, train rmse: 0.79850\n",
      "Epoch 1522 of 2000. Loss per movie: 0.7229549106604705, test rmse: 0.89390, train rmse: 0.79849\n",
      "Epoch 1523 of 2000. Loss per movie: 0.7229326031437668, test rmse: 0.89390, train rmse: 0.79847\n",
      "Epoch 1524 of 2000. Loss per movie: 0.7229102983202577, test rmse: 0.89390, train rmse: 0.79846\n",
      "Epoch 1525 of 2000. Loss per movie: 0.7228880348868942, test rmse: 0.89390, train rmse: 0.79845\n",
      "Epoch 1526 of 2000. Loss per movie: 0.7228658070320465, test rmse: 0.89390, train rmse: 0.79844\n",
      "Epoch 1527 of 2000. Loss per movie: 0.7228436461526914, test rmse: 0.89390, train rmse: 0.79842\n",
      "Epoch 1528 of 2000. Loss per movie: 0.722821451608406, test rmse: 0.89390, train rmse: 0.79841\n",
      "Epoch 1529 of 2000. Loss per movie: 0.7227993381434473, test rmse: 0.89390, train rmse: 0.79840\n",
      "Epoch 1530 of 2000. Loss per movie: 0.7227772204260764, test rmse: 0.89390, train rmse: 0.79839\n",
      "Epoch 1531 of 2000. Loss per movie: 0.7227551654317859, test rmse: 0.89390, train rmse: 0.79838\n",
      "Epoch 1532 of 2000. Loss per movie: 0.7227331362354628, test rmse: 0.89390, train rmse: 0.79836\n",
      "Epoch 1533 of 2000. Loss per movie: 0.7227111087401047, test rmse: 0.89390, train rmse: 0.79835\n",
      "Epoch 1534 of 2000. Loss per movie: 0.7226891567250636, test rmse: 0.89390, train rmse: 0.79834\n",
      "Epoch 1535 of 2000. Loss per movie: 0.7226671867790176, test rmse: 0.89390, train rmse: 0.79833\n",
      "Epoch 1536 of 2000. Loss per movie: 0.7226452683580333, test rmse: 0.89390, train rmse: 0.79831\n",
      "Epoch 1537 of 2000. Loss per movie: 0.7226233922485505, test rmse: 0.89390, train rmse: 0.79830\n",
      "Epoch 1538 of 2000. Loss per movie: 0.7226015319438667, test rmse: 0.89390, train rmse: 0.79829\n",
      "Epoch 1539 of 2000. Loss per movie: 0.7225796941769675, test rmse: 0.89390, train rmse: 0.79828\n",
      "Epoch 1540 of 2000. Loss per movie: 0.7225579251573996, test rmse: 0.89390, train rmse: 0.79827\n",
      "Epoch 1541 of 2000. Loss per movie: 0.7225361520271664, test rmse: 0.89389, train rmse: 0.79825\n",
      "Epoch 1542 of 2000. Loss per movie: 0.722514390874561, test rmse: 0.89389, train rmse: 0.79824\n",
      "Epoch 1543 of 2000. Loss per movie: 0.7224927077537201, test rmse: 0.89389, train rmse: 0.79823\n",
      "Epoch 1544 of 2000. Loss per movie: 0.722471037815357, test rmse: 0.89389, train rmse: 0.79822\n",
      "Epoch 1545 of 2000. Loss per movie: 0.7224493623488581, test rmse: 0.89389, train rmse: 0.79821\n",
      "Epoch 1546 of 2000. Loss per movie: 0.7224277478336012, test rmse: 0.89389, train rmse: 0.79820\n",
      "Epoch 1547 of 2000. Loss per movie: 0.7224061529503141, test rmse: 0.89389, train rmse: 0.79818\n",
      "Epoch 1548 of 2000. Loss per movie: 0.7223845900309922, test rmse: 0.89389, train rmse: 0.79817\n",
      "Epoch 1549 of 2000. Loss per movie: 0.7223630495077081, test rmse: 0.89389, train rmse: 0.79816\n",
      "Epoch 1550 of 2000. Loss per movie: 0.7223415164261454, test rmse: 0.89389, train rmse: 0.79815\n",
      "Epoch 1551 of 2000. Loss per movie: 0.7223200741335838, test rmse: 0.89389, train rmse: 0.79814\n",
      "Epoch 1552 of 2000. Loss per movie: 0.7222985993100687, test rmse: 0.89389, train rmse: 0.79812\n",
      "Epoch 1553 of 2000. Loss per movie: 0.7222771779252006, test rmse: 0.89389, train rmse: 0.79811\n",
      "Epoch 1554 of 2000. Loss per movie: 0.722255786165471, test rmse: 0.89389, train rmse: 0.79810\n",
      "Epoch 1555 of 2000. Loss per movie: 0.7222344228260299, test rmse: 0.89389, train rmse: 0.79809\n",
      "Epoch 1556 of 2000. Loss per movie: 0.7222130656525865, test rmse: 0.89389, train rmse: 0.79808\n",
      "Epoch 1557 of 2000. Loss per movie: 0.7221917492314269, test rmse: 0.89389, train rmse: 0.79807\n",
      "Epoch 1558 of 2000. Loss per movie: 0.7221704766101134, test rmse: 0.89389, train rmse: 0.79805\n",
      "Epoch 1559 of 2000. Loss per movie: 0.722149212847992, test rmse: 0.89389, train rmse: 0.79804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1560 of 2000. Loss per movie: 0.7221279924604754, test rmse: 0.89389, train rmse: 0.79803\n",
      "Epoch 1561 of 2000. Loss per movie: 0.7221067581817454, test rmse: 0.89389, train rmse: 0.79802\n",
      "Epoch 1562 of 2000. Loss per movie: 0.7220855840037751, test rmse: 0.89389, train rmse: 0.79801\n",
      "Epoch 1563 of 2000. Loss per movie: 0.7220644461839293, test rmse: 0.89389, train rmse: 0.79800\n",
      "Epoch 1564 of 2000. Loss per movie: 0.7220433174358962, test rmse: 0.89389, train rmse: 0.79798\n",
      "Epoch 1565 of 2000. Loss per movie: 0.722022219943093, test rmse: 0.89389, train rmse: 0.79797\n",
      "Epoch 1566 of 2000. Loss per movie: 0.7220011676676067, test rmse: 0.89389, train rmse: 0.79796\n",
      "Epoch 1567 of 2000. Loss per movie: 0.7219801199280266, test rmse: 0.89389, train rmse: 0.79795\n",
      "Epoch 1568 of 2000. Loss per movie: 0.7219591052155149, test rmse: 0.89389, train rmse: 0.79794\n",
      "Epoch 1569 of 2000. Loss per movie: 0.7219381422406853, test rmse: 0.89389, train rmse: 0.79793\n",
      "Epoch 1570 of 2000. Loss per movie: 0.7219171723911226, test rmse: 0.89389, train rmse: 0.79791\n",
      "Epoch 1571 of 2000. Loss per movie: 0.7218962351433871, test rmse: 0.89389, train rmse: 0.79790\n",
      "Epoch 1572 of 2000. Loss per movie: 0.7218753386479354, test rmse: 0.89389, train rmse: 0.79789\n",
      "Epoch 1573 of 2000. Loss per movie: 0.7218544853853414, test rmse: 0.89389, train rmse: 0.79788\n",
      "Epoch 1574 of 2000. Loss per movie: 0.7218336282955764, test rmse: 0.89389, train rmse: 0.79787\n",
      "Epoch 1575 of 2000. Loss per movie: 0.7218128111076129, test rmse: 0.89389, train rmse: 0.79786\n",
      "Epoch 1576 of 2000. Loss per movie: 0.7217920119215276, test rmse: 0.89389, train rmse: 0.79785\n",
      "Epoch 1577 of 2000. Loss per movie: 0.7217712671663191, test rmse: 0.89389, train rmse: 0.79783\n",
      "Epoch 1578 of 2000. Loss per movie: 0.7217505172373424, test rmse: 0.89389, train rmse: 0.79782\n",
      "Epoch 1579 of 2000. Loss per movie: 0.7217297886413002, test rmse: 0.89389, train rmse: 0.79781\n",
      "Epoch 1580 of 2000. Loss per movie: 0.7217091303518071, test rmse: 0.89389, train rmse: 0.79780\n",
      "Epoch 1581 of 2000. Loss per movie: 0.7216884607934214, test rmse: 0.89389, train rmse: 0.79779\n",
      "Epoch 1582 of 2000. Loss per movie: 0.7216678390037999, test rmse: 0.89389, train rmse: 0.79778\n",
      "Epoch 1583 of 2000. Loss per movie: 0.7216472407441927, test rmse: 0.89389, train rmse: 0.79777\n",
      "Epoch 1584 of 2000. Loss per movie: 0.7216266832368694, test rmse: 0.89389, train rmse: 0.79775\n",
      "Epoch 1585 of 2000. Loss per movie: 0.7216061413217243, test rmse: 0.89389, train rmse: 0.79774\n",
      "Epoch 1586 of 2000. Loss per movie: 0.7215856121638159, test rmse: 0.89389, train rmse: 0.79773\n",
      "Epoch 1587 of 2000. Loss per movie: 0.721565116458217, test rmse: 0.89389, train rmse: 0.79772\n",
      "Epoch 1588 of 2000. Loss per movie: 0.7215446621427638, test rmse: 0.89389, train rmse: 0.79771\n",
      "Epoch 1589 of 2000. Loss per movie: 0.7215242358932313, test rmse: 0.89389, train rmse: 0.79770\n",
      "Epoch 1590 of 2000. Loss per movie: 0.7215037892321202, test rmse: 0.89389, train rmse: 0.79769\n",
      "Epoch 1591 of 2000. Loss per movie: 0.7214834247843122, test rmse: 0.89389, train rmse: 0.79768\n",
      "Epoch 1592 of 2000. Loss per movie: 0.7214630755743147, test rmse: 0.89389, train rmse: 0.79766\n",
      "Epoch 1593 of 2000. Loss per movie: 0.7214427413186335, test rmse: 0.89389, train rmse: 0.79765\n",
      "Epoch 1594 of 2000. Loss per movie: 0.721422432081311, test rmse: 0.89389, train rmse: 0.79764\n",
      "Epoch 1595 of 2000. Loss per movie: 0.7214021538157241, test rmse: 0.89390, train rmse: 0.79763\n",
      "Epoch 1596 of 2000. Loss per movie: 0.7213819194208569, test rmse: 0.89390, train rmse: 0.79762\n",
      "Epoch 1597 of 2000. Loss per movie: 0.7213616822619217, test rmse: 0.89390, train rmse: 0.79761\n",
      "Epoch 1598 of 2000. Loss per movie: 0.7213414568679937, test rmse: 0.89390, train rmse: 0.79760\n",
      "Epoch 1599 of 2000. Loss per movie: 0.7213213002922703, test rmse: 0.89390, train rmse: 0.79759\n",
      "Epoch 1600 of 2000. Loss per movie: 0.7213011347864811, test rmse: 0.89390, train rmse: 0.79757\n",
      "Epoch 1601 of 2000. Loss per movie: 0.7212810230028331, test rmse: 0.89390, train rmse: 0.79756\n",
      "Epoch 1602 of 2000. Loss per movie: 0.7212609168181947, test rmse: 0.89390, train rmse: 0.79755\n",
      "Epoch 1603 of 2000. Loss per movie: 0.7212408534411726, test rmse: 0.89390, train rmse: 0.79754\n",
      "Epoch 1604 of 2000. Loss per movie: 0.7212207743302254, test rmse: 0.89390, train rmse: 0.79753\n",
      "Epoch 1605 of 2000. Loss per movie: 0.7212007468152132, test rmse: 0.89390, train rmse: 0.79752\n",
      "Epoch 1606 of 2000. Loss per movie: 0.7211807477913629, test rmse: 0.89390, train rmse: 0.79751\n",
      "Epoch 1607 of 2000. Loss per movie: 0.7211607654227938, test rmse: 0.89390, train rmse: 0.79750\n",
      "Epoch 1608 of 2000. Loss per movie: 0.7211408124667428, test rmse: 0.89390, train rmse: 0.79749\n",
      "Epoch 1609 of 2000. Loss per movie: 0.7211208791426614, test rmse: 0.89390, train rmse: 0.79748\n",
      "Epoch 1610 of 2000. Loss per movie: 0.7211009620486202, test rmse: 0.89390, train rmse: 0.79746\n",
      "Epoch 1611 of 2000. Loss per movie: 0.7210810960543992, test rmse: 0.89390, train rmse: 0.79745\n",
      "Epoch 1612 of 2000. Loss per movie: 0.7210612217679744, test rmse: 0.89390, train rmse: 0.79744\n",
      "Epoch 1613 of 2000. Loss per movie: 0.7210413781697912, test rmse: 0.89390, train rmse: 0.79743\n",
      "Epoch 1614 of 2000. Loss per movie: 0.7210215648346083, test rmse: 0.89390, train rmse: 0.79742\n",
      "Epoch 1615 of 2000. Loss per movie: 0.7210017750294399, test rmse: 0.89390, train rmse: 0.79741\n",
      "Epoch 1616 of 2000. Loss per movie: 0.7209819865708685, test rmse: 0.89390, train rmse: 0.79740\n",
      "Epoch 1617 of 2000. Loss per movie: 0.7209622332655717, test rmse: 0.89390, train rmse: 0.79739\n",
      "Epoch 1618 of 2000. Loss per movie: 0.7209425057582425, test rmse: 0.89390, train rmse: 0.79738\n",
      "Epoch 1619 of 2000. Loss per movie: 0.7209228188614499, test rmse: 0.89390, train rmse: 0.79737\n",
      "Epoch 1620 of 2000. Loss per movie: 0.7209030995045774, test rmse: 0.89390, train rmse: 0.79736\n",
      "Epoch 1621 of 2000. Loss per movie: 0.7208834474066916, test rmse: 0.89390, train rmse: 0.79734\n",
      "Epoch 1622 of 2000. Loss per movie: 0.7208637838272929, test rmse: 0.89390, train rmse: 0.79733\n",
      "Epoch 1623 of 2000. Loss per movie: 0.7208441730486794, test rmse: 0.89390, train rmse: 0.79732\n",
      "Epoch 1624 of 2000. Loss per movie: 0.7208245682943165, test rmse: 0.89390, train rmse: 0.79731\n",
      "Epoch 1625 of 2000. Loss per movie: 0.720804998693228, test rmse: 0.89390, train rmse: 0.79730\n",
      "Epoch 1626 of 2000. Loss per movie: 0.7207854320688282, test rmse: 0.89390, train rmse: 0.79729\n",
      "Epoch 1627 of 2000. Loss per movie: 0.720765889683178, test rmse: 0.89390, train rmse: 0.79728\n",
      "Epoch 1628 of 2000. Loss per movie: 0.7207463704023009, test rmse: 0.89390, train rmse: 0.79727\n",
      "Epoch 1629 of 2000. Loss per movie: 0.7207268723834851, test rmse: 0.89390, train rmse: 0.79726\n",
      "Epoch 1630 of 2000. Loss per movie: 0.7207073879723883, test rmse: 0.89390, train rmse: 0.79725\n",
      "Epoch 1631 of 2000. Loss per movie: 0.7206879209961817, test rmse: 0.89390, train rmse: 0.79724\n",
      "Epoch 1632 of 2000. Loss per movie: 0.7206684515394014, test rmse: 0.89390, train rmse: 0.79723\n",
      "Epoch 1633 of 2000. Loss per movie: 0.7206490490581136, test rmse: 0.89390, train rmse: 0.79722\n",
      "Epoch 1634 of 2000. Loss per movie: 0.720629639205978, test rmse: 0.89390, train rmse: 0.79720\n",
      "Epoch 1635 of 2000. Loss per movie: 0.720610237362552, test rmse: 0.89390, train rmse: 0.79719\n",
      "Epoch 1636 of 2000. Loss per movie: 0.7205908759879158, test rmse: 0.89390, train rmse: 0.79718\n",
      "Epoch 1637 of 2000. Loss per movie: 0.7205715462937509, test rmse: 0.89390, train rmse: 0.79717\n",
      "Epoch 1638 of 2000. Loss per movie: 0.7205522257422721, test rmse: 0.89390, train rmse: 0.79716\n",
      "Epoch 1639 of 2000. Loss per movie: 0.7205329032063342, test rmse: 0.89390, train rmse: 0.79715\n",
      "Epoch 1640 of 2000. Loss per movie: 0.7205135950577245, test rmse: 0.89390, train rmse: 0.79714\n",
      "Epoch 1641 of 2000. Loss per movie: 0.7204943284410076, test rmse: 0.89390, train rmse: 0.79713\n",
      "Epoch 1642 of 2000. Loss per movie: 0.720475067281553, test rmse: 0.89390, train rmse: 0.79712\n",
      "Epoch 1643 of 2000. Loss per movie: 0.7204558238404827, test rmse: 0.89390, train rmse: 0.79711\n",
      "Epoch 1644 of 2000. Loss per movie: 0.7204366210808226, test rmse: 0.89390, train rmse: 0.79710\n",
      "Epoch 1645 of 2000. Loss per movie: 0.7204174168328185, test rmse: 0.89390, train rmse: 0.79709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1646 of 2000. Loss per movie: 0.7203982279643718, test rmse: 0.89390, train rmse: 0.79708\n",
      "Epoch 1647 of 2000. Loss per movie: 0.7203790368279719, test rmse: 0.89390, train rmse: 0.79707\n",
      "Epoch 1648 of 2000. Loss per movie: 0.7203598867982236, test rmse: 0.89390, train rmse: 0.79706\n",
      "Epoch 1649 of 2000. Loss per movie: 0.7203407625664429, test rmse: 0.89390, train rmse: 0.79705\n",
      "Epoch 1650 of 2000. Loss per movie: 0.7203216482569573, test rmse: 0.89390, train rmse: 0.79703\n",
      "Epoch 1651 of 2000. Loss per movie: 0.7203025377037693, test rmse: 0.89390, train rmse: 0.79702\n",
      "Epoch 1652 of 2000. Loss per movie: 0.7202834688950945, test rmse: 0.89390, train rmse: 0.79701\n",
      "Epoch 1653 of 2000. Loss per movie: 0.7202643940621692, test rmse: 0.89390, train rmse: 0.79700\n",
      "Epoch 1654 of 2000. Loss per movie: 0.720245352752427, test rmse: 0.89390, train rmse: 0.79699\n",
      "Epoch 1655 of 2000. Loss per movie: 0.7202262962048742, test rmse: 0.89390, train rmse: 0.79698\n",
      "Epoch 1656 of 2000. Loss per movie: 0.720207293946451, test rmse: 0.89390, train rmse: 0.79697\n",
      "Epoch 1657 of 2000. Loss per movie: 0.7201882746783789, test rmse: 0.89390, train rmse: 0.79696\n",
      "Epoch 1658 of 2000. Loss per movie: 0.7201693044547944, test rmse: 0.89390, train rmse: 0.79695\n",
      "Epoch 1659 of 2000. Loss per movie: 0.7201503345147041, test rmse: 0.89390, train rmse: 0.79694\n",
      "Epoch 1660 of 2000. Loss per movie: 0.7201313786784477, test rmse: 0.89390, train rmse: 0.79693\n",
      "Epoch 1661 of 2000. Loss per movie: 0.7201124596964307, test rmse: 0.89390, train rmse: 0.79692\n",
      "Epoch 1662 of 2000. Loss per movie: 0.7200935074747247, test rmse: 0.89390, train rmse: 0.79691\n",
      "Epoch 1663 of 2000. Loss per movie: 0.7200746105343777, test rmse: 0.89390, train rmse: 0.79690\n",
      "Epoch 1664 of 2000. Loss per movie: 0.7200557264930144, test rmse: 0.89390, train rmse: 0.79689\n",
      "Epoch 1665 of 2000. Loss per movie: 0.720036853366176, test rmse: 0.89390, train rmse: 0.79688\n",
      "Epoch 1666 of 2000. Loss per movie: 0.7200180119198086, test rmse: 0.89390, train rmse: 0.79687\n",
      "Epoch 1667 of 2000. Loss per movie: 0.7199991654414201, test rmse: 0.89390, train rmse: 0.79686\n",
      "Epoch 1668 of 2000. Loss per movie: 0.7199803304445446, test rmse: 0.89390, train rmse: 0.79685\n",
      "Epoch 1669 of 2000. Loss per movie: 0.7199615189068099, test rmse: 0.89390, train rmse: 0.79684\n",
      "Epoch 1670 of 2000. Loss per movie: 0.7199427255835743, test rmse: 0.89390, train rmse: 0.79683\n",
      "Epoch 1671 of 2000. Loss per movie: 0.7199239606097535, test rmse: 0.89390, train rmse: 0.79682\n",
      "Epoch 1672 of 2000. Loss per movie: 0.7199051799728811, test rmse: 0.89390, train rmse: 0.79680\n",
      "Epoch 1673 of 2000. Loss per movie: 0.7198864285359059, test rmse: 0.89390, train rmse: 0.79679\n",
      "Epoch 1674 of 2000. Loss per movie: 0.7198677215366386, test rmse: 0.89390, train rmse: 0.79678\n",
      "Epoch 1675 of 2000. Loss per movie: 0.7198489933461837, test rmse: 0.89390, train rmse: 0.79677\n",
      "Epoch 1676 of 2000. Loss per movie: 0.7198303046322889, test rmse: 0.89390, train rmse: 0.79676\n",
      "Epoch 1677 of 2000. Loss per movie: 0.7198116115242349, test rmse: 0.89390, train rmse: 0.79675\n",
      "Epoch 1678 of 2000. Loss per movie: 0.7197929493879166, test rmse: 0.89390, train rmse: 0.79674\n",
      "Epoch 1679 of 2000. Loss per movie: 0.7197742932049754, test rmse: 0.89390, train rmse: 0.79673\n",
      "Epoch 1680 of 2000. Loss per movie: 0.7197556661510581, test rmse: 0.89390, train rmse: 0.79672\n",
      "Epoch 1681 of 2000. Loss per movie: 0.7197370448378972, test rmse: 0.89390, train rmse: 0.79671\n",
      "Epoch 1682 of 2000. Loss per movie: 0.7197184306829636, test rmse: 0.89390, train rmse: 0.79670\n",
      "Epoch 1683 of 2000. Loss per movie: 0.7196998512560633, test rmse: 0.89390, train rmse: 0.79669\n",
      "Epoch 1684 of 2000. Loss per movie: 0.7196812821058257, test rmse: 0.89390, train rmse: 0.79668\n",
      "Epoch 1685 of 2000. Loss per movie: 0.7196627284060193, test rmse: 0.89390, train rmse: 0.79667\n",
      "Epoch 1686 of 2000. Loss per movie: 0.7196441762654308, test rmse: 0.89390, train rmse: 0.79666\n",
      "Epoch 1687 of 2000. Loss per movie: 0.719625642693709, test rmse: 0.89390, train rmse: 0.79665\n",
      "Epoch 1688 of 2000. Loss per movie: 0.7196071482441797, test rmse: 0.89390, train rmse: 0.79664\n",
      "Epoch 1689 of 2000. Loss per movie: 0.7195886646383015, test rmse: 0.89390, train rmse: 0.79663\n",
      "Epoch 1690 of 2000. Loss per movie: 0.7195701861353181, test rmse: 0.89390, train rmse: 0.79662\n",
      "Epoch 1691 of 2000. Loss per movie: 0.719551726272075, test rmse: 0.89390, train rmse: 0.79661\n",
      "Epoch 1692 of 2000. Loss per movie: 0.7195332713699795, test rmse: 0.89390, train rmse: 0.79660\n",
      "Epoch 1693 of 2000. Loss per movie: 0.7195148303590971, test rmse: 0.89390, train rmse: 0.79659\n",
      "Epoch 1694 of 2000. Loss per movie: 0.7194964353451405, test rmse: 0.89391, train rmse: 0.79658\n",
      "Epoch 1695 of 2000. Loss per movie: 0.7194780356535305, test rmse: 0.89391, train rmse: 0.79657\n",
      "Epoch 1696 of 2000. Loss per movie: 0.7194596413483092, test rmse: 0.89391, train rmse: 0.79656\n",
      "Epoch 1697 of 2000. Loss per movie: 0.7194412964419434, test rmse: 0.89391, train rmse: 0.79655\n",
      "Epoch 1698 of 2000. Loss per movie: 0.7194229273677014, test rmse: 0.89391, train rmse: 0.79654\n",
      "Epoch 1699 of 2000. Loss per movie: 0.7194046140000596, test rmse: 0.89391, train rmse: 0.79653\n",
      "Epoch 1700 of 2000. Loss per movie: 0.7193862933324435, test rmse: 0.89391, train rmse: 0.79652\n",
      "Epoch 1701 of 2000. Loss per movie: 0.7193679642308765, test rmse: 0.89391, train rmse: 0.79651\n",
      "Epoch 1702 of 2000. Loss per movie: 0.7193496841737972, test rmse: 0.89391, train rmse: 0.79650\n",
      "Epoch 1703 of 2000. Loss per movie: 0.7193314207719991, test rmse: 0.89391, train rmse: 0.79649\n",
      "Epoch 1704 of 2000. Loss per movie: 0.7193131707652995, test rmse: 0.89391, train rmse: 0.79648\n",
      "Epoch 1705 of 2000. Loss per movie: 0.7192949362090312, test rmse: 0.89391, train rmse: 0.79647\n",
      "Epoch 1706 of 2000. Loss per movie: 0.7192767142682523, test rmse: 0.89391, train rmse: 0.79646\n",
      "Epoch 1707 of 2000. Loss per movie: 0.7192585001235626, test rmse: 0.89391, train rmse: 0.79645\n",
      "Epoch 1708 of 2000. Loss per movie: 0.7192403004370745, test rmse: 0.89391, train rmse: 0.79644\n",
      "Epoch 1709 of 2000. Loss per movie: 0.7192221459679031, test rmse: 0.89391, train rmse: 0.79643\n",
      "Epoch 1710 of 2000. Loss per movie: 0.7192039630784433, test rmse: 0.89391, train rmse: 0.79642\n",
      "Epoch 1711 of 2000. Loss per movie: 0.719185838447031, test rmse: 0.89391, train rmse: 0.79641\n",
      "Epoch 1712 of 2000. Loss per movie: 0.7191677178554104, test rmse: 0.89391, train rmse: 0.79640\n",
      "Epoch 1713 of 2000. Loss per movie: 0.7191496064064761, test rmse: 0.89391, train rmse: 0.79639\n",
      "Epoch 1714 of 2000. Loss per movie: 0.7191314912721178, test rmse: 0.89391, train rmse: 0.79638\n",
      "Epoch 1715 of 2000. Loss per movie: 0.7191134175279053, test rmse: 0.89391, train rmse: 0.79637\n",
      "Epoch 1716 of 2000. Loss per movie: 0.7190953671010865, test rmse: 0.89391, train rmse: 0.79636\n",
      "Epoch 1717 of 2000. Loss per movie: 0.7190773026413073, test rmse: 0.89391, train rmse: 0.79635\n",
      "Epoch 1718 of 2000. Loss per movie: 0.7190592571047625, test rmse: 0.89391, train rmse: 0.79634\n",
      "Epoch 1719 of 2000. Loss per movie: 0.7190412413351034, test rmse: 0.89391, train rmse: 0.79633\n",
      "Epoch 1720 of 2000. Loss per movie: 0.7190232331489127, test rmse: 0.89391, train rmse: 0.79632\n",
      "Epoch 1721 of 2000. Loss per movie: 0.7190052542334929, test rmse: 0.89391, train rmse: 0.79631\n",
      "Epoch 1722 of 2000. Loss per movie: 0.7189872758850615, test rmse: 0.89391, train rmse: 0.79630\n",
      "Epoch 1723 of 2000. Loss per movie: 0.7189693354539723, test rmse: 0.89391, train rmse: 0.79629\n",
      "Epoch 1724 of 2000. Loss per movie: 0.7189513843918527, test rmse: 0.89391, train rmse: 0.79628\n",
      "Epoch 1725 of 2000. Loss per movie: 0.7189334511189909, test rmse: 0.89391, train rmse: 0.79627\n",
      "Epoch 1726 of 2000. Loss per movie: 0.7189155399586726, test rmse: 0.89391, train rmse: 0.79626\n",
      "Epoch 1727 of 2000. Loss per movie: 0.7188976329090195, test rmse: 0.89391, train rmse: 0.79625\n",
      "Epoch 1728 of 2000. Loss per movie: 0.71887977922714, test rmse: 0.89391, train rmse: 0.79624\n",
      "Epoch 1729 of 2000. Loss per movie: 0.7188618993220517, test rmse: 0.89391, train rmse: 0.79623\n",
      "Epoch 1730 of 2000. Loss per movie: 0.7188440571216851, test rmse: 0.89391, train rmse: 0.79622\n",
      "Epoch 1731 of 2000. Loss per movie: 0.718826206133, test rmse: 0.89391, train rmse: 0.79621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1732 of 2000. Loss per movie: 0.7188083696025164, test rmse: 0.89391, train rmse: 0.79620\n",
      "Epoch 1733 of 2000. Loss per movie: 0.7187905831796235, test rmse: 0.89391, train rmse: 0.79619\n",
      "Epoch 1734 of 2000. Loss per movie: 0.7187727817315409, test rmse: 0.89391, train rmse: 0.79618\n",
      "Epoch 1735 of 2000. Loss per movie: 0.7187550121765499, test rmse: 0.89391, train rmse: 0.79617\n",
      "Epoch 1736 of 2000. Loss per movie: 0.7187372329118844, test rmse: 0.89391, train rmse: 0.79616\n",
      "Epoch 1737 of 2000. Loss per movie: 0.7187194951791117, test rmse: 0.89391, train rmse: 0.79615\n",
      "Epoch 1738 of 2000. Loss per movie: 0.7187017654550487, test rmse: 0.89391, train rmse: 0.79614\n",
      "Epoch 1739 of 2000. Loss per movie: 0.7186840481338547, test rmse: 0.89391, train rmse: 0.79613\n",
      "Epoch 1740 of 2000. Loss per movie: 0.7186663478931831, test rmse: 0.89391, train rmse: 0.79612\n",
      "Epoch 1741 of 2000. Loss per movie: 0.7186486658670106, test rmse: 0.89391, train rmse: 0.79611\n",
      "Epoch 1742 of 2000. Loss per movie: 0.7186310047485315, test rmse: 0.89391, train rmse: 0.79610\n",
      "Epoch 1743 of 2000. Loss per movie: 0.7186133387397784, test rmse: 0.89391, train rmse: 0.79609\n",
      "Epoch 1744 of 2000. Loss per movie: 0.7185957003008313, test rmse: 0.89391, train rmse: 0.79608\n",
      "Epoch 1745 of 2000. Loss per movie: 0.7185780926209993, test rmse: 0.89391, train rmse: 0.79607\n",
      "Epoch 1746 of 2000. Loss per movie: 0.7185604898314414, test rmse: 0.89391, train rmse: 0.79606\n",
      "Epoch 1747 of 2000. Loss per movie: 0.7185429146825629, test rmse: 0.89391, train rmse: 0.79605\n",
      "Epoch 1748 of 2000. Loss per movie: 0.7185253247211152, test rmse: 0.89391, train rmse: 0.79604\n",
      "Epoch 1749 of 2000. Loss per movie: 0.7185077892614177, test rmse: 0.89392, train rmse: 0.79603\n",
      "Epoch 1750 of 2000. Loss per movie: 0.7184902404066215, test rmse: 0.89392, train rmse: 0.79602\n",
      "Epoch 1751 of 2000. Loss per movie: 0.7184727287604324, test rmse: 0.89392, train rmse: 0.79601\n",
      "Epoch 1752 of 2000. Loss per movie: 0.7184552072628216, test rmse: 0.89392, train rmse: 0.79600\n",
      "Epoch 1753 of 2000. Loss per movie: 0.7184377316203894, test rmse: 0.89392, train rmse: 0.79599\n",
      "Epoch 1754 of 2000. Loss per movie: 0.7184202673885967, test rmse: 0.89392, train rmse: 0.79598\n",
      "Epoch 1755 of 2000. Loss per movie: 0.7184028143548228, test rmse: 0.89392, train rmse: 0.79598\n",
      "Epoch 1756 of 2000. Loss per movie: 0.7183853963325764, test rmse: 0.89392, train rmse: 0.79597\n",
      "Epoch 1757 of 2000. Loss per movie: 0.7183679615841752, test rmse: 0.89392, train rmse: 0.79596\n",
      "Epoch 1758 of 2000. Loss per movie: 0.7183505774394794, test rmse: 0.89392, train rmse: 0.79595\n",
      "Epoch 1759 of 2000. Loss per movie: 0.7183331980433106, test rmse: 0.89392, train rmse: 0.79594\n",
      "Epoch 1760 of 2000. Loss per movie: 0.7183158492645101, test rmse: 0.89392, train rmse: 0.79593\n",
      "Epoch 1761 of 2000. Loss per movie: 0.7182985032497773, test rmse: 0.89392, train rmse: 0.79592\n",
      "Epoch 1762 of 2000. Loss per movie: 0.7182811900494923, test rmse: 0.89392, train rmse: 0.79591\n",
      "Epoch 1763 of 2000. Loss per movie: 0.7182638842909288, test rmse: 0.89392, train rmse: 0.79590\n",
      "Epoch 1764 of 2000. Loss per movie: 0.7182465825012833, test rmse: 0.89392, train rmse: 0.79589\n",
      "Epoch 1765 of 2000. Loss per movie: 0.7182293318823316, test rmse: 0.89392, train rmse: 0.79588\n",
      "Epoch 1766 of 2000. Loss per movie: 0.7182120986273967, test rmse: 0.89392, train rmse: 0.79587\n",
      "Epoch 1767 of 2000. Loss per movie: 0.7181948654433351, test rmse: 0.89392, train rmse: 0.79586\n",
      "Epoch 1768 of 2000. Loss per movie: 0.7181776478514518, test rmse: 0.89392, train rmse: 0.79585\n",
      "Epoch 1769 of 2000. Loss per movie: 0.7181604808632741, test rmse: 0.89392, train rmse: 0.79584\n",
      "Epoch 1770 of 2000. Loss per movie: 0.7181433240808857, test rmse: 0.89392, train rmse: 0.79583\n",
      "Epoch 1771 of 2000. Loss per movie: 0.7181261789926309, test rmse: 0.89392, train rmse: 0.79582\n",
      "Epoch 1772 of 2000. Loss per movie: 0.7181090533945989, test rmse: 0.89392, train rmse: 0.79581\n",
      "Epoch 1773 of 2000. Loss per movie: 0.7180919594061644, test rmse: 0.89392, train rmse: 0.79580\n",
      "Epoch 1774 of 2000. Loss per movie: 0.7180748771118636, test rmse: 0.89392, train rmse: 0.79579\n",
      "Epoch 1775 of 2000. Loss per movie: 0.7180578060864552, test rmse: 0.89392, train rmse: 0.79578\n",
      "Epoch 1776 of 2000. Loss per movie: 0.7180407782939044, test rmse: 0.89392, train rmse: 0.79578\n",
      "Epoch 1777 of 2000. Loss per movie: 0.7180237529819276, test rmse: 0.89392, train rmse: 0.79577\n",
      "Epoch 1778 of 2000. Loss per movie: 0.7180067747299793, test rmse: 0.89392, train rmse: 0.79576\n",
      "Epoch 1779 of 2000. Loss per movie: 0.7179897916586305, test rmse: 0.89392, train rmse: 0.79575\n",
      "Epoch 1780 of 2000. Loss per movie: 0.7179728435279357, test rmse: 0.89392, train rmse: 0.79574\n",
      "Epoch 1781 of 2000. Loss per movie: 0.7179559279990678, test rmse: 0.89392, train rmse: 0.79573\n",
      "Epoch 1782 of 2000. Loss per movie: 0.7179389903576564, test rmse: 0.89392, train rmse: 0.79572\n",
      "Epoch 1783 of 2000. Loss per movie: 0.717922121463576, test rmse: 0.89392, train rmse: 0.79571\n",
      "Epoch 1784 of 2000. Loss per movie: 0.7179052439937977, test rmse: 0.89392, train rmse: 0.79570\n",
      "Epoch 1785 of 2000. Loss per movie: 0.7178883976375022, test rmse: 0.89392, train rmse: 0.79569\n",
      "Epoch 1786 of 2000. Loss per movie: 0.7178715793471274, test rmse: 0.89392, train rmse: 0.79568\n",
      "Epoch 1787 of 2000. Loss per movie: 0.7178547844450199, test rmse: 0.89392, train rmse: 0.79567\n",
      "Epoch 1788 of 2000. Loss per movie: 0.7178379726750105, test rmse: 0.89392, train rmse: 0.79566\n",
      "Epoch 1789 of 2000. Loss per movie: 0.717821250205658, test rmse: 0.89392, train rmse: 0.79565\n",
      "Epoch 1790 of 2000. Loss per movie: 0.7178044981820405, test rmse: 0.89392, train rmse: 0.79564\n",
      "Epoch 1791 of 2000. Loss per movie: 0.7177877935728193, test rmse: 0.89392, train rmse: 0.79564\n",
      "Epoch 1792 of 2000. Loss per movie: 0.7177710946334812, test rmse: 0.89392, train rmse: 0.79563\n",
      "Epoch 1793 of 2000. Loss per movie: 0.7177544324066353, test rmse: 0.89392, train rmse: 0.79562\n",
      "Epoch 1794 of 2000. Loss per movie: 0.7177377888195297, test rmse: 0.89392, train rmse: 0.79561\n",
      "Epoch 1795 of 2000. Loss per movie: 0.7177211564304429, test rmse: 0.89392, train rmse: 0.79560\n",
      "Epoch 1796 of 2000. Loss per movie: 0.7177045515402886, test rmse: 0.89392, train rmse: 0.79559\n",
      "Epoch 1797 of 2000. Loss per movie: 0.7176879684791837, test rmse: 0.89392, train rmse: 0.79558\n",
      "Epoch 1798 of 2000. Loss per movie: 0.7176714121374024, test rmse: 0.89393, train rmse: 0.79557\n",
      "Epoch 1799 of 2000. Loss per movie: 0.7176548768450616, test rmse: 0.89393, train rmse: 0.79556\n",
      "Epoch 1800 of 2000. Loss per movie: 0.7176383622477936, test rmse: 0.89393, train rmse: 0.79555\n",
      "Epoch 1801 of 2000. Loss per movie: 0.7176218680621044, test rmse: 0.89393, train rmse: 0.79554\n",
      "Epoch 1802 of 2000. Loss per movie: 0.717605395492844, test rmse: 0.89393, train rmse: 0.79553\n",
      "Epoch 1803 of 2000. Loss per movie: 0.717588934192476, test rmse: 0.89393, train rmse: 0.79553\n",
      "Epoch 1804 of 2000. Loss per movie: 0.7175725046434527, test rmse: 0.89393, train rmse: 0.79552\n",
      "Epoch 1805 of 2000. Loss per movie: 0.7175561101768303, test rmse: 0.89393, train rmse: 0.79551\n",
      "Epoch 1806 of 2000. Loss per movie: 0.7175397174111728, test rmse: 0.89393, train rmse: 0.79550\n",
      "Epoch 1807 of 2000. Loss per movie: 0.7175233579560778, test rmse: 0.89393, train rmse: 0.79549\n",
      "Epoch 1808 of 2000. Loss per movie: 0.7175069941776969, test rmse: 0.89393, train rmse: 0.79548\n",
      "Epoch 1809 of 2000. Loss per movie: 0.7174906982252912, test rmse: 0.89393, train rmse: 0.79547\n",
      "Epoch 1810 of 2000. Loss per movie: 0.7174744041155975, test rmse: 0.89393, train rmse: 0.79546\n",
      "Epoch 1811 of 2000. Loss per movie: 0.7174581263068173, test rmse: 0.89393, train rmse: 0.79545\n",
      "Epoch 1812 of 2000. Loss per movie: 0.7174418607591591, test rmse: 0.89393, train rmse: 0.79544\n",
      "Epoch 1813 of 2000. Loss per movie: 0.7174256509889746, test rmse: 0.89393, train rmse: 0.79544\n",
      "Epoch 1814 of 2000. Loss per movie: 0.7174094365411366, test rmse: 0.89393, train rmse: 0.79543\n",
      "Epoch 1815 of 2000. Loss per movie: 0.7173932344961677, test rmse: 0.89393, train rmse: 0.79542\n",
      "Epoch 1816 of 2000. Loss per movie: 0.7173770622180844, test rmse: 0.89393, train rmse: 0.79541\n",
      "Epoch 1817 of 2000. Loss per movie: 0.7173609206282426, test rmse: 0.89393, train rmse: 0.79540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1818 of 2000. Loss per movie: 0.7173448068208275, test rmse: 0.89393, train rmse: 0.79539\n",
      "Epoch 1819 of 2000. Loss per movie: 0.7173286768542458, test rmse: 0.89393, train rmse: 0.79538\n",
      "Epoch 1820 of 2000. Loss per movie: 0.7173125899079014, test rmse: 0.89393, train rmse: 0.79537\n",
      "Epoch 1821 of 2000. Loss per movie: 0.7172965195459645, test rmse: 0.89393, train rmse: 0.79536\n",
      "Epoch 1822 of 2000. Loss per movie: 0.717280490786794, test rmse: 0.89393, train rmse: 0.79536\n",
      "Epoch 1823 of 2000. Loss per movie: 0.7172644580587054, test rmse: 0.89393, train rmse: 0.79535\n",
      "Epoch 1824 of 2000. Loss per movie: 0.7172484712566689, test rmse: 0.89393, train rmse: 0.79534\n",
      "Epoch 1825 of 2000. Loss per movie: 0.7172324965740072, test rmse: 0.89393, train rmse: 0.79533\n",
      "Epoch 1826 of 2000. Loss per movie: 0.7172165221748398, test rmse: 0.89393, train rmse: 0.79532\n",
      "Epoch 1827 of 2000. Loss per movie: 0.7172005849842793, test rmse: 0.89393, train rmse: 0.79531\n",
      "Epoch 1828 of 2000. Loss per movie: 0.7171846654412296, test rmse: 0.89393, train rmse: 0.79530\n",
      "Epoch 1829 of 2000. Loss per movie: 0.7171687635456906, test rmse: 0.89393, train rmse: 0.79529\n",
      "Epoch 1830 of 2000. Loss per movie: 0.7171528939684846, test rmse: 0.89393, train rmse: 0.79528\n",
      "Epoch 1831 of 2000. Loss per movie: 0.717137027864082, test rmse: 0.89393, train rmse: 0.79528\n",
      "Epoch 1832 of 2000. Loss per movie: 0.7171211963459655, test rmse: 0.89393, train rmse: 0.79527\n",
      "Epoch 1833 of 2000. Loss per movie: 0.7171053709938466, test rmse: 0.89393, train rmse: 0.79526\n",
      "Epoch 1834 of 2000. Loss per movie: 0.7170895661241802, test rmse: 0.89393, train rmse: 0.79525\n",
      "Epoch 1835 of 2000. Loss per movie: 0.7170737778389213, test rmse: 0.89393, train rmse: 0.79524\n",
      "Epoch 1836 of 2000. Loss per movie: 0.7170580222263631, test rmse: 0.89393, train rmse: 0.79523\n",
      "Epoch 1837 of 2000. Loss per movie: 0.7170422853244187, test rmse: 0.89393, train rmse: 0.79522\n",
      "Epoch 1838 of 2000. Loss per movie: 0.7170265513282894, test rmse: 0.89393, train rmse: 0.79522\n",
      "Epoch 1839 of 2000. Loss per movie: 0.7170108436971159, test rmse: 0.89393, train rmse: 0.79521\n",
      "Epoch 1840 of 2000. Loss per movie: 0.7169951873075098, test rmse: 0.89393, train rmse: 0.79520\n",
      "Epoch 1841 of 2000. Loss per movie: 0.716979491512217, test rmse: 0.89393, train rmse: 0.79519\n",
      "Epoch 1842 of 2000. Loss per movie: 0.7169638390206553, test rmse: 0.89393, train rmse: 0.79518\n",
      "Epoch 1843 of 2000. Loss per movie: 0.7169482244464361, test rmse: 0.89393, train rmse: 0.79517\n",
      "Epoch 1844 of 2000. Loss per movie: 0.71693262135373, test rmse: 0.89393, train rmse: 0.79516\n",
      "Epoch 1845 of 2000. Loss per movie: 0.7169170130163821, test rmse: 0.89393, train rmse: 0.79515\n",
      "Epoch 1846 of 2000. Loss per movie: 0.7169014399031821, test rmse: 0.89393, train rmse: 0.79515\n",
      "Epoch 1847 of 2000. Loss per movie: 0.7168858941471676, test rmse: 0.89393, train rmse: 0.79514\n",
      "Epoch 1848 of 2000. Loss per movie: 0.7168703623532399, test rmse: 0.89393, train rmse: 0.79513\n",
      "Epoch 1849 of 2000. Loss per movie: 0.7168548332525065, test rmse: 0.89393, train rmse: 0.79512\n",
      "Epoch 1850 of 2000. Loss per movie: 0.7168393424234832, test rmse: 0.89393, train rmse: 0.79511\n",
      "Epoch 1851 of 2000. Loss per movie: 0.7168238411051765, test rmse: 0.89393, train rmse: 0.79510\n",
      "Epoch 1852 of 2000. Loss per movie: 0.7168083981866644, test rmse: 0.89393, train rmse: 0.79509\n",
      "Epoch 1853 of 2000. Loss per movie: 0.7167929497400164, test rmse: 0.89393, train rmse: 0.79509\n",
      "Epoch 1854 of 2000. Loss per movie: 0.7167775182321438, test rmse: 0.89393, train rmse: 0.79508\n",
      "Epoch 1855 of 2000. Loss per movie: 0.7167620962213251, test rmse: 0.89393, train rmse: 0.79507\n",
      "Epoch 1856 of 2000. Loss per movie: 0.716746700008474, test rmse: 0.89393, train rmse: 0.79506\n",
      "Epoch 1857 of 2000. Loss per movie: 0.7167313162693654, test rmse: 0.89393, train rmse: 0.79505\n",
      "Epoch 1858 of 2000. Loss per movie: 0.7167159690301285, test rmse: 0.89393, train rmse: 0.79504\n",
      "Epoch 1859 of 2000. Loss per movie: 0.7167006188850765, test rmse: 0.89393, train rmse: 0.79503\n",
      "Epoch 1860 of 2000. Loss per movie: 0.7166853108389057, test rmse: 0.89393, train rmse: 0.79503\n",
      "Epoch 1861 of 2000. Loss per movie: 0.7166699986111962, test rmse: 0.89393, train rmse: 0.79502\n",
      "Epoch 1862 of 2000. Loss per movie: 0.7166546890766811, test rmse: 0.89393, train rmse: 0.79501\n",
      "Epoch 1863 of 2000. Loss per movie: 0.7166394343274102, test rmse: 0.89393, train rmse: 0.79500\n",
      "Epoch 1864 of 2000. Loss per movie: 0.7166241636315934, test rmse: 0.89393, train rmse: 0.79499\n",
      "Epoch 1865 of 2000. Loss per movie: 0.7166089398540583, test rmse: 0.89393, train rmse: 0.79498\n",
      "Epoch 1866 of 2000. Loss per movie: 0.7165936993503684, test rmse: 0.89393, train rmse: 0.79498\n",
      "Epoch 1867 of 2000. Loss per movie: 0.716578486487358, test rmse: 0.89393, train rmse: 0.79497\n",
      "Epoch 1868 of 2000. Loss per movie: 0.7165632822709191, test rmse: 0.89393, train rmse: 0.79496\n",
      "Epoch 1869 of 2000. Loss per movie: 0.7165480925835553, test rmse: 0.89393, train rmse: 0.79495\n",
      "Epoch 1870 of 2000. Loss per movie: 0.716532940671787, test rmse: 0.89393, train rmse: 0.79494\n",
      "Epoch 1871 of 2000. Loss per movie: 0.7165177980444519, test rmse: 0.89393, train rmse: 0.79493\n",
      "Epoch 1872 of 2000. Loss per movie: 0.7165026678908594, test rmse: 0.89393, train rmse: 0.79493\n",
      "Epoch 1873 of 2000. Loss per movie: 0.7164875833089513, test rmse: 0.89393, train rmse: 0.79492\n",
      "Epoch 1874 of 2000. Loss per movie: 0.7164724406816163, test rmse: 0.89393, train rmse: 0.79491\n",
      "Epoch 1875 of 2000. Loss per movie: 0.7164573745268279, test rmse: 0.89393, train rmse: 0.79490\n",
      "Epoch 1876 of 2000. Loss per movie: 0.7164423030565241, test rmse: 0.89393, train rmse: 0.79489\n",
      "Epoch 1877 of 2000. Loss per movie: 0.7164272454774337, test rmse: 0.89393, train rmse: 0.79488\n",
      "Epoch 1878 of 2000. Loss per movie: 0.7164122146885403, test rmse: 0.89393, train rmse: 0.79488\n",
      "Epoch 1879 of 2000. Loss per movie: 0.7163972031063756, test rmse: 0.89393, train rmse: 0.79487\n",
      "Epoch 1880 of 2000. Loss per movie: 0.7163821630330488, test rmse: 0.89393, train rmse: 0.79486\n",
      "Epoch 1881 of 2000. Loss per movie: 0.7163671863915378, test rmse: 0.89393, train rmse: 0.79485\n",
      "Epoch 1882 of 2000. Loss per movie: 0.7163522014578231, test rmse: 0.89393, train rmse: 0.79484\n",
      "Epoch 1883 of 2000. Loss per movie: 0.7163372204930264, test rmse: 0.89393, train rmse: 0.79483\n",
      "Epoch 1884 of 2000. Loss per movie: 0.7163222848872934, test rmse: 0.89393, train rmse: 0.79483\n",
      "Epoch 1885 of 2000. Loss per movie: 0.7163073408476096, test rmse: 0.89393, train rmse: 0.79482\n",
      "Epoch 1886 of 2000. Loss per movie: 0.7162924302602353, test rmse: 0.89393, train rmse: 0.79481\n",
      "Epoch 1887 of 2000. Loss per movie: 0.7162775102466805, test rmse: 0.89393, train rmse: 0.79480\n",
      "Epoch 1888 of 2000. Loss per movie: 0.7162626255281473, test rmse: 0.89393, train rmse: 0.79479\n",
      "Epoch 1889 of 2000. Loss per movie: 0.7162477456998881, test rmse: 0.89393, train rmse: 0.79478\n",
      "Epoch 1890 of 2000. Loss per movie: 0.716232879337601, test rmse: 0.89393, train rmse: 0.79478\n",
      "Epoch 1891 of 2000. Loss per movie: 0.716218029205354, test rmse: 0.89393, train rmse: 0.79477\n",
      "Epoch 1892 of 2000. Loss per movie: 0.7162031888536551, test rmse: 0.89393, train rmse: 0.79476\n",
      "Epoch 1893 of 2000. Loss per movie: 0.7161883424068319, test rmse: 0.89393, train rmse: 0.79475\n",
      "Epoch 1894 of 2000. Loss per movie: 0.7161735238841824, test rmse: 0.89393, train rmse: 0.79474\n",
      "Epoch 1895 of 2000. Loss per movie: 0.7161587250643763, test rmse: 0.89393, train rmse: 0.79474\n",
      "Epoch 1896 of 2000. Loss per movie: 0.7161439344659005, test rmse: 0.89393, train rmse: 0.79473\n",
      "Epoch 1897 of 2000. Loss per movie: 0.7161291686731627, test rmse: 0.89393, train rmse: 0.79472\n",
      "Epoch 1898 of 2000. Loss per movie: 0.7161144138658231, test rmse: 0.89393, train rmse: 0.79471\n",
      "Epoch 1899 of 2000. Loss per movie: 0.7160996565070362, test rmse: 0.89393, train rmse: 0.79470\n",
      "Epoch 1900 of 2000. Loss per movie: 0.7160849014162025, test rmse: 0.89393, train rmse: 0.79469\n",
      "Epoch 1901 of 2000. Loss per movie: 0.7160701736825542, test rmse: 0.89393, train rmse: 0.79469\n",
      "Epoch 1902 of 2000. Loss per movie: 0.7160554782672388, test rmse: 0.89393, train rmse: 0.79468\n",
      "Epoch 1903 of 2000. Loss per movie: 0.716040765771401, test rmse: 0.89393, train rmse: 0.79467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1904 of 2000. Loss per movie: 0.7160260869404932, test rmse: 0.89393, train rmse: 0.79466\n",
      "Epoch 1905 of 2000. Loss per movie: 0.7160114202998339, test rmse: 0.89393, train rmse: 0.79465\n",
      "Epoch 1906 of 2000. Loss per movie: 0.7159967292078043, test rmse: 0.89393, train rmse: 0.79465\n",
      "Epoch 1907 of 2000. Loss per movie: 0.71598211295823, test rmse: 0.89393, train rmse: 0.79464\n",
      "Epoch 1908 of 2000. Loss per movie: 0.7159674485855239, test rmse: 0.89393, train rmse: 0.79463\n",
      "Epoch 1909 of 2000. Loss per movie: 0.7159528136253357, test rmse: 0.89393, train rmse: 0.79462\n",
      "Epoch 1910 of 2000. Loss per movie: 0.7159382149523985, test rmse: 0.89393, train rmse: 0.79461\n",
      "Epoch 1911 of 2000. Loss per movie: 0.7159235953008944, test rmse: 0.89393, train rmse: 0.79461\n",
      "Epoch 1912 of 2000. Loss per movie: 0.7159090159055594, test rmse: 0.89393, train rmse: 0.79460\n",
      "Epoch 1913 of 2000. Loss per movie: 0.715894426162688, test rmse: 0.89393, train rmse: 0.79459\n",
      "Epoch 1914 of 2000. Loss per movie: 0.7158798646274843, test rmse: 0.89393, train rmse: 0.79458\n",
      "Epoch 1915 of 2000. Loss per movie: 0.7158652926029971, test rmse: 0.89393, train rmse: 0.79457\n",
      "Epoch 1916 of 2000. Loss per movie: 0.7158507558735315, test rmse: 0.89393, train rmse: 0.79456\n",
      "Epoch 1917 of 2000. Loss per movie: 0.7158362120567122, test rmse: 0.89393, train rmse: 0.79456\n",
      "Epoch 1918 of 2000. Loss per movie: 0.7158216707913402, test rmse: 0.89393, train rmse: 0.79455\n",
      "Epoch 1919 of 2000. Loss per movie: 0.7158071490870644, test rmse: 0.89393, train rmse: 0.79454\n",
      "Epoch 1920 of 2000. Loss per movie: 0.7157926656544988, test rmse: 0.89393, train rmse: 0.79453\n",
      "Epoch 1921 of 2000. Loss per movie: 0.7157781587627924, test rmse: 0.89393, train rmse: 0.79452\n",
      "Epoch 1922 of 2000. Loss per movie: 0.7157636817797186, test rmse: 0.89393, train rmse: 0.79452\n",
      "Epoch 1923 of 2000. Loss per movie: 0.7157492082694481, test rmse: 0.89393, train rmse: 0.79451\n",
      "Epoch 1924 of 2000. Loss per movie: 0.7157347310737537, test rmse: 0.89393, train rmse: 0.79450\n",
      "Epoch 1925 of 2000. Loss per movie: 0.7157202774789472, test rmse: 0.89393, train rmse: 0.79449\n",
      "Epoch 1926 of 2000. Loss per movie: 0.7157058452170754, test rmse: 0.89393, train rmse: 0.79448\n",
      "Epoch 1927 of 2000. Loss per movie: 0.715691400339714, test rmse: 0.89393, train rmse: 0.79448\n",
      "Epoch 1928 of 2000. Loss per movie: 0.7156769616992238, test rmse: 0.89393, train rmse: 0.79447\n",
      "Epoch 1929 of 2000. Loss per movie: 0.7156625494236895, test rmse: 0.89393, train rmse: 0.79446\n",
      "Epoch 1930 of 2000. Loss per movie: 0.7156481746402563, test rmse: 0.89393, train rmse: 0.79445\n",
      "Epoch 1931 of 2000. Loss per movie: 0.7156337633569515, test rmse: 0.89393, train rmse: 0.79444\n",
      "Epoch 1932 of 2000. Loss per movie: 0.7156193844628532, test rmse: 0.89393, train rmse: 0.79444\n",
      "Epoch 1933 of 2000. Loss per movie: 0.7156050062066168, test rmse: 0.89393, train rmse: 0.79443\n",
      "Epoch 1934 of 2000. Loss per movie: 0.7155906545988303, test rmse: 0.89393, train rmse: 0.79442\n",
      "Epoch 1935 of 2000. Loss per movie: 0.7155762907299219, test rmse: 0.89393, train rmse: 0.79441\n",
      "Epoch 1936 of 2000. Loss per movie: 0.7155619530133487, test rmse: 0.89393, train rmse: 0.79441\n",
      "Epoch 1937 of 2000. Loss per movie: 0.7155476096268925, test rmse: 0.89393, train rmse: 0.79440\n",
      "Epoch 1938 of 2000. Loss per movie: 0.7155332871481298, test rmse: 0.89393, train rmse: 0.79439\n",
      "Epoch 1939 of 2000. Loss per movie: 0.7155189919556789, test rmse: 0.89393, train rmse: 0.79438\n",
      "Epoch 1940 of 2000. Loss per movie: 0.7155046628148037, test rmse: 0.89393, train rmse: 0.79437\n",
      "Epoch 1941 of 2000. Loss per movie: 0.7154903783242569, test rmse: 0.89393, train rmse: 0.79437\n",
      "Epoch 1942 of 2000. Loss per movie: 0.7154760936210895, test rmse: 0.89393, train rmse: 0.79436\n",
      "Epoch 1943 of 2000. Loss per movie: 0.7154618291877538, test rmse: 0.89393, train rmse: 0.79435\n",
      "Epoch 1944 of 2000. Loss per movie: 0.7154475740388514, test rmse: 0.89393, train rmse: 0.79434\n",
      "Epoch 1945 of 2000. Loss per movie: 0.7154333137161808, test rmse: 0.89393, train rmse: 0.79433\n",
      "Epoch 1946 of 2000. Loss per movie: 0.7154190625361966, test rmse: 0.89393, train rmse: 0.79433\n",
      "Epoch 1947 of 2000. Loss per movie: 0.7154048319804116, test rmse: 0.89392, train rmse: 0.79432\n",
      "Epoch 1948 of 2000. Loss per movie: 0.7153906007158912, test rmse: 0.89392, train rmse: 0.79431\n",
      "Epoch 1949 of 2000. Loss per movie: 0.7153763844765607, test rmse: 0.89392, train rmse: 0.79430\n",
      "Epoch 1950 of 2000. Loss per movie: 0.7153622148720178, test rmse: 0.89392, train rmse: 0.79429\n",
      "Epoch 1951 of 2000. Loss per movie: 0.7153480254937579, test rmse: 0.89392, train rmse: 0.79429\n",
      "Epoch 1952 of 2000. Loss per movie: 0.7153338417853811, test rmse: 0.89392, train rmse: 0.79428\n",
      "Epoch 1953 of 2000. Loss per movie: 0.7153196677158051, test rmse: 0.89392, train rmse: 0.79427\n",
      "Epoch 1954 of 2000. Loss per movie: 0.7153055025054215, test rmse: 0.89392, train rmse: 0.79426\n",
      "Epoch 1955 of 2000. Loss per movie: 0.7152913567852606, test rmse: 0.89392, train rmse: 0.79426\n",
      "Epoch 1956 of 2000. Loss per movie: 0.7152772265155307, test rmse: 0.89392, train rmse: 0.79425\n",
      "Epoch 1957 of 2000. Loss per movie: 0.7152631030496605, test rmse: 0.89392, train rmse: 0.79424\n",
      "Epoch 1958 of 2000. Loss per movie: 0.7152489890808441, test rmse: 0.89392, train rmse: 0.79423\n",
      "Epoch 1959 of 2000. Loss per movie: 0.7152348873731498, test rmse: 0.89392, train rmse: 0.79422\n",
      "Epoch 1960 of 2000. Loss per movie: 0.7152208021789895, test rmse: 0.89392, train rmse: 0.79422\n",
      "Epoch 1961 of 2000. Loss per movie: 0.715206705999431, test rmse: 0.89392, train rmse: 0.79421\n",
      "Epoch 1962 of 2000. Loss per movie: 0.7151926432013086, test rmse: 0.89392, train rmse: 0.79420\n",
      "Epoch 1963 of 2000. Loss per movie: 0.7151785846555985, test rmse: 0.89392, train rmse: 0.79419\n",
      "Epoch 1964 of 2000. Loss per movie: 0.7151645397176074, test rmse: 0.89392, train rmse: 0.79419\n",
      "Epoch 1965 of 2000. Loss per movie: 0.71515049144856, test rmse: 0.89392, train rmse: 0.79418\n",
      "Epoch 1966 of 2000. Loss per movie: 0.7151364927909888, test rmse: 0.89392, train rmse: 0.79417\n",
      "Epoch 1967 of 2000. Loss per movie: 0.7151224742179535, test rmse: 0.89392, train rmse: 0.79416\n",
      "Epoch 1968 of 2000. Loss per movie: 0.7151084785370707, test rmse: 0.89392, train rmse: 0.79415\n",
      "Epoch 1969 of 2000. Loss per movie: 0.715094502771652, test rmse: 0.89392, train rmse: 0.79415\n",
      "Epoch 1970 of 2000. Loss per movie: 0.7150805124771581, test rmse: 0.89392, train rmse: 0.79414\n",
      "Epoch 1971 of 2000. Loss per movie: 0.7150665599582595, test rmse: 0.89392, train rmse: 0.79413\n",
      "Epoch 1972 of 2000. Loss per movie: 0.7150525910675738, test rmse: 0.89392, train rmse: 0.79412\n",
      "Epoch 1973 of 2000. Loss per movie: 0.7150386739145703, test rmse: 0.89393, train rmse: 0.79412\n",
      "Epoch 1974 of 2000. Loss per movie: 0.7150247510916838, test rmse: 0.89393, train rmse: 0.79411\n",
      "Epoch 1975 of 2000. Loss per movie: 0.7150108374823572, test rmse: 0.89393, train rmse: 0.79410\n",
      "Epoch 1976 of 2000. Loss per movie: 0.7149969391817145, test rmse: 0.89393, train rmse: 0.79409\n",
      "Epoch 1977 of 2000. Loss per movie: 0.7149830454878517, test rmse: 0.89393, train rmse: 0.79408\n",
      "Epoch 1978 of 2000. Loss per movie: 0.7149691734812914, test rmse: 0.89393, train rmse: 0.79408\n",
      "Epoch 1979 of 2000. Loss per movie: 0.7149552980019278, test rmse: 0.89393, train rmse: 0.79407\n",
      "Epoch 1980 of 2000. Loss per movie: 0.7149414664641572, test rmse: 0.89393, train rmse: 0.79406\n",
      "Epoch 1981 of 2000. Loss per movie: 0.7149276280516534, test rmse: 0.89393, train rmse: 0.79405\n",
      "Epoch 1982 of 2000. Loss per movie: 0.7149137827644166, test rmse: 0.89393, train rmse: 0.79405\n",
      "Epoch 1983 of 2000. Loss per movie: 0.7148999899944709, test rmse: 0.89393, train rmse: 0.79404\n",
      "Epoch 1984 of 2000. Loss per movie: 0.7148861881527124, test rmse: 0.89393, train rmse: 0.79403\n",
      "Epoch 1985 of 2000. Loss per movie: 0.7148724107623242, test rmse: 0.89393, train rmse: 0.79402\n",
      "Epoch 1986 of 2000. Loss per movie: 0.7148586346476596, test rmse: 0.89393, train rmse: 0.79402\n",
      "Epoch 1987 of 2000. Loss per movie: 0.7148448729203232, test rmse: 0.89393, train rmse: 0.79401\n",
      "Epoch 1988 of 2000. Loss per movie: 0.7148311236667293, test rmse: 0.89393, train rmse: 0.79400\n",
      "Epoch 1989 of 2000. Loss per movie: 0.7148173777441916, test rmse: 0.89393, train rmse: 0.79399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1990 of 2000. Loss per movie: 0.7148036822836125, test rmse: 0.89393, train rmse: 0.79399\n",
      "Epoch 1991 of 2000. Loss per movie: 0.7147899710182344, test rmse: 0.89393, train rmse: 0.79398\n",
      "Epoch 1992 of 2000. Loss per movie: 0.7147762709508753, test rmse: 0.89393, train rmse: 0.79397\n",
      "Epoch 1993 of 2000. Loss per movie: 0.7147625818689144, test rmse: 0.89393, train rmse: 0.79396\n",
      "Epoch 1994 of 2000. Loss per movie: 0.7147489119936822, test rmse: 0.89393, train rmse: 0.79396\n",
      "Epoch 1995 of 2000. Loss per movie: 0.7147352651523495, test rmse: 0.89393, train rmse: 0.79395\n",
      "Epoch 1996 of 2000. Loss per movie: 0.7147216149799606, test rmse: 0.89393, train rmse: 0.79394\n",
      "Epoch 1997 of 2000. Loss per movie: 0.7147079944327102, test rmse: 0.89393, train rmse: 0.79393\n",
      "Epoch 1998 of 2000. Loss per movie: 0.7146943911077293, test rmse: 0.89393, train rmse: 0.79392\n",
      "Epoch 1999 of 2000. Loss per movie: 0.7146807766556031, test rmse: 0.89393, train rmse: 0.79392\n"
     ]
    }
   ],
   "source": [
    "autorec = train_movielens(k=100, epochs=2000, alpha=0.05, optimizer=keras.optimizers.Adam(lr=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6, 0.8],\n",
       "       [0.6, 0.8]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[3.0, 4.0], [3.0, 4.0]])\n",
    "tf.math.l2_normalize(x, axis=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[30., 40.],\n",
       "       [30., 40.]], dtype=float32)>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "folder = 'C:/Users/Ahmad/Documents/Jupyter notebooks/movielens/ml-100k'\n",
    "items_cols = \"movie id | movie title | release date | video release date | IMDb URL | unknown | Action | Adventure | Animation | Children's | Comedy | Crime | Documentary | Drama | Fantasy | Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi | Thriller | War | Western\"\n",
    "items_cols = items_cols.split(' | ')\n",
    "data1 = pd.read_csv(folder + '/u.item', sep='|', encoding='iso-8859-1', names=items_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['movie id', 'movie title', 'release date', 'video release date',\n",
       "       'IMDb URL', 'unknown', 'Action', 'Adventure', 'Animation', 'Children's',\n",
       "       'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir',\n",
       "       'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War',\n",
       "       'Western'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols = \"user id | item id | rating | timestamp\".split(' | ')\n",
    "data = pd.read_csv(folder + '/u.data', sep='\\t', encoding='iso-8859-1', names=data_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(folder + '/u1.base', sep='\\t', encoding='iso-8859-1', names=data_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user id</th>\n",
       "      <th>item id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>874965758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>876893171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>878542960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>876893119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>889751712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79995</th>\n",
       "      <td>943</td>\n",
       "      <td>1067</td>\n",
       "      <td>2</td>\n",
       "      <td>875501756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79996</th>\n",
       "      <td>943</td>\n",
       "      <td>1074</td>\n",
       "      <td>4</td>\n",
       "      <td>888640250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79997</th>\n",
       "      <td>943</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>888640250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79998</th>\n",
       "      <td>943</td>\n",
       "      <td>1228</td>\n",
       "      <td>3</td>\n",
       "      <td>888640275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79999</th>\n",
       "      <td>943</td>\n",
       "      <td>1330</td>\n",
       "      <td>3</td>\n",
       "      <td>888692465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user id  item id  rating  timestamp\n",
       "0            1        1       5  874965758\n",
       "1            1        2       3  876893171\n",
       "2            1        3       4  878542960\n",
       "3            1        4       3  876893119\n",
       "4            1        5       3  889751712\n",
       "...        ...      ...     ...        ...\n",
       "79995      943     1067       2  875501756\n",
       "79996      943     1074       4  888640250\n",
       "79997      943     1188       3  888640250\n",
       "79998      943     1228       3  888640275\n",
       "79999      943     1330       3  888692465\n",
       "\n",
       "[80000 rows x 4 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user id</th>\n",
       "      <th>item id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>880</td>\n",
       "      <td>476</td>\n",
       "      <td>3</td>\n",
       "      <td>880175444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>716</td>\n",
       "      <td>204</td>\n",
       "      <td>5</td>\n",
       "      <td>879795543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>276</td>\n",
       "      <td>1090</td>\n",
       "      <td>1</td>\n",
       "      <td>874795795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>13</td>\n",
       "      <td>225</td>\n",
       "      <td>2</td>\n",
       "      <td>882399156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>12</td>\n",
       "      <td>203</td>\n",
       "      <td>3</td>\n",
       "      <td>879959583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user id  item id  rating  timestamp\n",
       "0          196      242       3  881250949\n",
       "1          186      302       3  891717742\n",
       "2           22      377       1  878887116\n",
       "3          244       51       2  880606923\n",
       "4          166      346       1  886397596\n",
       "...        ...      ...     ...        ...\n",
       "99995      880      476       3  880175444\n",
       "99996      716      204       5  879795543\n",
       "99997      276     1090       1  874795795\n",
       "99998       13      225       2  882399156\n",
       "99999       12      203       3  879959583\n",
       "\n",
       "[100000 rows x 4 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = np.array(\n",
    "    [[0.1, 0.9, 1.0],\n",
    "     [0.3, 0.8, 0.8],\n",
    "     [0.8, 0.1, 0.2]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1.08      , 0.90999997, 1.02      ],\n",
       "       [0.9100001 , 0.99000007, 1.1       ],\n",
       "       [0.27      , 0.81999993, 0.92      ]], dtype=float32)>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(ratings) @ tf.constant(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.1, 0.9, 1. ], dtype=float32)>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(ratings)[0, :] @ tf.constant(ratings)[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = MovieLens()\n",
    "mat = ml.mat.transpose().astype('float32') # shape [items, users]\n",
    "train_mask, test_mask = ml.train_test_split(mask=ml.mat_ind.transpose(), test_size=0.05)\n",
    "validation_mask, test_mask = ml.train_test_split(mask=test_mask, test_size=0.5)\n",
    "\n",
    "train_mask = train_mask.astype(np.float32)\n",
    "validation_mask = validation_mask.astype(np.float32)\n",
    "test_mask = test_mask.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "autorec = AutoRec(m=mat.shape[1], k=200)\n",
    "autorec.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 2000. Epoch loss: 1901.60796, validation rmse: 1.32634, train rmse: 1.32319\n",
      "Epoch 1 of 2000. Epoch loss: 1638.07111, validation rmse: 1.26741, train rmse: 1.25850\n",
      "Epoch 2 of 2000. Epoch loss: 1487.07471, validation rmse: 1.22352, train rmse: 1.20877\n",
      "Epoch 3 of 2000. Epoch loss: 1374.55220, validation rmse: 1.18752, train rmse: 1.16719\n",
      "Epoch 4 of 2000. Epoch loss: 1283.02793, validation rmse: 1.15650, train rmse: 1.13057\n",
      "Epoch 5 of 2000. Epoch loss: 1204.96891, validation rmse: 1.12990, train rmse: 1.09871\n",
      "Epoch 6 of 2000. Epoch loss: 1139.49163, validation rmse: 1.10821, train rmse: 1.07224\n",
      "Epoch 7 of 2000. Epoch loss: 1086.79321, validation rmse: 1.09048, train rmse: 1.05082\n",
      "Epoch 8 of 2000. Epoch loss: 1045.06169, validation rmse: 1.07685, train rmse: 1.03343\n",
      "Epoch 9 of 2000. Epoch loss: 1011.71471, validation rmse: 1.06557, train rmse: 1.01917\n",
      "Epoch 10 of 2000. Epoch loss: 984.66938, validation rmse: 1.05609, train rmse: 1.00707\n",
      "Epoch 11 of 2000. Epoch loss: 961.89120, validation rmse: 1.04806, train rmse: 0.99647\n",
      "Epoch 12 of 2000. Epoch loss: 942.09195, validation rmse: 1.04095, train rmse: 0.98696\n",
      "Epoch 13 of 2000. Epoch loss: 924.41042, validation rmse: 1.03457, train rmse: 0.97823\n",
      "Epoch 14 of 2000. Epoch loss: 908.31068, validation rmse: 1.02855, train rmse: 0.97006\n",
      "Epoch 15 of 2000. Epoch loss: 893.33304, validation rmse: 1.02283, train rmse: 0.96230\n",
      "Epoch 16 of 2000. Epoch loss: 879.22098, validation rmse: 1.01750, train rmse: 0.95491\n",
      "Epoch 17 of 2000. Epoch loss: 865.87988, validation rmse: 1.01245, train rmse: 0.94783\n",
      "Epoch 18 of 2000. Epoch loss: 853.16341, validation rmse: 1.00770, train rmse: 0.94100\n",
      "Epoch 19 of 2000. Epoch loss: 841.00292, validation rmse: 1.00312, train rmse: 0.93439\n",
      "Epoch 20 of 2000. Epoch loss: 829.29182, validation rmse: 0.99876, train rmse: 0.92798\n",
      "Epoch 21 of 2000. Epoch loss: 818.05879, validation rmse: 0.99454, train rmse: 0.92176\n",
      "Epoch 22 of 2000. Epoch loss: 807.19349, validation rmse: 0.99054, train rmse: 0.91577\n",
      "Epoch 23 of 2000. Epoch loss: 796.84313, validation rmse: 0.98674, train rmse: 0.90999\n",
      "Epoch 24 of 2000. Epoch loss: 786.88298, validation rmse: 0.98314, train rmse: 0.90444\n",
      "Epoch 25 of 2000. Epoch loss: 777.42582, validation rmse: 0.97968, train rmse: 0.89905\n",
      "Epoch 26 of 2000. Epoch loss: 768.23957, validation rmse: 0.97636, train rmse: 0.89385\n",
      "Epoch 27 of 2000. Epoch loss: 759.47112, validation rmse: 0.97325, train rmse: 0.88884\n",
      "Epoch 28 of 2000. Epoch loss: 751.07384, validation rmse: 0.97024, train rmse: 0.88400\n",
      "Epoch 29 of 2000. Epoch loss: 742.95566, validation rmse: 0.96735, train rmse: 0.87933\n",
      "Epoch 30 of 2000. Epoch loss: 735.25523, validation rmse: 0.96467, train rmse: 0.87484\n",
      "Epoch 31 of 2000. Epoch loss: 727.82382, validation rmse: 0.96201, train rmse: 0.87051\n",
      "Epoch 32 of 2000. Epoch loss: 720.68811, validation rmse: 0.95963, train rmse: 0.86632\n",
      "Epoch 33 of 2000. Epoch loss: 713.86248, validation rmse: 0.95714, train rmse: 0.86228\n",
      "Epoch 34 of 2000. Epoch loss: 707.28440, validation rmse: 0.95494, train rmse: 0.85838\n",
      "Epoch 35 of 2000. Epoch loss: 700.96647, validation rmse: 0.95289, train rmse: 0.85463\n",
      "Epoch 36 of 2000. Epoch loss: 694.89723, validation rmse: 0.95081, train rmse: 0.85097\n",
      "Epoch 37 of 2000. Epoch loss: 689.03333, validation rmse: 0.94896, train rmse: 0.84746\n",
      "Epoch 38 of 2000. Epoch loss: 683.41037, validation rmse: 0.94685, train rmse: 0.84405\n",
      "Epoch 39 of 2000. Epoch loss: 677.96517, validation rmse: 0.94523, train rmse: 0.84078\n",
      "Epoch 40 of 2000. Epoch loss: 672.79512, validation rmse: 0.94303, train rmse: 0.83760\n",
      "Epoch 41 of 2000. Epoch loss: 667.74542, validation rmse: 0.94183, train rmse: 0.83457\n",
      "Epoch 42 of 2000. Epoch loss: 662.99744, validation rmse: 0.93937, train rmse: 0.83162\n",
      "Epoch 43 of 2000. Epoch loss: 658.29115, validation rmse: 0.93898, train rmse: 0.82890\n",
      "Epoch 44 of 2000. Epoch loss: 654.10258, validation rmse: 0.93571, train rmse: 0.82630\n",
      "Epoch 45 of 2000. Epoch loss: 649.78067, validation rmse: 0.93733, train rmse: 0.82408\n",
      "Epoch 46 of 2000. Epoch loss: 646.36299, validation rmse: 0.93254, train rmse: 0.82221\n",
      "Epoch 47 of 2000. Epoch loss: 642.75216, validation rmse: 0.93829, train rmse: 0.82128\n",
      "Epoch 48 of 2000. Epoch loss: 640.92666, validation rmse: 0.93087, train rmse: 0.82147\n",
      "Epoch 49 of 2000. Epoch loss: 639.51843, validation rmse: 0.94360, train rmse: 0.82242\n",
      "Epoch 50 of 2000. Epoch loss: 639.67994, validation rmse: 0.93198, train rmse: 0.82480\n",
      "Epoch 51 of 2000. Epoch loss: 641.59182, validation rmse: 0.94598, train rmse: 0.82215\n",
      "Epoch 52 of 2000. Epoch loss: 637.71303, validation rmse: 0.92947, train rmse: 0.82124\n",
      "Epoch 53 of 2000. Epoch loss: 636.68286, validation rmse: 0.93908, train rmse: 0.81456\n",
      "Epoch 54 of 2000. Epoch loss: 628.37838, validation rmse: 0.92399, train rmse: 0.81131\n",
      "Epoch 55 of 2000. Epoch loss: 624.18180, validation rmse: 0.93173, train rmse: 0.80705\n",
      "Epoch 56 of 2000. Epoch loss: 618.75104, validation rmse: 0.92094, train rmse: 0.80463\n",
      "Epoch 57 of 2000. Epoch loss: 615.27549, validation rmse: 0.92747, train rmse: 0.80224\n",
      "Epoch 58 of 2000. Epoch loss: 612.09462, validation rmse: 0.91899, train rmse: 0.80055\n",
      "Epoch 59 of 2000. Epoch loss: 609.41144, validation rmse: 0.92533, train rmse: 0.79890\n",
      "Epoch 60 of 2000. Epoch loss: 607.18561, validation rmse: 0.91728, train rmse: 0.79770\n",
      "Epoch 61 of 2000. Epoch loss: 604.99236, validation rmse: 0.92453, train rmse: 0.79653\n",
      "Epoch 62 of 2000. Epoch loss: 603.37451, validation rmse: 0.91581, train rmse: 0.79584\n",
      "Epoch 63 of 2000. Epoch loss: 601.71940, validation rmse: 0.92483, train rmse: 0.79506\n",
      "Epoch 64 of 2000. Epoch loss: 600.58572, validation rmse: 0.91472, train rmse: 0.79498\n",
      "Epoch 65 of 2000. Epoch loss: 599.63104, validation rmse: 0.92599, train rmse: 0.79437\n",
      "Epoch 66 of 2000. Epoch loss: 598.69964, validation rmse: 0.91397, train rmse: 0.79481\n",
      "Epoch 67 of 2000. Epoch loss: 598.44374, validation rmse: 0.92706, train rmse: 0.79377\n",
      "Epoch 68 of 2000. Epoch loss: 597.06043, validation rmse: 0.91313, train rmse: 0.79413\n",
      "Epoch 69 of 2000. Epoch loss: 596.92420, validation rmse: 0.92674, train rmse: 0.79221\n",
      "Epoch 70 of 2000. Epoch loss: 594.60959, validation rmse: 0.91177, train rmse: 0.79195\n",
      "Epoch 71 of 2000. Epoch loss: 593.92543, validation rmse: 0.92498, train rmse: 0.78966\n",
      "Epoch 72 of 2000. Epoch loss: 591.15094, validation rmse: 0.91023, train rmse: 0.78906\n",
      "Epoch 73 of 2000. Epoch loss: 590.18406, validation rmse: 0.92297, train rmse: 0.78702\n",
      "Epoch 74 of 2000. Epoch loss: 587.62498, validation rmse: 0.90902, train rmse: 0.78644\n",
      "Epoch 75 of 2000. Epoch loss: 586.73094, validation rmse: 0.92140, train rmse: 0.78487\n",
      "Epoch 76 of 2000. Epoch loss: 584.57239, validation rmse: 0.90816, train rmse: 0.78453\n",
      "Epoch 77 of 2000. Epoch loss: 584.07916, validation rmse: 0.92047, train rmse: 0.78329\n",
      "Epoch 78 of 2000. Epoch loss: 582.14311, validation rmse: 0.90767, train rmse: 0.78320\n",
      "Epoch 79 of 2000. Epoch loss: 582.10421, validation rmse: 0.91992, train rmse: 0.78214\n",
      "Epoch 80 of 2000. Epoch loss: 580.20075, validation rmse: 0.90733, train rmse: 0.78220\n",
      "Epoch 81 of 2000. Epoch loss: 580.50088, validation rmse: 0.91924, train rmse: 0.78104\n",
      "Epoch 82 of 2000. Epoch loss: 578.39335, validation rmse: 0.90699, train rmse: 0.78106\n",
      "Epoch 83 of 2000. Epoch loss: 578.82214, validation rmse: 0.91820, train rmse: 0.77973\n",
      "Epoch 84 of 2000. Epoch loss: 576.44468, validation rmse: 0.90642, train rmse: 0.77952\n",
      "Epoch 85 of 2000. Epoch loss: 576.72737, validation rmse: 0.91669, train rmse: 0.77803\n",
      "Epoch 86 of 2000. Epoch loss: 574.17100, validation rmse: 0.90553, train rmse: 0.77763\n",
      "Epoch 87 of 2000. Epoch loss: 574.25155, validation rmse: 0.91505, train rmse: 0.77618\n",
      "Epoch 88 of 2000. Epoch loss: 571.79313, validation rmse: 0.90458, train rmse: 0.77568\n",
      "Epoch 89 of 2000. Epoch loss: 571.71407, validation rmse: 0.91368, train rmse: 0.77442\n",
      "Epoch 90 of 2000. Epoch loss: 569.49554, validation rmse: 0.90354, train rmse: 0.77402\n",
      "Epoch 91 of 2000. Epoch loss: 569.49427, validation rmse: 0.91293, train rmse: 0.77306\n",
      "Epoch 92 of 2000. Epoch loss: 567.63144, validation rmse: 0.90255, train rmse: 0.77289\n",
      "Epoch 93 of 2000. Epoch loss: 567.80834, validation rmse: 0.91316, train rmse: 0.77230\n",
      "Epoch 94 of 2000. Epoch loss: 566.38606, validation rmse: 0.90167, train rmse: 0.77264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95 of 2000. Epoch loss: 566.98252, validation rmse: 0.91483, train rmse: 0.77268\n",
      "Epoch 96 of 2000. Epoch loss: 566.26863, validation rmse: 0.90143, train rmse: 0.77401\n",
      "Epoch 97 of 2000. Epoch loss: 567.80377, validation rmse: 0.91832, train rmse: 0.77487\n",
      "Epoch 98 of 2000. Epoch loss: 567.92818, validation rmse: 0.90256, train rmse: 0.77781\n",
      "Epoch 99 of 2000. Epoch loss: 571.16591, validation rmse: 0.92233, train rmse: 0.77822\n",
      "Epoch 100 of 2000. Epoch loss: 570.91866, validation rmse: 0.90414, train rmse: 0.78108\n",
      "Epoch 101 of 2000. Epoch loss: 574.47186, validation rmse: 0.92119, train rmse: 0.77749\n",
      "Epoch 102 of 2000. Epoch loss: 570.39726, validation rmse: 0.90250, train rmse: 0.77745\n",
      "Epoch 103 of 2000. Epoch loss: 570.63393, validation rmse: 0.91539, train rmse: 0.77261\n",
      "Epoch 104 of 2000. Epoch loss: 565.61670, validation rmse: 0.90003, train rmse: 0.77126\n",
      "Epoch 105 of 2000. Epoch loss: 563.82456, validation rmse: 0.91037, train rmse: 0.76850\n",
      "Epoch 106 of 2000. Epoch loss: 561.19577, validation rmse: 0.89901, train rmse: 0.76761\n",
      "Epoch 107 of 2000. Epoch loss: 559.64129, validation rmse: 0.90774, train rmse: 0.76624\n",
      "Epoch 108 of 2000. Epoch loss: 558.56999, validation rmse: 0.89843, train rmse: 0.76578\n",
      "Epoch 109 of 2000. Epoch loss: 557.36927, validation rmse: 0.90670, train rmse: 0.76504\n",
      "Epoch 110 of 2000. Epoch loss: 557.03898, validation rmse: 0.89788, train rmse: 0.76487\n",
      "Epoch 111 of 2000. Epoch loss: 556.04088, validation rmse: 0.90669, train rmse: 0.76448\n",
      "Epoch 112 of 2000. Epoch loss: 556.18023, validation rmse: 0.89731, train rmse: 0.76458\n",
      "Epoch 113 of 2000. Epoch loss: 555.35215, validation rmse: 0.90744, train rmse: 0.76441\n",
      "Epoch 114 of 2000. Epoch loss: 555.80268, validation rmse: 0.89674, train rmse: 0.76486\n",
      "Epoch 115 of 2000. Epoch loss: 555.27530, validation rmse: 0.90893, train rmse: 0.76483\n",
      "Epoch 116 of 2000. Epoch loss: 555.90438, validation rmse: 0.89627, train rmse: 0.76568\n",
      "Epoch 117 of 2000. Epoch loss: 555.80496, validation rmse: 0.91111, train rmse: 0.76574\n",
      "Epoch 118 of 2000. Epoch loss: 556.45756, validation rmse: 0.89609, train rmse: 0.76702\n",
      "Epoch 119 of 2000. Epoch loss: 557.00416, validation rmse: 0.91343, train rmse: 0.76685\n",
      "Epoch 120 of 2000. Epoch loss: 557.19466, validation rmse: 0.89628, train rmse: 0.76824\n",
      "Epoch 121 of 2000. Epoch loss: 558.29197, validation rmse: 0.91517, train rmse: 0.76765\n",
      "Epoch 122 of 2000. Epoch loss: 557.54663, validation rmse: 0.89666, train rmse: 0.76904\n",
      "Epoch 123 of 2000. Epoch loss: 559.30956, validation rmse: 0.91595, train rmse: 0.76808\n",
      "Epoch 124 of 2000. Epoch loss: 557.56405, validation rmse: 0.89731, train rmse: 0.76922\n",
      "Epoch 125 of 2000. Epoch loss: 559.75473, validation rmse: 0.91514, train rmse: 0.76784\n",
      "Epoch 126 of 2000. Epoch loss: 556.87627, validation rmse: 0.89799, train rmse: 0.76873\n",
      "Epoch 127 of 2000. Epoch loss: 559.43931, validation rmse: 0.91319, train rmse: 0.76697\n",
      "Epoch 128 of 2000. Epoch loss: 555.71942, validation rmse: 0.89804, train rmse: 0.76713\n",
      "Epoch 129 of 2000. Epoch loss: 557.80553, validation rmse: 0.91014, train rmse: 0.76503\n",
      "Epoch 130 of 2000. Epoch loss: 553.64378, validation rmse: 0.89729, train rmse: 0.76462\n",
      "Epoch 131 of 2000. Epoch loss: 555.02584, validation rmse: 0.90728, train rmse: 0.76281\n",
      "Epoch 132 of 2000. Epoch loss: 551.34260, validation rmse: 0.89646, train rmse: 0.76226\n",
      "Epoch 133 of 2000. Epoch loss: 552.29230, validation rmse: 0.90525, train rmse: 0.76102\n",
      "Epoch 134 of 2000. Epoch loss: 549.44001, validation rmse: 0.89577, train rmse: 0.76072\n",
      "Epoch 135 of 2000. Epoch loss: 550.34783, validation rmse: 0.90435, train rmse: 0.75997\n",
      "Epoch 136 of 2000. Epoch loss: 548.28619, validation rmse: 0.89528, train rmse: 0.76003\n",
      "Epoch 137 of 2000. Epoch loss: 549.26328, validation rmse: 0.90453, train rmse: 0.75972\n",
      "Epoch 138 of 2000. Epoch loss: 547.88458, validation rmse: 0.89503, train rmse: 0.76022\n",
      "Epoch 139 of 2000. Epoch loss: 549.09184, validation rmse: 0.90569, train rmse: 0.76022\n",
      "Epoch 140 of 2000. Epoch loss: 548.21857, validation rmse: 0.89502, train rmse: 0.76121\n",
      "Epoch 141 of 2000. Epoch loss: 549.72001, validation rmse: 0.90736, train rmse: 0.76125\n",
      "Epoch 142 of 2000. Epoch loss: 549.14136, validation rmse: 0.89511, train rmse: 0.76260\n",
      "Epoch 143 of 2000. Epoch loss: 550.77863, validation rmse: 0.90888, train rmse: 0.76224\n",
      "Epoch 144 of 2000. Epoch loss: 550.07178, validation rmse: 0.89495, train rmse: 0.76349\n",
      "Epoch 145 of 2000. Epoch loss: 551.41429, validation rmse: 0.90914, train rmse: 0.76218\n",
      "Epoch 146 of 2000. Epoch loss: 550.12103, validation rmse: 0.89411, train rmse: 0.76280\n",
      "Epoch 147 of 2000. Epoch loss: 550.52581, validation rmse: 0.90806, train rmse: 0.76089\n",
      "Epoch 148 of 2000. Epoch loss: 548.87043, validation rmse: 0.89290, train rmse: 0.76104\n",
      "Epoch 149 of 2000. Epoch loss: 548.56629, validation rmse: 0.90679, train rmse: 0.75936\n",
      "Epoch 150 of 2000. Epoch loss: 547.30318, validation rmse: 0.89190, train rmse: 0.75945\n",
      "Epoch 151 of 2000. Epoch loss: 546.76221, validation rmse: 0.90622, train rmse: 0.75841\n",
      "Epoch 152 of 2000. Epoch loss: 546.17609, validation rmse: 0.89140, train rmse: 0.75885\n",
      "Epoch 153 of 2000. Epoch loss: 545.99326, validation rmse: 0.90682, train rmse: 0.75839\n",
      "Epoch 154 of 2000. Epoch loss: 545.91803, validation rmse: 0.89140, train rmse: 0.75923\n",
      "Epoch 155 of 2000. Epoch loss: 546.27466, validation rmse: 0.90827, train rmse: 0.75912\n",
      "Epoch 156 of 2000. Epoch loss: 546.33262, validation rmse: 0.89194, train rmse: 0.76044\n",
      "Epoch 157 of 2000. Epoch loss: 547.49505, validation rmse: 0.90994, train rmse: 0.76025\n",
      "Epoch 158 of 2000. Epoch loss: 547.17602, validation rmse: 0.89282, train rmse: 0.76155\n",
      "Epoch 159 of 2000. Epoch loss: 548.78669, validation rmse: 0.91047, train rmse: 0.76077\n",
      "Epoch 160 of 2000. Epoch loss: 547.41910, validation rmse: 0.89343, train rmse: 0.76158\n",
      "Epoch 161 of 2000. Epoch loss: 549.06050, validation rmse: 0.90912, train rmse: 0.76006\n",
      "Epoch 162 of 2000. Epoch loss: 546.50734, validation rmse: 0.89359, train rmse: 0.76034\n",
      "Epoch 163 of 2000. Epoch loss: 547.97388, validation rmse: 0.90692, train rmse: 0.75870\n",
      "Epoch 164 of 2000. Epoch loss: 544.92686, validation rmse: 0.89342, train rmse: 0.75866\n",
      "Epoch 165 of 2000. Epoch loss: 546.29171, validation rmse: 0.90484, train rmse: 0.75737\n",
      "Epoch 166 of 2000. Epoch loss: 543.38361, validation rmse: 0.89324, train rmse: 0.75733\n",
      "Epoch 167 of 2000. Epoch loss: 544.87259, validation rmse: 0.90340, train rmse: 0.75638\n",
      "Epoch 168 of 2000. Epoch loss: 542.20582, validation rmse: 0.89302, train rmse: 0.75639\n",
      "Epoch 169 of 2000. Epoch loss: 543.78016, validation rmse: 0.90271, train rmse: 0.75578\n",
      "Epoch 170 of 2000. Epoch loss: 541.43907, validation rmse: 0.89275, train rmse: 0.75596\n",
      "Epoch 171 of 2000. Epoch loss: 543.16456, validation rmse: 0.90282, train rmse: 0.75561\n",
      "Epoch 172 of 2000. Epoch loss: 541.11662, validation rmse: 0.89252, train rmse: 0.75599\n",
      "Epoch 173 of 2000. Epoch loss: 542.98606, validation rmse: 0.90358, train rmse: 0.75578\n",
      "Epoch 174 of 2000. Epoch loss: 541.18399, validation rmse: 0.89229, train rmse: 0.75649\n",
      "Epoch 175 of 2000. Epoch loss: 543.24173, validation rmse: 0.90495, train rmse: 0.75633\n",
      "Best validation rmse: 0.89140\n"
     ]
    }
   ],
   "source": [
    "autorec.fit(mat, train_mask, max_epochs=2000, validation_mask=validation_mask, alpha=0.1, optimizer=keras.optimizers.Adam(lr=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "943"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Best validation rmse: 0.88956, k=300, alpha=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Best validation rmse: 0.88296, k=100, alpha=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.01, 'activation1': 'relu', 'activation2': 'relu', 'k': 50, 'rmse': 0.89449304, 'model': <autorec.AutoRec object at 0x0000016136B58CF8>}\n",
      "{'alpha': 0.01, 'activation1': 'relu', 'activation2': 'relu', 'k': 100, 'rmse': 0.8970445, 'model': <autorec.AutoRec object at 0x0000015FB16360F0>}\n",
      "{'alpha': 0.01, 'activation1': 'relu', 'activation2': 'relu', 'k': 200, 'rmse': 0.9014169, 'model': <autorec.AutoRec object at 0x00000161247BF320>}\n",
      "{'alpha': 0.01, 'activation1': 'relu', 'activation2': 'relu', 'k': 300, 'rmse': 0.8944723, 'model': <autorec.AutoRec object at 0x0000016136B9BBA8>}\n",
      "{'alpha': 0.01, 'activation1': 'relu', 'activation2': 'sigmoid', 'k': 50, 'rmse': 0.8961052, 'model': <autorec.AutoRec object at 0x000001613D03F898>}\n",
      "{'alpha': 0.01, 'activation1': 'relu', 'activation2': 'sigmoid', 'k': 100, 'rmse': 0.89458317, 'model': <autorec.AutoRec object at 0x0000015FB2D13208>}\n",
      "{'alpha': 0.01, 'activation1': 'relu', 'activation2': 'sigmoid', 'k': 200, 'rmse': 0.8970079, 'model': <autorec.AutoRec object at 0x000001600AC46128>}\n",
      "{'alpha': 0.01, 'activation1': 'relu', 'activation2': 'sigmoid', 'k': 300, 'rmse': 0.900283, 'model': <autorec.AutoRec object at 0x000001601A2A8208>}\n",
      "{'alpha': 0.01, 'activation1': 'sigmoid', 'activation2': 'relu', 'k': 50, 'rmse': 0.89225245, 'model': <autorec.AutoRec object at 0x000001600AC2CFD0>}\n",
      "{'alpha': 0.01, 'activation1': 'sigmoid', 'activation2': 'relu', 'k': 100, 'rmse': 0.8987644, 'model': <autorec.AutoRec object at 0x00000161445F60B8>}\n",
      "{'alpha': 0.01, 'activation1': 'sigmoid', 'activation2': 'relu', 'k': 200, 'rmse': 0.89701796, 'model': <autorec.AutoRec object at 0x000001600E9FE518>}\n",
      "{'alpha': 0.01, 'activation1': 'sigmoid', 'activation2': 'relu', 'k': 300, 'rmse': 0.8971096, 'model': <autorec.AutoRec object at 0x000001612A4BC1D0>}\n",
      "{'alpha': 0.01, 'activation1': 'sigmoid', 'activation2': 'sigmoid', 'k': 50, 'rmse': 0.8917625, 'model': <autorec.AutoRec object at 0x000001614448D908>}\n",
      "{'alpha': 0.01, 'activation1': 'sigmoid', 'activation2': 'sigmoid', 'k': 100, 'rmse': 0.8959569, 'model': <autorec.AutoRec object at 0x0000015FB2D30390>}\n",
      "{'alpha': 0.01, 'activation1': 'sigmoid', 'activation2': 'sigmoid', 'k': 200, 'rmse': 0.89603096, 'model': <autorec.AutoRec object at 0x0000015FB2D94978>}\n",
      "{'alpha': 0.01, 'activation1': 'sigmoid', 'activation2': 'sigmoid', 'k': 300, 'rmse': 0.90262496, 'model': <autorec.AutoRec object at 0x0000015FB2D25400>}\n",
      "{'alpha': 0.1, 'activation1': 'relu', 'activation2': 'relu', 'k': 50, 'rmse': 0.89390737, 'model': <autorec.AutoRec object at 0x00000161247298D0>}\n",
      "{'alpha': 0.1, 'activation1': 'relu', 'activation2': 'relu', 'k': 100, 'rmse': 0.89721906, 'model': <autorec.AutoRec object at 0x000001613C8BE7F0>}\n",
      "{'alpha': 0.1, 'activation1': 'relu', 'activation2': 'relu', 'k': 200, 'rmse': 0.9029309, 'model': <autorec.AutoRec object at 0x000001600C68A898>}\n",
      "{'alpha': 0.1, 'activation1': 'relu', 'activation2': 'relu', 'k': 300, 'rmse': 0.90140504, 'model': <autorec.AutoRec object at 0x000001612EC0E160>}\n",
      "{'alpha': 0.1, 'activation1': 'relu', 'activation2': 'sigmoid', 'k': 50, 'rmse': 0.89372075, 'model': <autorec.AutoRec object at 0x00000161462850B8>}\n",
      "{'alpha': 0.1, 'activation1': 'relu', 'activation2': 'sigmoid', 'k': 100, 'rmse': 0.89557225, 'model': <autorec.AutoRec object at 0x0000016144645A58>}\n",
      "{'alpha': 0.1, 'activation1': 'relu', 'activation2': 'sigmoid', 'k': 200, 'rmse': 0.89532226, 'model': <autorec.AutoRec object at 0x0000015FB1494400>}\n",
      "{'alpha': 0.1, 'activation1': 'relu', 'activation2': 'sigmoid', 'k': 300, 'rmse': 0.9038261, 'model': <autorec.AutoRec object at 0x000001613EFFFC50>}\n",
      "{'alpha': 0.1, 'activation1': 'sigmoid', 'activation2': 'relu', 'k': 50, 'rmse': 0.8958035, 'model': <autorec.AutoRec object at 0x0000016145945DD8>}\n",
      "{'alpha': 0.1, 'activation1': 'sigmoid', 'activation2': 'relu', 'k': 100, 'rmse': 0.9029905, 'model': <autorec.AutoRec object at 0x0000016145A1AC50>}\n",
      "{'alpha': 0.1, 'activation1': 'sigmoid', 'activation2': 'relu', 'k': 200, 'rmse': 0.8986823, 'model': <autorec.AutoRec object at 0x0000016124729A20>}\n",
      "{'alpha': 0.1, 'activation1': 'sigmoid', 'activation2': 'relu', 'k': 300, 'rmse': 0.90390706, 'model': <autorec.AutoRec object at 0x0000016145B33080>}\n",
      "{'alpha': 0.1, 'activation1': 'sigmoid', 'activation2': 'sigmoid', 'k': 50, 'rmse': 0.896075, 'model': <autorec.AutoRec object at 0x000001614459A550>}\n",
      "{'alpha': 0.1, 'activation1': 'sigmoid', 'activation2': 'sigmoid', 'k': 100, 'rmse': 0.89545584, 'model': <autorec.AutoRec object at 0x0000015FAD257F98>}\n",
      "{'alpha': 0.1, 'activation1': 'sigmoid', 'activation2': 'sigmoid', 'k': 200, 'rmse': 0.9060647, 'model': <autorec.AutoRec object at 0x000001600E9F9940>}\n",
      "{'alpha': 0.1, 'activation1': 'sigmoid', 'activation2': 'sigmoid', 'k': 300, 'rmse': 0.89952475, 'model': <autorec.AutoRec object at 0x00000161446A4588>}\n",
      "{'alpha': 1.0, 'activation1': 'relu', 'activation2': 'relu', 'k': 50, 'rmse': 0.8941436, 'model': <autorec.AutoRec object at 0x0000015FB2D2FE80>}\n",
      "{'alpha': 1.0, 'activation1': 'relu', 'activation2': 'relu', 'k': 100, 'rmse': 0.8956992, 'model': <autorec.AutoRec object at 0x000001613EFEC400>}\n",
      "{'alpha': 1.0, 'activation1': 'relu', 'activation2': 'relu', 'k': 200, 'rmse': 0.8969167, 'model': <autorec.AutoRec object at 0x000001612A487550>}\n",
      "{'alpha': 1.0, 'activation1': 'relu', 'activation2': 'relu', 'k': 300, 'rmse': 0.89922756, 'model': <autorec.AutoRec object at 0x000001613EF9C438>}\n",
      "{'alpha': 1.0, 'activation1': 'relu', 'activation2': 'sigmoid', 'k': 50, 'rmse': 0.89262265, 'model': <autorec.AutoRec object at 0x000001600C69D438>}\n",
      "{'alpha': 1.0, 'activation1': 'relu', 'activation2': 'sigmoid', 'k': 100, 'rmse': 0.8988337, 'model': <autorec.AutoRec object at 0x000001612E0CABE0>}\n",
      "{'alpha': 1.0, 'activation1': 'relu', 'activation2': 'sigmoid', 'k': 200, 'rmse': 0.9015427, 'model': <autorec.AutoRec object at 0x000001612EC0EB00>}\n",
      "{'alpha': 1.0, 'activation1': 'relu', 'activation2': 'sigmoid', 'k': 300, 'rmse': 0.89520603, 'model': <autorec.AutoRec object at 0x000001612EBD41D0>}\n",
      "{'alpha': 1.0, 'activation1': 'sigmoid', 'activation2': 'relu', 'k': 50, 'rmse': 0.8943422, 'model': <autorec.AutoRec object at 0x00000161514DF710>}\n",
      "{'alpha': 1.0, 'activation1': 'sigmoid', 'activation2': 'relu', 'k': 100, 'rmse': 0.8984094, 'model': <autorec.AutoRec object at 0x00000161445C5C88>}\n",
      "{'alpha': 1.0, 'activation1': 'sigmoid', 'activation2': 'relu', 'k': 200, 'rmse': 0.8981048, 'model': <autorec.AutoRec object at 0x000001613C8E8C50>}\n",
      "{'alpha': 1.0, 'activation1': 'sigmoid', 'activation2': 'relu', 'k': 300, 'rmse': 0.8984, 'model': <autorec.AutoRec object at 0x000001600AC54470>}\n",
      "{'alpha': 1.0, 'activation1': 'sigmoid', 'activation2': 'sigmoid', 'k': 50, 'rmse': 0.90078735, 'model': <autorec.AutoRec object at 0x000001612A4EE710>}\n",
      "{'alpha': 1.0, 'activation1': 'sigmoid', 'activation2': 'sigmoid', 'k': 100, 'rmse': 0.8981802, 'model': <autorec.AutoRec object at 0x000001600AC3F160>}\n",
      "{'alpha': 1.0, 'activation1': 'sigmoid', 'activation2': 'sigmoid', 'k': 200, 'rmse': 0.89722806, 'model': <autorec.AutoRec object at 0x000001613F0C7828>}\n",
      "{'alpha': 1.0, 'activation1': 'sigmoid', 'activation2': 'sigmoid', 'k': 300, 'rmse': 0.8987403, 'model': <autorec.AutoRec object at 0x0000016124729978>}\n"
     ]
    }
   ],
   "source": [
    "# Do a grid search\n",
    "res = []\n",
    "for alpha in [0.01, 0.1, 1.0]:\n",
    "    for activation1 in ['relu', 'sigmoid']:\n",
    "        for activation2 in ['relu', 'sigmoid']:\n",
    "            for k in [50, 100, 200, 300]:\n",
    "                autorec = AutoRec(m=mat.shape[1], k=k)\n",
    "                autorec.create_model()\n",
    "                best_model, rmse = autorec.fit(mat, train_mask, max_epochs=1000, validation_mask=validation_mask, verbose=False,\n",
    "                                   alpha=alpha, optimizer=keras.optimizers.Adam(lr=1e-4))\n",
    "                res.append({'alpha':alpha, 'activation1':activation1, 'activation2':activation2, \n",
    "                            'k':k, 'rmse':rmse, 'model':best_model})\n",
    "                print(res[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "autorec = AutoRec(m=mat.shape[1], k=200, activation1='relu', activation2='relu')\n",
    "autorec.create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 of 1000. Epoch loss: 6458.14147, validation rmse: 2.38761, train rmse: 2.38336\n",
      "Epoch 1 of 1000. Epoch loss: 5335.63937, validation rmse: 2.30039, train rmse: 2.28501\n",
      "Epoch 2 of 1000. Epoch loss: 4908.13789, validation rmse: 2.22918, train rmse: 2.20539\n",
      "Epoch 3 of 1000. Epoch loss: 4571.35740, validation rmse: 2.16504, train rmse: 2.13073\n",
      "Epoch 4 of 1000. Epoch loss: 4266.07818, validation rmse: 2.10636, train rmse: 2.05849\n",
      "Epoch 5 of 1000. Epoch loss: 3982.58365, validation rmse: 2.05228, train rmse: 1.98917\n",
      "Epoch 6 of 1000. Epoch loss: 3720.32765, validation rmse: 2.00450, train rmse: 1.92300\n",
      "Epoch 7 of 1000. Epoch loss: 3477.39129, validation rmse: 1.96013, train rmse: 1.85885\n",
      "Epoch 8 of 1000. Epoch loss: 3250.46512, validation rmse: 1.91966, train rmse: 1.79805\n",
      "Epoch 9 of 1000. Epoch loss: 3042.80303, validation rmse: 1.88273, train rmse: 1.74074\n",
      "Epoch 10 of 1000. Epoch loss: 2853.23395, validation rmse: 1.84597, train rmse: 1.68592\n",
      "Epoch 11 of 1000. Epoch loss: 2677.66982, validation rmse: 1.80987, train rmse: 1.63438\n",
      "Epoch 12 of 1000. Epoch loss: 2518.24648, validation rmse: 1.77573, train rmse: 1.58660\n",
      "Epoch 13 of 1000. Epoch loss: 2374.82464, validation rmse: 1.74397, train rmse: 1.54195\n",
      "Epoch 14 of 1000. Epoch loss: 2244.41982, validation rmse: 1.71271, train rmse: 1.50029\n",
      "Epoch 15 of 1000. Epoch loss: 2125.52974, validation rmse: 1.68760, train rmse: 1.46114\n",
      "Epoch 16 of 1000. Epoch loss: 2017.06170, validation rmse: 1.66245, train rmse: 1.42478\n",
      "Epoch 17 of 1000. Epoch loss: 1918.17000, validation rmse: 1.64119, train rmse: 1.39104\n",
      "Epoch 18 of 1000. Epoch loss: 1829.08405, validation rmse: 1.61760, train rmse: 1.36304\n",
      "Epoch 19 of 1000. Epoch loss: 1751.47806, validation rmse: 1.61748, train rmse: 1.35003\n",
      "Epoch 20 of 1000. Epoch loss: 1706.35711, validation rmse: 1.62417, train rmse: 1.38276\n",
      "Epoch 21 of 1000. Epoch loss: 1738.54709, validation rmse: 1.74588, train rmse: 1.48455\n",
      "Epoch 22 of 1000. Epoch loss: 1875.19898, validation rmse: 1.75670, train rmse: 1.57007\n",
      "Epoch 23 of 1000. Epoch loss: 2184.47759, validation rmse: 1.57797, train rmse: 1.29069\n",
      "Epoch 24 of 1000. Epoch loss: 1566.35468, validation rmse: 1.52324, train rmse: 1.24146\n",
      "Epoch 25 of 1000. Epoch loss: 1449.41889, validation rmse: 1.52084, train rmse: 1.21396\n",
      "Epoch 26 of 1000. Epoch loss: 1386.90419, validation rmse: 1.49662, train rmse: 1.19884\n",
      "Epoch 27 of 1000. Epoch loss: 1348.55648, validation rmse: 1.50104, train rmse: 1.18162\n",
      "Epoch 28 of 1000. Epoch loss: 1307.74743, validation rmse: 1.48039, train rmse: 1.17947\n",
      "Epoch 29 of 1000. Epoch loss: 1293.12086, validation rmse: 1.49984, train rmse: 1.17387\n",
      "Epoch 30 of 1000. Epoch loss: 1271.07716, validation rmse: 1.48643, train rmse: 1.19226\n",
      "Epoch 31 of 1000. Epoch loss: 1294.24469, validation rmse: 1.51868, train rmse: 1.18976\n",
      "Epoch 32 of 1000. Epoch loss: 1272.34166, validation rmse: 1.50374, train rmse: 1.22141\n",
      "Epoch 33 of 1000. Epoch loss: 1331.94990, validation rmse: 1.51245, train rmse: 1.17902\n",
      "Epoch 34 of 1000. Epoch loss: 1241.53614, validation rmse: 1.48158, train rmse: 1.18802\n",
      "Epoch 35 of 1000. Epoch loss: 1265.42939, validation rmse: 1.47280, train rmse: 1.12421\n",
      "Epoch 36 of 1000. Epoch loss: 1149.19724, validation rmse: 1.43317, train rmse: 1.11312\n",
      "Epoch 37 of 1000. Epoch loss: 1131.54916, validation rmse: 1.42905, train rmse: 1.06614\n",
      "Epoch 38 of 1000. Epoch loss: 1052.16421, validation rmse: 1.39519, train rmse: 1.05249\n",
      "Epoch 39 of 1000. Epoch loss: 1026.50686, validation rmse: 1.39807, train rmse: 1.02428\n",
      "Epoch 40 of 1000. Epoch loss: 979.95731, validation rmse: 1.37246, train rmse: 1.01410\n",
      "Epoch 41 of 1000. Epoch loss: 959.20894, validation rmse: 1.37766, train rmse: 0.99516\n",
      "Epoch 42 of 1000. Epoch loss: 927.71882, validation rmse: 1.35554, train rmse: 0.98822\n",
      "Epoch 43 of 1000. Epoch loss: 912.04123, validation rmse: 1.36307, train rmse: 0.97397\n",
      "Epoch 44 of 1000. Epoch loss: 888.04408, validation rmse: 1.34449, train rmse: 0.97076\n",
      "Epoch 45 of 1000. Epoch loss: 877.77269, validation rmse: 1.35555, train rmse: 0.96042\n",
      "Epoch 46 of 1000. Epoch loss: 859.18828, validation rmse: 1.33803, train rmse: 0.96277\n",
      "Epoch 47 of 1000. Epoch loss: 856.76075, validation rmse: 1.35441, train rmse: 0.95643\n",
      "Epoch 48 of 1000. Epoch loss: 842.78515, validation rmse: 1.34074, train rmse: 0.96809\n",
      "Epoch 49 of 1000. Epoch loss: 853.60780, validation rmse: 1.36552, train rmse: 0.96692\n",
      "Epoch 50 of 1000. Epoch loss: 844.34806, validation rmse: 1.35408, train rmse: 0.99085\n",
      "Epoch 51 of 1000. Epoch loss: 875.72443, validation rmse: 1.38076, train rmse: 0.98629\n",
      "Epoch 52 of 1000. Epoch loss: 857.58004, validation rmse: 1.37130, train rmse: 1.01640\n",
      "Epoch 53 of 1000. Epoch loss: 905.61939, validation rmse: 1.38649, train rmse: 0.98907\n",
      "Epoch 54 of 1000. Epoch loss: 855.21356, validation rmse: 1.36426, train rmse: 1.00424\n",
      "Epoch 55 of 1000. Epoch loss: 885.59865, validation rmse: 1.36269, train rmse: 0.95418\n",
      "Epoch 56 of 1000. Epoch loss: 808.95045, validation rmse: 1.33389, train rmse: 0.94905\n",
      "Epoch 57 of 1000. Epoch loss: 805.76542, validation rmse: 1.33010, train rmse: 0.90316\n",
      "Epoch 58 of 1000. Epoch loss: 743.03825, validation rmse: 1.30105, train rmse: 0.89122\n",
      "Epoch 59 of 1000. Epoch loss: 726.20871, validation rmse: 1.30165, train rmse: 0.86100\n",
      "Epoch 60 of 1000. Epoch loss: 687.19035, validation rmse: 1.28144, train rmse: 0.85166\n",
      "Epoch 61 of 1000. Epoch loss: 672.44230, validation rmse: 1.28432, train rmse: 0.83314\n",
      "Epoch 62 of 1000. Epoch loss: 649.24994, validation rmse: 1.26875, train rmse: 0.82706\n",
      "Epoch 63 of 1000. Epoch loss: 638.28538, validation rmse: 1.27230, train rmse: 0.81427\n",
      "Epoch 64 of 1000. Epoch loss: 622.27234, validation rmse: 1.26043, train rmse: 0.81042\n",
      "Epoch 65 of 1000. Epoch loss: 613.71559, validation rmse: 1.26447, train rmse: 0.80036\n",
      "Epoch 66 of 1000. Epoch loss: 601.12262, validation rmse: 1.25411, train rmse: 0.79870\n",
      "Epoch 67 of 1000. Epoch loss: 595.11151, validation rmse: 1.26001, train rmse: 0.79142\n",
      "Epoch 68 of 1000. Epoch loss: 585.84911, validation rmse: 1.25085, train rmse: 0.79366\n",
      "Epoch 69 of 1000. Epoch loss: 584.05123, validation rmse: 1.25844, train rmse: 0.78840\n",
      "Epoch 70 of 1000. Epoch loss: 576.86301, validation rmse: 1.25169, train rmse: 0.79589\n",
      "Epoch 71 of 1000. Epoch loss: 580.64269, validation rmse: 1.26317, train rmse: 0.79438\n",
      "Epoch 72 of 1000. Epoch loss: 576.41690, validation rmse: 1.25906, train rmse: 0.81071\n",
      "Epoch 73 of 1000. Epoch loss: 590.89261, validation rmse: 1.27339, train rmse: 0.81212\n",
      "Epoch 74 of 1000. Epoch loss: 587.99277, validation rmse: 1.27526, train rmse: 0.83919\n",
      "Epoch 75 of 1000. Epoch loss: 617.63856, validation rmse: 1.29217, train rmse: 0.84023\n",
      "Epoch 76 of 1000. Epoch loss: 609.73160, validation rmse: 1.29617, train rmse: 0.87585\n",
      "Epoch 77 of 1000. Epoch loss: 658.08772, validation rmse: 1.30236, train rmse: 0.85645\n",
      "Epoch 78 of 1000. Epoch loss: 623.92750, validation rmse: 1.30308, train rmse: 0.88355\n",
      "Epoch 79 of 1000. Epoch loss: 666.15033, validation rmse: 1.29141, train rmse: 0.83762\n",
      "Epoch 80 of 1000. Epoch loss: 603.45816, validation rmse: 1.28060, train rmse: 0.84377\n",
      "Epoch 81 of 1000. Epoch loss: 618.28624, validation rmse: 1.26448, train rmse: 0.79187\n",
      "Epoch 82 of 1000. Epoch loss: 556.27784, validation rmse: 1.25031, train rmse: 0.78370\n",
      "Epoch 83 of 1000. Epoch loss: 548.24384, validation rmse: 1.24018, train rmse: 0.74641\n",
      "Epoch 84 of 1000. Epoch loss: 508.34771, validation rmse: 1.22822, train rmse: 0.73802\n",
      "Epoch 85 of 1000. Epoch loss: 497.71030, validation rmse: 1.22311, train rmse: 0.71504\n",
      "Epoch 86 of 1000. Epoch loss: 474.72837, validation rmse: 1.21600, train rmse: 0.70968\n",
      "Epoch 87 of 1000. Epoch loss: 466.42942, validation rmse: 1.21333, train rmse: 0.69564\n",
      "Epoch 88 of 1000. Epoch loss: 452.99035, validation rmse: 1.20877, train rmse: 0.69314\n",
      "Epoch 89 of 1000. Epoch loss: 447.44165, validation rmse: 1.20711, train rmse: 0.68342\n",
      "Epoch 90 of 1000. Epoch loss: 438.42552, validation rmse: 1.20440, train rmse: 0.68282\n",
      "Epoch 91 of 1000. Epoch loss: 434.58850, validation rmse: 1.20360, train rmse: 0.67563\n",
      "Epoch 92 of 1000. Epoch loss: 428.10109, validation rmse: 1.20160, train rmse: 0.67741\n",
      "Epoch 93 of 1000. Epoch loss: 426.38318, validation rmse: 1.20093, train rmse: 0.67177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94 of 1000. Epoch loss: 421.38791, validation rmse: 1.20139, train rmse: 0.67666\n",
      "Epoch 95 of 1000. Epoch loss: 422.23791, validation rmse: 1.20148, train rmse: 0.67307\n",
      "Epoch 96 of 1000. Epoch loss: 418.88398, validation rmse: 1.20375, train rmse: 0.68314\n",
      "Epoch 97 of 1000. Epoch loss: 424.63996, validation rmse: 1.20408, train rmse: 0.68133\n",
      "Epoch 98 of 1000. Epoch loss: 422.12479, validation rmse: 1.21198, train rmse: 0.69866\n",
      "Epoch 99 of 1000. Epoch loss: 435.27308, validation rmse: 1.21272, train rmse: 0.69921\n",
      "Epoch 100 of 1000. Epoch loss: 432.96986, validation rmse: 1.22507, train rmse: 0.72664\n",
      "Epoch 101 of 1000. Epoch loss: 458.70447, validation rmse: 1.22264, train rmse: 0.72474\n",
      "Epoch 102 of 1000. Epoch loss: 451.00646, validation rmse: 1.24424, train rmse: 0.75949\n",
      "Epoch 103 of 1000. Epoch loss: 489.33161, validation rmse: 1.23520, train rmse: 0.74759\n",
      "Epoch 104 of 1000. Epoch loss: 467.92354, validation rmse: 1.25460, train rmse: 0.78124\n",
      "Epoch 105 of 1000. Epoch loss: 512.49425, validation rmse: 1.23153, train rmse: 0.74693\n",
      "Epoch 106 of 1000. Epoch loss: 467.05474, validation rmse: 1.24958, train rmse: 0.76604\n",
      "Epoch 107 of 1000. Epoch loss: 495.89823, validation rmse: 1.21896, train rmse: 0.71836\n",
      "Epoch 108 of 1000. Epoch loss: 443.00450, validation rmse: 1.22639, train rmse: 0.72203\n",
      "Epoch 109 of 1000. Epoch loss: 451.01694, validation rmse: 1.19909, train rmse: 0.67811\n",
      "Epoch 110 of 1000. Epoch loss: 407.71084, validation rmse: 1.20457, train rmse: 0.67274\n",
      "Epoch 111 of 1000. Epoch loss: 402.70495, validation rmse: 1.18425, train rmse: 0.64254\n",
      "Epoch 112 of 1000. Epoch loss: 376.18995, validation rmse: 1.18933, train rmse: 0.63898\n",
      "Epoch 113 of 1000. Epoch loss: 371.10493, validation rmse: 1.17539, train rmse: 0.62020\n",
      "Epoch 114 of 1000. Epoch loss: 355.82304, validation rmse: 1.18108, train rmse: 0.61944\n",
      "Epoch 115 of 1000. Epoch loss: 352.57292, validation rmse: 1.17043, train rmse: 0.60716\n",
      "Epoch 116 of 1000. Epoch loss: 343.28969, validation rmse: 1.17644, train rmse: 0.60879\n",
      "Epoch 117 of 1000. Epoch loss: 341.82637, validation rmse: 1.16701, train rmse: 0.59993\n",
      "Epoch 118 of 1000. Epoch loss: 335.48731, validation rmse: 1.17441, train rmse: 0.60334\n",
      "Epoch 119 of 1000. Epoch loss: 335.28955, validation rmse: 1.16497, train rmse: 0.59682\n",
      "Epoch 120 of 1000. Epoch loss: 330.82630, validation rmse: 1.17458, train rmse: 0.60342\n",
      "Epoch 121 of 1000. Epoch loss: 333.13799, validation rmse: 1.16428, train rmse: 0.59863\n",
      "Epoch 122 of 1000. Epoch loss: 329.83196, validation rmse: 1.17740, train rmse: 0.60918\n",
      "Epoch 123 of 1000. Epoch loss: 335.35360, validation rmse: 1.16532, train rmse: 0.60580\n",
      "Epoch 124 of 1000. Epoch loss: 332.54284, validation rmse: 1.18244, train rmse: 0.62212\n",
      "Epoch 125 of 1000. Epoch loss: 343.54111, validation rmse: 1.16816, train rmse: 0.61947\n",
      "Epoch 126 of 1000. Epoch loss: 340.05109, validation rmse: 1.19411, train rmse: 0.64278\n",
      "Epoch 127 of 1000. Epoch loss: 358.55852, validation rmse: 1.17664, train rmse: 0.64073\n",
      "Epoch 128 of 1000. Epoch loss: 352.68861, validation rmse: 1.20679, train rmse: 0.67345\n",
      "Epoch 129 of 1000. Epoch loss: 384.28526, validation rmse: 1.18162, train rmse: 0.66321\n",
      "Epoch 130 of 1000. Epoch loss: 367.70615, validation rmse: 1.22180, train rmse: 0.69755\n",
      "Epoch 131 of 1000. Epoch loss: 404.95037, validation rmse: 1.18854, train rmse: 0.67717\n",
      "Epoch 132 of 1000. Epoch loss: 377.13560, validation rmse: 1.22568, train rmse: 0.70697\n",
      "Epoch 133 of 1000. Epoch loss: 414.70995, validation rmse: 1.18074, train rmse: 0.66927\n",
      "Epoch 134 of 1000. Epoch loss: 370.98301, validation rmse: 1.21704, train rmse: 0.68649\n",
      "Epoch 135 of 1000. Epoch loss: 395.04421, validation rmse: 1.17167, train rmse: 0.64333\n",
      "Epoch 136 of 1000. Epoch loss: 353.04656, validation rmse: 1.19851, train rmse: 0.64906\n",
      "Epoch 137 of 1000. Epoch loss: 361.38684, validation rmse: 1.15902, train rmse: 0.60958\n",
      "Epoch 138 of 1000. Epoch loss: 327.02742, validation rmse: 1.18200, train rmse: 0.60991\n",
      "Epoch 139 of 1000. Epoch loss: 327.47070, validation rmse: 1.15069, train rmse: 0.58162\n",
      "Epoch 140 of 1000. Epoch loss: 305.36643, validation rmse: 1.17025, train rmse: 0.58254\n",
      "Epoch 141 of 1000. Epoch loss: 304.43548, validation rmse: 1.14504, train rmse: 0.56366\n",
      "Epoch 142 of 1000. Epoch loss: 290.98311, validation rmse: 1.16406, train rmse: 0.56612\n",
      "Epoch 143 of 1000. Epoch loss: 290.52650, validation rmse: 1.14199, train rmse: 0.55359\n",
      "Epoch 144 of 1000. Epoch loss: 282.29894, validation rmse: 1.16128, train rmse: 0.55859\n",
      "Epoch 145 of 1000. Epoch loss: 283.53275, validation rmse: 1.13991, train rmse: 0.54941\n",
      "Epoch 146 of 1000. Epoch loss: 277.92750, validation rmse: 1.16048, train rmse: 0.55640\n",
      "Epoch 147 of 1000. Epoch loss: 280.49986, validation rmse: 1.13899, train rmse: 0.54939\n",
      "Epoch 148 of 1000. Epoch loss: 276.21577, validation rmse: 1.16181, train rmse: 0.56016\n",
      "Epoch 149 of 1000. Epoch loss: 281.72381, validation rmse: 1.13838, train rmse: 0.55439\n",
      "Epoch 150 of 1000. Epoch loss: 277.94410, validation rmse: 1.16645, train rmse: 0.56940\n",
      "Epoch 151 of 1000. Epoch loss: 286.95108, validation rmse: 1.14029, train rmse: 0.56521\n",
      "Epoch 152 of 1000. Epoch loss: 283.13261, validation rmse: 1.17386, train rmse: 0.58688\n",
      "Epoch 153 of 1000. Epoch loss: 298.70618, validation rmse: 1.14310, train rmse: 0.58184\n",
      "Epoch 154 of 1000. Epoch loss: 292.24903, validation rmse: 1.18616, train rmse: 0.61030\n",
      "Epoch 155 of 1000. Epoch loss: 315.73430, validation rmse: 1.14817, train rmse: 0.60374\n",
      "Epoch 156 of 1000. Epoch loss: 304.54284, validation rmse: 1.19830, train rmse: 0.63568\n",
      "Epoch 157 of 1000. Epoch loss: 335.74508, validation rmse: 1.15109, train rmse: 0.62005\n",
      "Epoch 158 of 1000. Epoch loss: 315.01033, validation rmse: 1.20798, train rmse: 0.65261\n",
      "Epoch 159 of 1000. Epoch loss: 350.20865, validation rmse: 1.15209, train rmse: 0.62644\n",
      "Epoch 160 of 1000. Epoch loss: 319.26103, validation rmse: 1.20678, train rmse: 0.65204\n",
      "Epoch 161 of 1000. Epoch loss: 350.05018, validation rmse: 1.14670, train rmse: 0.61368\n",
      "Epoch 162 of 1000. Epoch loss: 311.78461, validation rmse: 1.19646, train rmse: 0.63027\n",
      "Epoch 163 of 1000. Epoch loss: 331.81012, validation rmse: 1.13919, train rmse: 0.59002\n",
      "Epoch 164 of 1000. Epoch loss: 295.49677, validation rmse: 1.18272, train rmse: 0.59925\n",
      "Epoch 165 of 1000. Epoch loss: 306.05822, validation rmse: 1.13187, train rmse: 0.56352\n",
      "Epoch 166 of 1000. Epoch loss: 277.55562, validation rmse: 1.17103, train rmse: 0.56820\n",
      "Epoch 167 of 1000. Epoch loss: 281.19427, validation rmse: 1.12639, train rmse: 0.54235\n",
      "Epoch 168 of 1000. Epoch loss: 262.39392, validation rmse: 1.16271, train rmse: 0.54783\n",
      "Epoch 169 of 1000. Epoch loss: 265.15654, validation rmse: 1.12298, train rmse: 0.52978\n",
      "Epoch 170 of 1000. Epoch loss: 253.06175, validation rmse: 1.15813, train rmse: 0.53655\n",
      "Epoch 171 of 1000. Epoch loss: 256.07605, validation rmse: 1.12080, train rmse: 0.52326\n",
      "Epoch 172 of 1000. Epoch loss: 247.65322, validation rmse: 1.15624, train rmse: 0.53222\n",
      "Epoch 173 of 1000. Epoch loss: 252.07465, validation rmse: 1.11971, train rmse: 0.52159\n",
      "Epoch 174 of 1000. Epoch loss: 245.44081, validation rmse: 1.15548, train rmse: 0.53291\n",
      "Epoch 175 of 1000. Epoch loss: 251.46526, validation rmse: 1.11922, train rmse: 0.52452\n",
      "Epoch 176 of 1000. Epoch loss: 245.83169, validation rmse: 1.15829, train rmse: 0.54004\n",
      "Epoch 177 of 1000. Epoch loss: 255.44414, validation rmse: 1.12023, train rmse: 0.53254\n",
      "Epoch 178 of 1000. Epoch loss: 249.31467, validation rmse: 1.16350, train rmse: 0.55412\n",
      "Epoch 179 of 1000. Epoch loss: 264.48306, validation rmse: 1.12118, train rmse: 0.54630\n",
      "Epoch 180 of 1000. Epoch loss: 256.06460, validation rmse: 1.17435, train rmse: 0.57207\n",
      "Epoch 181 of 1000. Epoch loss: 276.70952, validation rmse: 1.12425, train rmse: 0.56302\n",
      "Epoch 182 of 1000. Epoch loss: 264.96681, validation rmse: 1.18365, train rmse: 0.59384\n",
      "Epoch 183 of 1000. Epoch loss: 292.51159, validation rmse: 1.12543, train rmse: 0.58063\n",
      "Epoch 184 of 1000. Epoch loss: 274.90142, validation rmse: 1.19419, train rmse: 0.61163\n",
      "Epoch 185 of 1000. Epoch loss: 306.45016, validation rmse: 1.12744, train rmse: 0.58890\n",
      "Epoch 186 of 1000. Epoch loss: 280.29638, validation rmse: 1.19576, train rmse: 0.61663\n",
      "Epoch 187 of 1000. Epoch loss: 310.64044, validation rmse: 1.12343, train rmse: 0.58465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188 of 1000. Epoch loss: 278.14819, validation rmse: 1.19150, train rmse: 0.60593\n",
      "Epoch 189 of 1000. Epoch loss: 302.31293, validation rmse: 1.11994, train rmse: 0.56843\n",
      "Epoch 190 of 1000. Epoch loss: 268.62602, validation rmse: 1.18043, train rmse: 0.58307\n",
      "Epoch 191 of 1000. Epoch loss: 284.31679, validation rmse: 1.11515, train rmse: 0.54760\n",
      "Epoch 192 of 1000. Epoch loss: 255.65011, validation rmse: 1.17109, train rmse: 0.55769\n",
      "Epoch 193 of 1000. Epoch loss: 264.78754, validation rmse: 1.11140, train rmse: 0.52842\n",
      "Epoch 194 of 1000. Epoch loss: 243.08620, validation rmse: 1.16230, train rmse: 0.53747\n",
      "Epoch 195 of 1000. Epoch loss: 249.58202, validation rmse: 1.10809, train rmse: 0.51540\n",
      "Epoch 196 of 1000. Epoch loss: 234.26476, validation rmse: 1.15737, train rmse: 0.52556\n",
      "Epoch 197 of 1000. Epoch loss: 240.65169, validation rmse: 1.10654, train rmse: 0.50744\n",
      "Epoch 198 of 1000. Epoch loss: 228.33316, validation rmse: 1.15464, train rmse: 0.51844\n",
      "Epoch 199 of 1000. Epoch loss: 234.79795, validation rmse: 1.10506, train rmse: 0.50484\n",
      "Epoch 200 of 1000. Epoch loss: 225.74139, validation rmse: 1.15477, train rmse: 0.51857\n",
      "Epoch 201 of 1000. Epoch loss: 234.06330, validation rmse: 1.10522, train rmse: 0.50730\n",
      "Epoch 202 of 1000. Epoch loss: 225.97201, validation rmse: 1.15585, train rmse: 0.52401\n",
      "Epoch 203 of 1000. Epoch loss: 236.92874, validation rmse: 1.10494, train rmse: 0.51388\n",
      "Epoch 204 of 1000. Epoch loss: 228.68298, validation rmse: 1.16076, train rmse: 0.53508\n",
      "Epoch 205 of 1000. Epoch loss: 243.85681, validation rmse: 1.10652, train rmse: 0.52503\n",
      "Epoch 206 of 1000. Epoch loss: 233.95402, validation rmse: 1.16734, train rmse: 0.55081\n",
      "Epoch 207 of 1000. Epoch loss: 254.45207, validation rmse: 1.10709, train rmse: 0.53858\n",
      "Epoch 208 of 1000. Epoch loss: 240.64144, validation rmse: 1.17749, train rmse: 0.56709\n",
      "Epoch 209 of 1000. Epoch loss: 265.60198, validation rmse: 1.10908, train rmse: 0.55157\n",
      "Epoch 210 of 1000. Epoch loss: 247.83620, validation rmse: 1.18233, train rmse: 0.57940\n",
      "Epoch 211 of 1000. Epoch loss: 274.86849, validation rmse: 1.10657, train rmse: 0.55627\n",
      "Epoch 212 of 1000. Epoch loss: 250.67569, validation rmse: 1.18644, train rmse: 0.58205\n",
      "Epoch 213 of 1000. Epoch loss: 276.79789, validation rmse: 1.10671, train rmse: 0.55478\n",
      "Epoch 214 of 1000. Epoch loss: 249.87430, validation rmse: 1.18162, train rmse: 0.57679\n",
      "Epoch 215 of 1000. Epoch loss: 273.22209, validation rmse: 1.10241, train rmse: 0.54310\n",
      "Epoch 216 of 1000. Epoch loss: 243.72241, validation rmse: 1.17628, train rmse: 0.55959\n",
      "Epoch 217 of 1000. Epoch loss: 259.94547, validation rmse: 1.10107, train rmse: 0.52862\n",
      "Epoch 218 of 1000. Epoch loss: 234.92816, validation rmse: 1.16699, train rmse: 0.54302\n",
      "Epoch 219 of 1000. Epoch loss: 247.93443, validation rmse: 1.09865, train rmse: 0.51500\n",
      "Epoch 220 of 1000. Epoch loss: 226.77082, validation rmse: 1.16186, train rmse: 0.52790\n",
      "Epoch 221 of 1000. Epoch loss: 236.84249, validation rmse: 1.09776, train rmse: 0.50428\n",
      "Epoch 222 of 1000. Epoch loss: 219.89316, validation rmse: 1.15854, train rmse: 0.51837\n",
      "Epoch 223 of 1000. Epoch loss: 229.81426, validation rmse: 1.09600, train rmse: 0.49911\n",
      "Epoch 224 of 1000. Epoch loss: 215.99653, validation rmse: 1.15728, train rmse: 0.51454\n",
      "Epoch 225 of 1000. Epoch loss: 226.49199, validation rmse: 1.09496, train rmse: 0.49835\n",
      "Epoch 226 of 1000. Epoch loss: 214.33050, validation rmse: 1.15762, train rmse: 0.51515\n",
      "Epoch 227 of 1000. Epoch loss: 226.25336, validation rmse: 1.09344, train rmse: 0.50087\n",
      "Epoch 228 of 1000. Epoch loss: 215.03384, validation rmse: 1.15944, train rmse: 0.51909\n",
      "Epoch 229 of 1000. Epoch loss: 228.23555, validation rmse: 1.09328, train rmse: 0.50540\n",
      "Epoch 230 of 1000. Epoch loss: 216.52353, validation rmse: 1.16251, train rmse: 0.52612\n",
      "Epoch 231 of 1000. Epoch loss: 232.59867, validation rmse: 1.09330, train rmse: 0.51285\n",
      "Epoch 232 of 1000. Epoch loss: 219.81219, validation rmse: 1.16619, train rmse: 0.53639\n",
      "Epoch 233 of 1000. Epoch loss: 239.21909, validation rmse: 1.09402, train rmse: 0.52128\n",
      "Epoch 234 of 1000. Epoch loss: 223.73870, validation rmse: 1.16844, train rmse: 0.54636\n",
      "Epoch 235 of 1000. Epoch loss: 246.31344, validation rmse: 1.09346, train rmse: 0.52699\n",
      "Epoch 236 of 1000. Epoch loss: 226.94072, validation rmse: 1.17372, train rmse: 0.55182\n",
      "Epoch 237 of 1000. Epoch loss: 249.93812, validation rmse: 1.09486, train rmse: 0.52953\n",
      "Epoch 238 of 1000. Epoch loss: 228.32099, validation rmse: 1.17616, train rmse: 0.55562\n",
      "Epoch 239 of 1000. Epoch loss: 252.82299, validation rmse: 1.09202, train rmse: 0.52932\n",
      "Epoch 240 of 1000. Epoch loss: 228.45837, validation rmse: 1.17639, train rmse: 0.55062\n",
      "Epoch 241 of 1000. Epoch loss: 248.82701, validation rmse: 1.09026, train rmse: 0.52317\n",
      "Epoch 242 of 1000. Epoch loss: 224.94715, validation rmse: 1.17183, train rmse: 0.54081\n",
      "Epoch 243 of 1000. Epoch loss: 241.57031, validation rmse: 1.08780, train rmse: 0.51353\n",
      "Epoch 244 of 1000. Epoch loss: 219.71130, validation rmse: 1.16776, train rmse: 0.52937\n",
      "Epoch 245 of 1000. Epoch loss: 233.36386, validation rmse: 1.08779, train rmse: 0.50433\n",
      "Epoch 246 of 1000. Epoch loss: 213.97510, validation rmse: 1.16208, train rmse: 0.52027\n",
      "Epoch 247 of 1000. Epoch loss: 227.03036, validation rmse: 1.08665, train rmse: 0.49720\n",
      "Epoch 248 of 1000. Epoch loss: 209.53847, validation rmse: 1.15895, train rmse: 0.51316\n",
      "Epoch 249 of 1000. Epoch loss: 221.71532, validation rmse: 1.08702, train rmse: 0.49314\n",
      "Epoch 250 of 1000. Epoch loss: 206.57999, validation rmse: 1.15695, train rmse: 0.51071\n",
      "Epoch 251 of 1000. Epoch loss: 219.80862, validation rmse: 1.08591, train rmse: 0.49215\n",
      "Epoch 252 of 1000. Epoch loss: 205.40467, validation rmse: 1.15860, train rmse: 0.51106\n",
      "Epoch 253 of 1000. Epoch loss: 219.53621, validation rmse: 1.08666, train rmse: 0.49483\n",
      "Epoch 254 of 1000. Epoch loss: 205.83308, validation rmse: 1.16044, train rmse: 0.51548\n",
      "Epoch 255 of 1000. Epoch loss: 222.13768, validation rmse: 1.08483, train rmse: 0.49872\n",
      "Epoch 256 of 1000. Epoch loss: 207.39676, validation rmse: 1.16344, train rmse: 0.51949\n",
      "Epoch 257 of 1000. Epoch loss: 224.30977, validation rmse: 1.08438, train rmse: 0.50330\n",
      "Epoch 258 of 1000. Epoch loss: 209.09331, validation rmse: 1.16540, train rmse: 0.52559\n",
      "Epoch 259 of 1000. Epoch loss: 228.26250, validation rmse: 1.08347, train rmse: 0.50821\n",
      "Epoch 260 of 1000. Epoch loss: 211.57614, validation rmse: 1.16781, train rmse: 0.53055\n",
      "Epoch 261 of 1000. Epoch loss: 231.53295, validation rmse: 1.08353, train rmse: 0.51031\n",
      "Epoch 262 of 1000. Epoch loss: 212.64148, validation rmse: 1.16840, train rmse: 0.53330\n",
      "Epoch 263 of 1000. Epoch loss: 233.48068, validation rmse: 1.08199, train rmse: 0.51002\n",
      "Epoch 264 of 1000. Epoch loss: 212.49653, validation rmse: 1.16918, train rmse: 0.53120\n",
      "Epoch 265 of 1000. Epoch loss: 231.74266, validation rmse: 1.08238, train rmse: 0.50662\n",
      "Epoch 266 of 1000. Epoch loss: 210.83332, validation rmse: 1.16848, train rmse: 0.52675\n",
      "Epoch 267 of 1000. Epoch loss: 228.47349, validation rmse: 1.08134, train rmse: 0.50274\n",
      "Epoch 268 of 1000. Epoch loss: 208.35313, validation rmse: 1.16707, train rmse: 0.52253\n",
      "Epoch 269 of 1000. Epoch loss: 225.28762, validation rmse: 1.08191, train rmse: 0.49963\n",
      "Epoch 270 of 1000. Epoch loss: 206.67561, validation rmse: 1.16514, train rmse: 0.51855\n",
      "Epoch 271 of 1000. Epoch loss: 222.58334, validation rmse: 1.08131, train rmse: 0.49518\n",
      "Epoch 272 of 1000. Epoch loss: 203.74698, validation rmse: 1.16262, train rmse: 0.51350\n",
      "Epoch 273 of 1000. Epoch loss: 218.92630, validation rmse: 1.08054, train rmse: 0.49103\n",
      "Epoch 274 of 1000. Epoch loss: 201.38763, validation rmse: 1.16107, train rmse: 0.50862\n",
      "Epoch 275 of 1000. Epoch loss: 215.50321, validation rmse: 1.07930, train rmse: 0.48789\n",
      "Epoch 276 of 1000. Epoch loss: 199.02453, validation rmse: 1.16024, train rmse: 0.50676\n",
      "Epoch 277 of 1000. Epoch loss: 213.95584, validation rmse: 1.07782, train rmse: 0.48803\n",
      "Epoch 278 of 1000. Epoch loss: 198.27692, validation rmse: 1.16201, train rmse: 0.50815\n",
      "Epoch 279 of 1000. Epoch loss: 214.26083, validation rmse: 1.07648, train rmse: 0.49142\n",
      "Epoch 280 of 1000. Epoch loss: 199.53316, validation rmse: 1.16235, train rmse: 0.51089\n",
      "Epoch 281 of 1000. Epoch loss: 215.94234, validation rmse: 1.07587, train rmse: 0.49198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 282 of 1000. Epoch loss: 199.60607, validation rmse: 1.16407, train rmse: 0.51292\n",
      "Epoch 283 of 1000. Epoch loss: 217.08671, validation rmse: 1.07646, train rmse: 0.49436\n",
      "Epoch 284 of 1000. Epoch loss: 200.36017, validation rmse: 1.16336, train rmse: 0.51422\n",
      "Epoch 285 of 1000. Epoch loss: 217.74164, validation rmse: 1.07682, train rmse: 0.49633\n",
      "Epoch 286 of 1000. Epoch loss: 201.32320, validation rmse: 1.16561, train rmse: 0.51932\n",
      "Epoch 287 of 1000. Epoch loss: 221.17978, validation rmse: 1.07810, train rmse: 0.49846\n",
      "Epoch 288 of 1000. Epoch loss: 202.40297, validation rmse: 1.16557, train rmse: 0.51994\n",
      "Epoch 289 of 1000. Epoch loss: 221.66961, validation rmse: 1.07743, train rmse: 0.49639\n",
      "Epoch 290 of 1000. Epoch loss: 201.44079, validation rmse: 1.16675, train rmse: 0.51770\n",
      "Epoch 291 of 1000. Epoch loss: 219.90585, validation rmse: 1.07626, train rmse: 0.49388\n",
      "Epoch 292 of 1000. Epoch loss: 199.99843, validation rmse: 1.16534, train rmse: 0.51321\n",
      "Epoch 293 of 1000. Epoch loss: 216.79053, validation rmse: 1.07490, train rmse: 0.48943\n",
      "Epoch 294 of 1000. Epoch loss: 197.43620, validation rmse: 1.16460, train rmse: 0.50881\n",
      "Epoch 295 of 1000. Epoch loss: 213.48851, validation rmse: 1.07365, train rmse: 0.48718\n",
      "Epoch 296 of 1000. Epoch loss: 195.93076, validation rmse: 1.16394, train rmse: 0.50546\n",
      "Epoch 297 of 1000. Epoch loss: 210.96405, validation rmse: 1.07292, train rmse: 0.48429\n",
      "Epoch 298 of 1000. Epoch loss: 193.98298, validation rmse: 1.16330, train rmse: 0.50308\n",
      "Epoch 299 of 1000. Epoch loss: 209.24990, validation rmse: 1.07258, train rmse: 0.48296\n",
      "Epoch 300 of 1000. Epoch loss: 192.95305, validation rmse: 1.16210, train rmse: 0.50145\n",
      "Epoch 301 of 1000. Epoch loss: 207.98659, validation rmse: 1.07290, train rmse: 0.48175\n",
      "Epoch 302 of 1000. Epoch loss: 191.82652, validation rmse: 1.16124, train rmse: 0.50118\n",
      "Epoch 303 of 1000. Epoch loss: 207.61531, validation rmse: 1.07410, train rmse: 0.48396\n",
      "Epoch 304 of 1000. Epoch loss: 192.41697, validation rmse: 1.16090, train rmse: 0.50461\n",
      "Epoch 305 of 1000. Epoch loss: 209.75122, validation rmse: 1.07465, train rmse: 0.48629\n",
      "Epoch 306 of 1000. Epoch loss: 193.22486, validation rmse: 1.16406, train rmse: 0.50944\n",
      "Epoch 307 of 1000. Epoch loss: 212.82155, validation rmse: 1.07484, train rmse: 0.48982\n",
      "Epoch 308 of 1000. Epoch loss: 194.71883, validation rmse: 1.16451, train rmse: 0.51149\n",
      "Epoch 309 of 1000. Epoch loss: 214.23504, validation rmse: 1.07252, train rmse: 0.48922\n",
      "Epoch 310 of 1000. Epoch loss: 194.46512, validation rmse: 1.16550, train rmse: 0.50946\n",
      "Epoch 311 of 1000. Epoch loss: 212.74398, validation rmse: 1.07119, train rmse: 0.48717\n",
      "Epoch 312 of 1000. Epoch loss: 193.79476, validation rmse: 1.16479, train rmse: 0.50682\n",
      "Epoch 313 of 1000. Epoch loss: 210.73497, validation rmse: 1.07007, train rmse: 0.48438\n",
      "Epoch 314 of 1000. Epoch loss: 192.40385, validation rmse: 1.16545, train rmse: 0.50387\n",
      "Epoch 315 of 1000. Epoch loss: 208.56665, validation rmse: 1.06986, train rmse: 0.48166\n",
      "Epoch 316 of 1000. Epoch loss: 190.67051, validation rmse: 1.16485, train rmse: 0.49937\n",
      "Epoch 317 of 1000. Epoch loss: 205.35948, validation rmse: 1.07006, train rmse: 0.47807\n",
      "Epoch 318 of 1000. Epoch loss: 188.60376, validation rmse: 1.16225, train rmse: 0.49565\n",
      "Epoch 319 of 1000. Epoch loss: 202.81493, validation rmse: 1.06975, train rmse: 0.47574\n",
      "Epoch 320 of 1000. Epoch loss: 187.11568, validation rmse: 1.16138, train rmse: 0.49444\n",
      "Epoch 321 of 1000. Epoch loss: 201.80836, validation rmse: 1.07191, train rmse: 0.47594\n",
      "Epoch 322 of 1000. Epoch loss: 186.63088, validation rmse: 1.15957, train rmse: 0.49575\n",
      "Epoch 323 of 1000. Epoch loss: 202.71678, validation rmse: 1.07212, train rmse: 0.47772\n",
      "Epoch 324 of 1000. Epoch loss: 186.91726, validation rmse: 1.16190, train rmse: 0.49943\n",
      "Epoch 325 of 1000. Epoch loss: 204.84314, validation rmse: 1.07215, train rmse: 0.48048\n",
      "Epoch 326 of 1000. Epoch loss: 187.91830, validation rmse: 1.16289, train rmse: 0.50217\n",
      "Epoch 327 of 1000. Epoch loss: 206.82858, validation rmse: 1.07069, train rmse: 0.48047\n",
      "Epoch 328 of 1000. Epoch loss: 187.87322, validation rmse: 1.16329, train rmse: 0.50045\n",
      "Epoch 329 of 1000. Epoch loss: 205.34370, validation rmse: 1.06961, train rmse: 0.48011\n",
      "Epoch 330 of 1000. Epoch loss: 187.75373, validation rmse: 1.16442, train rmse: 0.50119\n",
      "Epoch 331 of 1000. Epoch loss: 205.68529, validation rmse: 1.06763, train rmse: 0.48062\n",
      "Epoch 332 of 1000. Epoch loss: 187.73763, validation rmse: 1.16593, train rmse: 0.50020\n",
      "Epoch 333 of 1000. Epoch loss: 204.53720, validation rmse: 1.06710, train rmse: 0.48045\n",
      "Epoch 334 of 1000. Epoch loss: 187.71496, validation rmse: 1.16608, train rmse: 0.50022\n",
      "Epoch 335 of 1000. Epoch loss: 204.58051, validation rmse: 1.06627, train rmse: 0.47817\n",
      "Epoch 336 of 1000. Epoch loss: 186.62770, validation rmse: 1.16380, train rmse: 0.49446\n",
      "Epoch 337 of 1000. Epoch loss: 200.77843, validation rmse: 1.06739, train rmse: 0.47324\n",
      "Epoch 338 of 1000. Epoch loss: 184.32158, validation rmse: 1.16207, train rmse: 0.49147\n",
      "Epoch 339 of 1000. Epoch loss: 198.94917, validation rmse: 1.06890, train rmse: 0.47177\n",
      "Epoch 340 of 1000. Epoch loss: 182.95034, validation rmse: 1.16114, train rmse: 0.49219\n",
      "Epoch 341 of 1000. Epoch loss: 199.15453, validation rmse: 1.07086, train rmse: 0.47365\n",
      "Epoch 342 of 1000. Epoch loss: 183.14410, validation rmse: 1.16151, train rmse: 0.49496\n",
      "Epoch 343 of 1000. Epoch loss: 201.03888, validation rmse: 1.06980, train rmse: 0.47398\n",
      "Epoch 344 of 1000. Epoch loss: 182.92240, validation rmse: 1.16200, train rmse: 0.49449\n",
      "Epoch 345 of 1000. Epoch loss: 200.52839, validation rmse: 1.06873, train rmse: 0.47383\n",
      "Epoch 346 of 1000. Epoch loss: 182.69267, validation rmse: 1.16192, train rmse: 0.49405\n",
      "Epoch 347 of 1000. Epoch loss: 200.07782, validation rmse: 1.06653, train rmse: 0.47426\n",
      "Epoch 348 of 1000. Epoch loss: 182.84265, validation rmse: 1.16318, train rmse: 0.49372\n",
      "Epoch 349 of 1000. Epoch loss: 199.54757, validation rmse: 1.06613, train rmse: 0.47428\n",
      "Epoch 350 of 1000. Epoch loss: 182.97977, validation rmse: 1.16352, train rmse: 0.49352\n",
      "Epoch 351 of 1000. Epoch loss: 199.43576, validation rmse: 1.06420, train rmse: 0.47274\n",
      "Epoch 352 of 1000. Epoch loss: 182.24516, validation rmse: 1.16470, train rmse: 0.49062\n",
      "Epoch 353 of 1000. Epoch loss: 197.08294, validation rmse: 1.06513, train rmse: 0.47100\n",
      "Epoch 354 of 1000. Epoch loss: 181.33257, validation rmse: 1.16251, train rmse: 0.48898\n",
      "Epoch 355 of 1000. Epoch loss: 196.47738, validation rmse: 1.06438, train rmse: 0.46813\n",
      "Epoch 356 of 1000. Epoch loss: 179.82730, validation rmse: 1.16227, train rmse: 0.48682\n",
      "Epoch 357 of 1000. Epoch loss: 194.80044, validation rmse: 1.06766, train rmse: 0.46758\n",
      "Epoch 358 of 1000. Epoch loss: 179.10864, validation rmse: 1.16123, train rmse: 0.48831\n",
      "Epoch 359 of 1000. Epoch loss: 195.71476, validation rmse: 1.06752, train rmse: 0.46888\n",
      "Epoch 360 of 1000. Epoch loss: 179.20731, validation rmse: 1.16193, train rmse: 0.48975\n",
      "Epoch 361 of 1000. Epoch loss: 196.40993, validation rmse: 1.06881, train rmse: 0.47060\n",
      "Epoch 362 of 1000. Epoch loss: 179.23754, validation rmse: 1.16065, train rmse: 0.49112\n",
      "Epoch 363 of 1000. Epoch loss: 197.27893, validation rmse: 1.06550, train rmse: 0.47180\n",
      "Epoch 364 of 1000. Epoch loss: 179.95104, validation rmse: 1.16289, train rmse: 0.49180\n",
      "Epoch 365 of 1000. Epoch loss: 197.32344, validation rmse: 1.06629, train rmse: 0.47237\n",
      "Epoch 366 of 1000. Epoch loss: 180.50583, validation rmse: 1.16182, train rmse: 0.49093\n",
      "Epoch 367 of 1000. Epoch loss: 196.95711, validation rmse: 1.06312, train rmse: 0.46900\n",
      "Epoch 368 of 1000. Epoch loss: 179.29935, validation rmse: 1.16317, train rmse: 0.48638\n",
      "Epoch 369 of 1000. Epoch loss: 193.91071, validation rmse: 1.06394, train rmse: 0.46603\n",
      "Epoch 370 of 1000. Epoch loss: 177.72359, validation rmse: 1.16206, train rmse: 0.48463\n",
      "Epoch 371 of 1000. Epoch loss: 192.82722, validation rmse: 1.06226, train rmse: 0.46437\n",
      "Epoch 372 of 1000. Epoch loss: 176.79188, validation rmse: 1.16330, train rmse: 0.48271\n",
      "Epoch 373 of 1000. Epoch loss: 191.33132, validation rmse: 1.06464, train rmse: 0.46347\n",
      "Epoch 374 of 1000. Epoch loss: 175.59299, validation rmse: 1.16114, train rmse: 0.48246\n",
      "Epoch 375 of 1000. Epoch loss: 191.23503, validation rmse: 1.06382, train rmse: 0.46186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 376 of 1000. Epoch loss: 174.64230, validation rmse: 1.16118, train rmse: 0.47959\n",
      "Epoch 377 of 1000. Epoch loss: 189.06993, validation rmse: 1.06610, train rmse: 0.46212\n",
      "Epoch 378 of 1000. Epoch loss: 174.22533, validation rmse: 1.15889, train rmse: 0.48210\n",
      "Epoch 379 of 1000. Epoch loss: 190.76349, validation rmse: 1.06521, train rmse: 0.46438\n",
      "Epoch 380 of 1000. Epoch loss: 174.98141, validation rmse: 1.16062, train rmse: 0.48484\n",
      "Epoch 381 of 1000. Epoch loss: 192.09138, validation rmse: 1.06737, train rmse: 0.46892\n",
      "Epoch 382 of 1000. Epoch loss: 176.75259, validation rmse: 1.16130, train rmse: 0.49059\n",
      "Epoch 383 of 1000. Epoch loss: 195.84484, validation rmse: 1.06417, train rmse: 0.47075\n",
      "Epoch 384 of 1000. Epoch loss: 177.92939, validation rmse: 1.16477, train rmse: 0.48965\n",
      "Epoch 385 of 1000. Epoch loss: 195.09873, validation rmse: 1.06428, train rmse: 0.46942\n",
      "Epoch 386 of 1000. Epoch loss: 177.70438, validation rmse: 1.16393, train rmse: 0.48793\n",
      "Epoch 387 of 1000. Epoch loss: 194.13768, validation rmse: 1.06090, train rmse: 0.46503\n",
      "Epoch 388 of 1000. Epoch loss: 176.12345, validation rmse: 1.16405, train rmse: 0.48203\n",
      "Epoch 389 of 1000. Epoch loss: 190.35174, validation rmse: 1.06305, train rmse: 0.46117\n",
      "Epoch 390 of 1000. Epoch loss: 174.06157, validation rmse: 1.16247, train rmse: 0.47972\n",
      "Epoch 391 of 1000. Epoch loss: 188.78744, validation rmse: 1.06203, train rmse: 0.46058\n",
      "Epoch 392 of 1000. Epoch loss: 172.94200, validation rmse: 1.16334, train rmse: 0.48029\n",
      "Epoch 393 of 1000. Epoch loss: 188.78317, validation rmse: 1.06276, train rmse: 0.46091\n",
      "Epoch 394 of 1000. Epoch loss: 172.97083, validation rmse: 1.16159, train rmse: 0.47906\n",
      "Epoch 395 of 1000. Epoch loss: 188.10275, validation rmse: 1.06223, train rmse: 0.45859\n",
      "Epoch 396 of 1000. Epoch loss: 171.76941, validation rmse: 1.16088, train rmse: 0.47508\n",
      "Epoch 397 of 1000. Epoch loss: 185.40642, validation rmse: 1.06403, train rmse: 0.45654\n",
      "Epoch 398 of 1000. Epoch loss: 170.60955, validation rmse: 1.15810, train rmse: 0.47567\n",
      "Epoch 399 of 1000. Epoch loss: 186.07143, validation rmse: 1.06355, train rmse: 0.45832\n",
      "Epoch 400 of 1000. Epoch loss: 170.76831, validation rmse: 1.16081, train rmse: 0.47819\n",
      "Epoch 401 of 1000. Epoch loss: 186.95252, validation rmse: 1.06655, train rmse: 0.46219\n",
      "Epoch 402 of 1000. Epoch loss: 172.45948, validation rmse: 1.16130, train rmse: 0.48339\n",
      "Epoch 403 of 1000. Epoch loss: 190.58254, validation rmse: 1.06403, train rmse: 0.46388\n",
      "Epoch 404 of 1000. Epoch loss: 173.06607, validation rmse: 1.16368, train rmse: 0.48276\n",
      "Epoch 405 of 1000. Epoch loss: 189.77741, validation rmse: 1.06517, train rmse: 0.46297\n",
      "Epoch 406 of 1000. Epoch loss: 173.15311, validation rmse: 1.16133, train rmse: 0.48122\n",
      "Epoch 407 of 1000. Epoch loss: 189.28790, validation rmse: 1.06162, train rmse: 0.45998\n",
      "Epoch 408 of 1000. Epoch loss: 171.58963, validation rmse: 1.16226, train rmse: 0.47818\n",
      "Best validation rmse: 1.06090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0609013"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autorec.fit(mat, train_mask, max_epochs=1000, validation_mask=validation_mask, alpha=0.01, optimizer=keras.optimizers.Adam(lr=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_dic, best_rmse = None, np.inf\n",
    "for i, dic in enumerate(res):\n",
    "    if dic['rmse'] < best_rmse:\n",
    "        best_rmse = dic['rmse']\n",
    "        best_dic = dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8917625"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.01,\n",
       " 'activation1': 'sigmoid',\n",
       " 'activation2': 'sigmoid',\n",
       " 'k': 50,\n",
       " 'rmse': 0.8917625,\n",
       " 'model': <autorec.AutoRec at 0x1614448d908>}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best_dic['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89576286"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.rmse(mat, train_mask, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
